{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NEAR AI: Building a Truly Open AI","text":"<p>Welcome! NEAR AI is a toolkit to help build, measure, and deploy AI systems focused on agents.</p> <p>Driven by one of the minds behinds TensorFlow and the Transformer Architecture, NEAR AI puts you back in control. Your data stays yours, and your AI works for you, with no compromises on privacy or ownership.</p> <ul> <li> <p> NEAR AI Agents</p> <p>Autonomous system that can interact with you and use tools to solve tasks</p> <p>  Quickstart  Registry  Tools </p> </li> <li> <p> AI Models</p> <p>Best in class AI models that you can use and fine-tune to solve your tasks</p> <p>  Benchmarks  Fine-Tuning </p> </li> <li> <p> Developer Hub </p> <p>NEAR AI developer hub where you can discover and deploy agents, datasets, and models with ease. </p> <p>  Agents  Models  Datasets </p> </li> <li> <p> Community </p> <p>Join our community! Get help and contribute to the future of AI</p> <p> Community</p> </li> </ul> <p>Alpha</p> <p>NEAR AI is currently on its alpha stage. We are actively working on improving the software and would love your help.</p> <p>If you would like to help build our future, please see our contributing guide.</p>"},{"location":"agent_triggers/","title":"Agent Triggers","text":"<p>Agents can be triggered by having them listen to a named event source.</p>"},{"location":"agent_triggers/#twitter-x","title":"Twitter (X)","text":"<p>NearAI maintains a read-only Twitter interface that can be used to trigger agents under certain conditions. </p>"},{"location":"agent_triggers/#the-x_mentions-event-source","title":"The x_mentions event source","text":"<p>The <code>x_mentions</code> event source produces an event when a configured account is mentioned.</p> <p>To have an agent listen for mentions, create trigger metadata in the agent's metadata.json file as in the example below.</p> <p>To trigger an agent, mention the X account configured in the metadata.json. The agent will be invoked.</p> <pre><code>{\n  \"name\": \"near-secret-agent\",\n  \"version\": \"0.0.1\",\n  \"description\": \"An example agent that responds to Twitter mentions\",\n  \"category\": \"agent\",\n  \"tags\": [\"twitter\"],\n  \"details\": {\n    \"agent\": {\n      \"welcome\": {\n        \"title\": \"No chat interface\",\n        \"description\": \"To use tweet a message and mention @nearsecretagent.\"\n      },\n      \"defaults\": {\n        \"max_iterations\": 1,\n        \"model\": \"llama-v3p2-3b-instruct\",\n        \"model_provider\": \"fireworks\",\n        \"model_temperature\": 0.0,\n        \"model_max_tokens\": 1000\n      }\n    },\n    \"triggers\": {\n      \"events\" : {\n        \"x_mentions\": [\"@nearsecretagent\"]\n      }\n    }\n  },\n  \"show_entry\": true\n}\n</code></pre>"},{"location":"agent_triggers/#posting-to-twitter-x","title":"Posting to Twitter (X)","text":"<p>To allow your agent to post to X you will need your own developer api key. Free X developer accounts have low read limits but fairly high write limits.</p> <p>NearAI Runners include the <code>tweepy</code> library, which supports several ways to authenticate with X https://docs.tweepy.org/en/stable/authentication.html</p> <p>The example agent https://app.near.ai/agents/flatirons.near/near-secret-agent/latest/source uses 3 legged Oauth to  authorize an X account other than the developer account to post through the api as described here in the twitter docs. To accomplish this it has four secrets set on the agent: X_ACCESS_TOKEN, X_ACCESS_TOKEN_SECRET, X_CONSUMER_KEY, X_CONSUMER_SECRET.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#nearai","title":"nearai","text":""},{"location":"api/#nearai.EntryLocation","title":"EntryLocation","text":"<p>               Bases: <code>BaseModel</code></p> <p>EntryLocation</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>class EntryLocation(BaseModel):\n    \"\"\"\n    EntryLocation\n    \"\"\" # noqa: E501\n    namespace: StrictStr\n    name: StrictStr\n    version: StrictStr\n    __properties: ClassVar[List[str]] = [\"namespace\", \"name\", \"version\"]\n\n    model_config = ConfigDict(\n        populate_by_name=True,\n        validate_assignment=True,\n        protected_namespaces=(),\n    )\n\n\n    def to_str(self) -&gt; str:\n        \"\"\"Returns the string representation of the model using alias\"\"\"\n        return pprint.pformat(self.model_dump(by_alias=True))\n\n    def to_json(self) -&gt; str:\n        \"\"\"Returns the JSON representation of the model using alias\"\"\"\n        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead\n        return json.dumps(self.to_dict())\n\n    @classmethod\n    def from_json(cls, json_str: str) -&gt; Optional[Self]:\n        \"\"\"Create an instance of EntryLocation from a JSON string\"\"\"\n        return cls.from_dict(json.loads(json_str))\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Return the dictionary representation of the model using alias.\n\n        This has the following differences from calling pydantic's\n        `self.model_dump(by_alias=True)`:\n\n        * `None` is only added to the output dict for nullable fields that\n          were set at model initialization. Other fields with value `None`\n          are ignored.\n        \"\"\"\n        excluded_fields: Set[str] = set([\n        ])\n\n        _dict = self.model_dump(\n            by_alias=True,\n            exclude=excluded_fields,\n            exclude_none=True,\n        )\n        return _dict\n\n    @classmethod\n    def from_dict(cls, obj: Optional[Dict[str, Any]]) -&gt; Optional[Self]:\n        \"\"\"Create an instance of EntryLocation from a dict\"\"\"\n        if obj is None:\n            return None\n\n        if not isinstance(obj, dict):\n            return cls.model_validate(obj)\n\n        _obj = cls.model_validate({\n            \"namespace\": obj.get(\"namespace\"),\n            \"name\": obj.get(\"name\"),\n            \"version\": obj.get(\"version\")\n        })\n        return _obj\n</code></pre>"},{"location":"api/#nearai.EntryLocation.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(obj: Optional[Dict[str, Any]]) -&gt; Optional[Self]\n</code></pre> <p>Create an instance of EntryLocation from a dict</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>@classmethod\ndef from_dict(cls, obj: Optional[Dict[str, Any]]) -&gt; Optional[Self]:\n    \"\"\"Create an instance of EntryLocation from a dict\"\"\"\n    if obj is None:\n        return None\n\n    if not isinstance(obj, dict):\n        return cls.model_validate(obj)\n\n    _obj = cls.model_validate({\n        \"namespace\": obj.get(\"namespace\"),\n        \"name\": obj.get(\"name\"),\n        \"version\": obj.get(\"version\")\n    })\n    return _obj\n</code></pre>"},{"location":"api/#nearai.EntryLocation.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(json_str: str) -&gt; Optional[Self]\n</code></pre> <p>Create an instance of EntryLocation from a JSON string</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>@classmethod\ndef from_json(cls, json_str: str) -&gt; Optional[Self]:\n    \"\"\"Create an instance of EntryLocation from a JSON string\"\"\"\n    return cls.from_dict(json.loads(json_str))\n</code></pre>"},{"location":"api/#nearai.EntryLocation.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Return the dictionary representation of the model using alias.</p> <p>This has the following differences from calling pydantic's <code>self.model_dump(by_alias=True)</code>:</p> <ul> <li><code>None</code> is only added to the output dict for nullable fields that   were set at model initialization. Other fields with value <code>None</code>   are ignored.</li> </ul> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Return the dictionary representation of the model using alias.\n\n    This has the following differences from calling pydantic's\n    `self.model_dump(by_alias=True)`:\n\n    * `None` is only added to the output dict for nullable fields that\n      were set at model initialization. Other fields with value `None`\n      are ignored.\n    \"\"\"\n    excluded_fields: Set[str] = set([\n    ])\n\n    _dict = self.model_dump(\n        by_alias=True,\n        exclude=excluded_fields,\n        exclude_none=True,\n    )\n    return _dict\n</code></pre>"},{"location":"api/#nearai.EntryLocation.to_json","title":"to_json","text":"<pre><code>to_json() -&gt; str\n</code></pre> <p>Returns the JSON representation of the model using alias</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Returns the JSON representation of the model using alias\"\"\"\n    # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead\n    return json.dumps(self.to_dict())\n</code></pre>"},{"location":"api/#nearai.EntryLocation.to_str","title":"to_str","text":"<pre><code>to_str() -&gt; str\n</code></pre> <p>Returns the string representation of the model using alias</p> Source code in <code>nearai/openapi_client/models/entry_location.py</code> <pre><code>def to_str(self) -&gt; str:\n    \"\"\"Returns the string representation of the model using alias\"\"\"\n    return pprint.pformat(self.model_dump(by_alias=True))\n</code></pre>"},{"location":"api/#nearai.parse_location","title":"parse_location","text":"<pre><code>parse_location(entry_location: str) -&gt; EntryLocation\n</code></pre> <p>Create a EntryLocation from a string in the format namespace/name/version.</p> Source code in <code>nearai/lib.py</code> <pre><code>def parse_location(entry_location: str) -&gt; EntryLocation:\n    \"\"\"Create a EntryLocation from a string in the format namespace/name/version.\"\"\"\n    match = entry_location_pattern.match(entry_location)\n\n    if match is None:\n        raise ValueError(f\"Invalid entry format: {entry_location}. Should have the format &lt;namespace&gt;/&lt;name&gt;/&lt;version&gt;\")\n\n    return EntryLocation(\n        namespace=match.group(\"namespace\"),\n        name=match.group(\"name\"),\n        version=match.group(\"version\"),\n    )\n</code></pre>"},{"location":"api/#nearai.agents","title":"agents","text":""},{"location":"api/#nearai.agents.agent","title":"agent","text":""},{"location":"api/#nearai.agents.agent.Agent","title":"Agent","text":"<p>               Bases: <code>object</code></p> Source code in <code>nearai/agents/agent.py</code> <pre><code>class Agent(object):\n    def __init__(  # noqa: D107\n        self, identifier: str, agent_files: Union[List, Path], metadata: Dict, change_to_temp_dir: bool = True\n    ):  # noqa: D107\n        self.code: Optional[CodeType] = None\n        self.file_cache: dict[str, Union[str, bytes]] = {}\n        self.identifier = identifier\n        name_parts = identifier.split(\"/\")\n        self.namespace = name_parts[0]\n        self.name = name_parts[1]\n        self.version = name_parts[2]\n\n        self.metadata = metadata\n        self.env_vars: Dict[str, Any] = {}\n\n        self.model = \"\"\n        self.model_provider = \"\"\n        self.model_temperature: Optional[float] = None\n        self.model_max_tokens: Optional[int] = None\n        self.max_iterations = 1\n        self.welcome_title: Optional[str] = None\n        self.welcome_description: Optional[str] = None\n\n        self.set_agent_metadata(metadata)\n        self.agent_files = agent_files\n        self.original_cwd = os.getcwd()\n\n        self.temp_dir = self.write_agent_files_to_temp(agent_files)\n        self.ts_runner_dir = \"\"\n        self.change_to_temp_dir = change_to_temp_dir\n        self.agent_filename = \"\"\n        self.agent_language = \"\"\n\n    def get_full_name(self):\n        \"\"\"Returns full agent name.\"\"\"\n        return f\"{self.namespace}/{self.name}/{self.version}\"\n\n    @staticmethod\n    def write_agent_files_to_temp(agent_files):\n        \"\"\"Write agent files to a temporary directory.\"\"\"\n        unique_id = uuid.uuid4().hex\n        temp_dir = os.path.join(tempfile.gettempdir(), f\"agent_{unique_id}\")\n\n        if isinstance(agent_files, List):\n            os.makedirs(temp_dir, exist_ok=True)\n\n            for file_obj in agent_files:\n                file_path = os.path.join(temp_dir, file_obj[\"filename\"])\n\n                try:\n                    if not os.path.exists(os.path.dirname(file_path)):\n                        os.makedirs(os.path.dirname(file_path))\n\n                    content = file_obj[\"content\"]\n\n                    if isinstance(content, dict) or isinstance(content, list):\n                        try:\n                            content = json.dumps(content)\n                        except Exception as e:\n                            print(f\"Error converting content to json: {e}\")\n                        content = str(content)\n\n                    if isinstance(content, str):\n                        content = content.encode(\"utf-8\")\n\n                    with open(file_path, \"wb\") as f:\n                        with io.BytesIO(content) as byte_stream:\n                            shutil.copyfileobj(byte_stream, f)\n                except Exception as e:\n                    print(f\"Error writing file {file_path}: {e}\")\n                    raise e\n\n        else:\n            # if agent files is a PosixPath, it is a path to the agent directory\n            # Copy all agent files including subfolders\n            shutil.copytree(agent_files, temp_dir, dirs_exist_ok=True)\n\n        return temp_dir\n\n    def set_agent_metadata(self, metadata) -&gt; None:\n        \"\"\"Set agent details from metadata.\"\"\"\n        try:\n            self.name = metadata[\"name\"]\n            self.version = metadata[\"version\"]\n        except KeyError as e:\n            raise ValueError(f\"Missing key in metadata: {e}\") from None\n\n        details = metadata.get(\"details\", {})\n        agent = details.get(\"agent\", {})\n        welcome = agent.get(\"welcome\", {})\n\n        self.env_vars = details.get(\"env_vars\", {})\n        self.welcome_title = welcome.get(\"title\")\n        self.welcome_description = welcome.get(\"description\")\n\n        if agent_metadata := details.get(\"agent\", None):\n            if defaults := agent_metadata.get(\"defaults\", None):\n                self.model = defaults.get(\"model\", self.model)\n                self.model_provider = defaults.get(\"model_provider\", self.model_provider)\n                self.model_temperature = defaults.get(\"model_temperature\", self.model_temperature)\n                self.model_max_tokens = defaults.get(\"model_max_tokens\", self.model_max_tokens)\n                self.max_iterations = defaults.get(\"max_iterations\", self.max_iterations)\n\n        if not self.version or not self.name:\n            raise ValueError(\"Both 'version' and 'name' must be non-empty in metadata.\")\n\n    def run_python_code(self, agent_namespace, agent_runner_user) -&gt; Tuple[Optional[str], Optional[str]]:\n        \"\"\"Launch python agent.\"\"\"\n        try:\n            # switch to user env.agent_runner_user\n            if agent_runner_user:\n                user_info = pwd.getpwnam(agent_runner_user)\n                os.setgid(user_info.pw_gid)\n                os.setuid(user_info.pw_uid)\n\n            # Run the code\n            # NOTE: runpy.run_path does not work in a multithreaded environment when running benchmark.\n            #       The performance of runpy.run_path may also change depending on a system, e.g. it may\n            #       work on Linux but not work on Mac.\n            #       `compile` and `exec` have been tested to work properly in a multithreaded environment.\n            if self.code:\n                exec(self.code, agent_namespace)\n\n            # If no errors occur, return None\n            return None, None\n\n        except Exception as e:\n            # Return error message and full traceback as strings\n            return str(e), traceback.format_exc()\n\n    def run_ts_agent(self, agent_filename, env_vars, json_params):\n        \"\"\"Launch typescript agent.\"\"\"\n        print(f\"Running typescript agent {agent_filename} from {self.ts_runner_dir}\")\n\n        # Configure npm to use tmp directories\n        env = os.environ.copy()\n        env.update(\n            {\n                \"NPM_CONFIG_CACHE\": \"/tmp/npm_cache\",\n                \"NPM_CONFIG_PREFIX\": \"/tmp/npm_prefix\",\n                \"HOME\": \"/tmp\",  # Redirect npm home\n                \"NPM_CONFIG_LOGLEVEL\": \"error\",  # Suppress warnings, show only errors\n            }\n        )\n\n        # Ensure directory structure exists\n        os.makedirs(\"/tmp/npm_cache\", exist_ok=True)\n        os.makedirs(\"/tmp/npm_prefix\", exist_ok=True)\n\n        # read file /tmp/build-info.txt if exists\n        if os.path.exists(\"/var/task/build-info.txt\"):\n            with open(\"/var/task/build-info.txt\", \"r\") as file:\n                print(\"BUILD ID: \", file.read())\n\n        if env_vars.get(\"DEBUG\"):\n            print(\"Directory structure:\", os.listdir(\"/tmp/ts_runner\"))\n            print(\"Check package.json:\", os.path.exists(os.path.join(self.ts_runner_dir, \"package.json\")))\n            print(\"Symlink exists:\", os.path.exists(\"/tmp/ts_runner/node_modules/.bin/tsc\"))\n            print(\"Build files exist:\", os.path.exists(\"/tmp/ts_runner/build/sdk/main.js\"))\n\n        # Launching a subprocess to run an npm script with specific configurations\n        ts_process = subprocess.Popen(\n            [\n                \"npm\",  # Command to run Node Package Manager\n                \"--loglevel=error\",  # Suppress npm warnings and info logs, only show errors\n                \"--prefix\",\n                self.ts_runner_dir,  # Specifies the directory where npm should look for package.json\n                \"run\",\n                \"start\",  # Runs the \"start\" script defined in package.json, this launches the agent\n                \"agents/agent.ts\",\n                json_params,  # Arguments passed to the \"start\" script to configure the agent\n            ],\n            stdout=subprocess.PIPE,  # Captures standard output from the process\n            stderr=subprocess.PIPE,  # Captures standard error\n            cwd=self.ts_runner_dir,  # Sets the current working directory for the process\n            env=env_vars,  # Provides custom environment variables to the subprocess\n        )\n\n        stdout, stderr = ts_process.communicate()\n\n        stdout = stdout.decode().strip()\n        if stdout and env_vars.get(\"DEBUG\"):\n            print(f\"TS AGENT STDOUT: {stdout}\")\n\n        stderr = stderr.decode().strip()\n        if stderr:\n            print(f\"TS AGENT STDERR: {stderr}\")\n\n    def run(self, env: Any, task: Optional[str] = None) -&gt; Tuple[Optional[str], Optional[str]]:\n        \"\"\"Run the agent code. Returns error message and traceback message.\"\"\"\n        # combine agent.env_vars and env.env_vars\n        total_env_vars = {**self.env_vars, **env.env_vars}\n\n        # save os env vars\n        os.environ.update(total_env_vars)\n        # save env.env_vars\n        env.env_vars = total_env_vars\n\n        agent_ts_files_to_transpile = []\n\n        if not self.agent_filename or True:\n            # if agent has \"agent.py\" file, we use python runner\n            if os.path.exists(os.path.join(self.temp_dir, AGENT_FILENAME_PY)):\n                self.agent_filename = os.path.join(self.temp_dir, AGENT_FILENAME_PY)\n                self.agent_language = \"py\"\n                with open(self.agent_filename, \"r\") as agent_file:\n                    self.code = compile(agent_file.read(), self.agent_filename, \"exec\")\n            # else, if agent has \"agent.ts\" file, we use typescript runner\n            elif os.path.exists(os.path.join(self.temp_dir, AGENT_FILENAME_TS)):\n                self.agent_filename = os.path.join(self.temp_dir, AGENT_FILENAME_TS)\n                self.agent_language = \"ts\"\n\n                # copy files from nearai/ts_runner_sdk to self.temp_dir\n                ts_runner_sdk_dir = \"/tmp/ts_runner\"\n                ts_runner_agent_dir = os.path.join(ts_runner_sdk_dir, \"agents\")\n\n                ts_runner_actual_path = \"/var/task/ts_runner\"\n\n                shutil.copytree(ts_runner_actual_path, ts_runner_sdk_dir, symlinks=True, dirs_exist_ok=True)\n\n                # make ts agents dir if not exists\n                if not os.path.exists(ts_runner_agent_dir):\n                    os.makedirs(ts_runner_agent_dir, exist_ok=True)\n\n                # copy agents files\n                shutil.copy(os.path.join(self.temp_dir, AGENT_FILENAME_TS), ts_runner_agent_dir)\n\n                self.ts_runner_dir = ts_runner_sdk_dir\n            else:\n                raise ValueError(f\"Agent run error: {AGENT_FILENAME_PY} or {AGENT_FILENAME_TS} does not exist\")\n\n            # cache all agent files in file_cache\n            for root, _, files in os.walk(self.temp_dir):\n                for file in files:\n                    file_path = os.path.join(root, file)\n\n                    # get file extension for agent_filename\n                    if file_path.endswith(\".ts\"):\n                        agent_ts_files_to_transpile.append(file_path)\n\n                    relative_path = os.path.relpath(file_path, self.temp_dir)\n                    try:\n                        with open(file_path, \"rb\") as f:\n                            content = f.read()\n                            try:\n                                # Try to decode as text\n                                self.file_cache[relative_path] = content.decode(\"utf-8\")\n                            except UnicodeDecodeError:\n                                # If decoding fails, store as binary\n                                self.file_cache[relative_path] = content\n\n                    except Exception as e:\n                        print(f\"Error with cache creation {file_path}: {e}\")\n\n        else:\n            print(\"Using cached agent code\")\n\n        namespace = {\n            \"env\": env,\n            \"agent\": self,\n            \"task\": task,\n            \"__name__\": \"__main__\",\n            \"__file__\": self.agent_filename,\n        }\n\n        user_auth = env.user_auth\n\n        # clear user_auth we saved before\n        env.user_auth = None\n\n        error_message, traceback_message = None, None\n\n        try:\n            if self.change_to_temp_dir:\n                if not os.path.exists(self.temp_dir):\n                    os.makedirs(self.temp_dir, exist_ok=True)\n                os.chdir(self.temp_dir)\n            sys.path.insert(0, self.temp_dir)\n\n            if self.agent_language == \"ts\":\n                agent_json_params = json.dumps(\n                    {\n                        \"thread_id\": env._thread_id,\n                        \"user_auth\": user_auth,\n                        \"base_url\": env.base_url,\n                        \"env_vars\": env.env_vars,\n                        \"agent_ts_files_to_transpile\": agent_ts_files_to_transpile,\n                    }\n                )\n\n                process = multiprocessing.Process(\n                    target=self.run_ts_agent, args=[self.agent_filename, env.env_vars, agent_json_params]\n                )\n                process.start()\n                process.join()\n            else:\n                if env.agent_runner_user:\n                    process = multiprocessing.Process(\n                        target=self.run_python_code, args=[namespace, env.agent_runner_user]\n                    )\n                    process.start()\n                    process.join()\n                else:\n                    error_message, traceback_message = self.run_python_code(namespace, env.agent_runner_user)\n        finally:\n            if os.path.exists(self.temp_dir):\n                sys.path.remove(self.temp_dir)\n            if self.change_to_temp_dir:\n                os.chdir(self.original_cwd)\n\n        return error_message, traceback_message\n\n    @staticmethod\n    def load_agents(agents: str, config: ClientConfig, local: bool = False):\n        \"\"\"Loads agents from the registry.\"\"\"\n        return [Agent.load_agent(agent, config, local) for agent in agents.split(\",\")]\n\n    @staticmethod\n    def load_agent(\n        name: str,\n        config: ClientConfig,\n        local: bool = False,\n    ):\n        \"\"\"Loads a single agent from the registry.\"\"\"\n        from nearai.registry import get_registry_folder, registry\n\n        identifier = None\n        if local:\n            agent_files_path = get_registry_folder() / name\n            if config.auth is None:\n                namespace = \"not-logged-in\"\n            else:\n                namespace = config.auth.account_id\n        else:\n            agent_files_path = registry.download(name)\n            identifier = name\n        assert agent_files_path is not None, f\"Agent {name} not found.\"\n\n        metadata_path = os.path.join(agent_files_path, \"metadata.json\")\n        if not os.path.exists(metadata_path):\n            raise FileNotFoundError(f\"Metadata file not found: {metadata_path}\")\n        with open(metadata_path) as f:\n            metadata: Dict[str, Any] = json.load(f)\n\n        if not identifier:\n            identifier = \"/\".join([namespace, metadata[\"name\"], metadata[\"version\"]])\n\n        return Agent(identifier, agent_files_path, metadata)\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.get_full_name","title":"get_full_name","text":"<pre><code>get_full_name()\n</code></pre> <p>Returns full agent name.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>def get_full_name(self):\n    \"\"\"Returns full agent name.\"\"\"\n    return f\"{self.namespace}/{self.name}/{self.version}\"\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.load_agent","title":"load_agent  <code>staticmethod</code>","text":"<pre><code>load_agent(name: str, config: ClientConfig, local: bool = False)\n</code></pre> <p>Loads a single agent from the registry.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>@staticmethod\ndef load_agent(\n    name: str,\n    config: ClientConfig,\n    local: bool = False,\n):\n    \"\"\"Loads a single agent from the registry.\"\"\"\n    from nearai.registry import get_registry_folder, registry\n\n    identifier = None\n    if local:\n        agent_files_path = get_registry_folder() / name\n        if config.auth is None:\n            namespace = \"not-logged-in\"\n        else:\n            namespace = config.auth.account_id\n    else:\n        agent_files_path = registry.download(name)\n        identifier = name\n    assert agent_files_path is not None, f\"Agent {name} not found.\"\n\n    metadata_path = os.path.join(agent_files_path, \"metadata.json\")\n    if not os.path.exists(metadata_path):\n        raise FileNotFoundError(f\"Metadata file not found: {metadata_path}\")\n    with open(metadata_path) as f:\n        metadata: Dict[str, Any] = json.load(f)\n\n    if not identifier:\n        identifier = \"/\".join([namespace, metadata[\"name\"], metadata[\"version\"]])\n\n    return Agent(identifier, agent_files_path, metadata)\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.load_agents","title":"load_agents  <code>staticmethod</code>","text":"<pre><code>load_agents(agents: str, config: ClientConfig, local: bool = False)\n</code></pre> <p>Loads agents from the registry.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>@staticmethod\ndef load_agents(agents: str, config: ClientConfig, local: bool = False):\n    \"\"\"Loads agents from the registry.\"\"\"\n    return [Agent.load_agent(agent, config, local) for agent in agents.split(\",\")]\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.run","title":"run","text":"<pre><code>run(env: Any, task: Optional[str] = None) -&gt; Tuple[Optional[str], Optional[str]]\n</code></pre> <p>Run the agent code. Returns error message and traceback message.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>def run(self, env: Any, task: Optional[str] = None) -&gt; Tuple[Optional[str], Optional[str]]:\n    \"\"\"Run the agent code. Returns error message and traceback message.\"\"\"\n    # combine agent.env_vars and env.env_vars\n    total_env_vars = {**self.env_vars, **env.env_vars}\n\n    # save os env vars\n    os.environ.update(total_env_vars)\n    # save env.env_vars\n    env.env_vars = total_env_vars\n\n    agent_ts_files_to_transpile = []\n\n    if not self.agent_filename or True:\n        # if agent has \"agent.py\" file, we use python runner\n        if os.path.exists(os.path.join(self.temp_dir, AGENT_FILENAME_PY)):\n            self.agent_filename = os.path.join(self.temp_dir, AGENT_FILENAME_PY)\n            self.agent_language = \"py\"\n            with open(self.agent_filename, \"r\") as agent_file:\n                self.code = compile(agent_file.read(), self.agent_filename, \"exec\")\n        # else, if agent has \"agent.ts\" file, we use typescript runner\n        elif os.path.exists(os.path.join(self.temp_dir, AGENT_FILENAME_TS)):\n            self.agent_filename = os.path.join(self.temp_dir, AGENT_FILENAME_TS)\n            self.agent_language = \"ts\"\n\n            # copy files from nearai/ts_runner_sdk to self.temp_dir\n            ts_runner_sdk_dir = \"/tmp/ts_runner\"\n            ts_runner_agent_dir = os.path.join(ts_runner_sdk_dir, \"agents\")\n\n            ts_runner_actual_path = \"/var/task/ts_runner\"\n\n            shutil.copytree(ts_runner_actual_path, ts_runner_sdk_dir, symlinks=True, dirs_exist_ok=True)\n\n            # make ts agents dir if not exists\n            if not os.path.exists(ts_runner_agent_dir):\n                os.makedirs(ts_runner_agent_dir, exist_ok=True)\n\n            # copy agents files\n            shutil.copy(os.path.join(self.temp_dir, AGENT_FILENAME_TS), ts_runner_agent_dir)\n\n            self.ts_runner_dir = ts_runner_sdk_dir\n        else:\n            raise ValueError(f\"Agent run error: {AGENT_FILENAME_PY} or {AGENT_FILENAME_TS} does not exist\")\n\n        # cache all agent files in file_cache\n        for root, _, files in os.walk(self.temp_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n\n                # get file extension for agent_filename\n                if file_path.endswith(\".ts\"):\n                    agent_ts_files_to_transpile.append(file_path)\n\n                relative_path = os.path.relpath(file_path, self.temp_dir)\n                try:\n                    with open(file_path, \"rb\") as f:\n                        content = f.read()\n                        try:\n                            # Try to decode as text\n                            self.file_cache[relative_path] = content.decode(\"utf-8\")\n                        except UnicodeDecodeError:\n                            # If decoding fails, store as binary\n                            self.file_cache[relative_path] = content\n\n                except Exception as e:\n                    print(f\"Error with cache creation {file_path}: {e}\")\n\n    else:\n        print(\"Using cached agent code\")\n\n    namespace = {\n        \"env\": env,\n        \"agent\": self,\n        \"task\": task,\n        \"__name__\": \"__main__\",\n        \"__file__\": self.agent_filename,\n    }\n\n    user_auth = env.user_auth\n\n    # clear user_auth we saved before\n    env.user_auth = None\n\n    error_message, traceback_message = None, None\n\n    try:\n        if self.change_to_temp_dir:\n            if not os.path.exists(self.temp_dir):\n                os.makedirs(self.temp_dir, exist_ok=True)\n            os.chdir(self.temp_dir)\n        sys.path.insert(0, self.temp_dir)\n\n        if self.agent_language == \"ts\":\n            agent_json_params = json.dumps(\n                {\n                    \"thread_id\": env._thread_id,\n                    \"user_auth\": user_auth,\n                    \"base_url\": env.base_url,\n                    \"env_vars\": env.env_vars,\n                    \"agent_ts_files_to_transpile\": agent_ts_files_to_transpile,\n                }\n            )\n\n            process = multiprocessing.Process(\n                target=self.run_ts_agent, args=[self.agent_filename, env.env_vars, agent_json_params]\n            )\n            process.start()\n            process.join()\n        else:\n            if env.agent_runner_user:\n                process = multiprocessing.Process(\n                    target=self.run_python_code, args=[namespace, env.agent_runner_user]\n                )\n                process.start()\n                process.join()\n            else:\n                error_message, traceback_message = self.run_python_code(namespace, env.agent_runner_user)\n    finally:\n        if os.path.exists(self.temp_dir):\n            sys.path.remove(self.temp_dir)\n        if self.change_to_temp_dir:\n            os.chdir(self.original_cwd)\n\n    return error_message, traceback_message\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.run_python_code","title":"run_python_code","text":"<pre><code>run_python_code(agent_namespace, agent_runner_user) -&gt; Tuple[Optional[str], Optional[str]]\n</code></pre> <p>Launch python agent.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>def run_python_code(self, agent_namespace, agent_runner_user) -&gt; Tuple[Optional[str], Optional[str]]:\n    \"\"\"Launch python agent.\"\"\"\n    try:\n        # switch to user env.agent_runner_user\n        if agent_runner_user:\n            user_info = pwd.getpwnam(agent_runner_user)\n            os.setgid(user_info.pw_gid)\n            os.setuid(user_info.pw_uid)\n\n        # Run the code\n        # NOTE: runpy.run_path does not work in a multithreaded environment when running benchmark.\n        #       The performance of runpy.run_path may also change depending on a system, e.g. it may\n        #       work on Linux but not work on Mac.\n        #       `compile` and `exec` have been tested to work properly in a multithreaded environment.\n        if self.code:\n            exec(self.code, agent_namespace)\n\n        # If no errors occur, return None\n        return None, None\n\n    except Exception as e:\n        # Return error message and full traceback as strings\n        return str(e), traceback.format_exc()\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.run_ts_agent","title":"run_ts_agent","text":"<pre><code>run_ts_agent(agent_filename, env_vars, json_params)\n</code></pre> <p>Launch typescript agent.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>def run_ts_agent(self, agent_filename, env_vars, json_params):\n    \"\"\"Launch typescript agent.\"\"\"\n    print(f\"Running typescript agent {agent_filename} from {self.ts_runner_dir}\")\n\n    # Configure npm to use tmp directories\n    env = os.environ.copy()\n    env.update(\n        {\n            \"NPM_CONFIG_CACHE\": \"/tmp/npm_cache\",\n            \"NPM_CONFIG_PREFIX\": \"/tmp/npm_prefix\",\n            \"HOME\": \"/tmp\",  # Redirect npm home\n            \"NPM_CONFIG_LOGLEVEL\": \"error\",  # Suppress warnings, show only errors\n        }\n    )\n\n    # Ensure directory structure exists\n    os.makedirs(\"/tmp/npm_cache\", exist_ok=True)\n    os.makedirs(\"/tmp/npm_prefix\", exist_ok=True)\n\n    # read file /tmp/build-info.txt if exists\n    if os.path.exists(\"/var/task/build-info.txt\"):\n        with open(\"/var/task/build-info.txt\", \"r\") as file:\n            print(\"BUILD ID: \", file.read())\n\n    if env_vars.get(\"DEBUG\"):\n        print(\"Directory structure:\", os.listdir(\"/tmp/ts_runner\"))\n        print(\"Check package.json:\", os.path.exists(os.path.join(self.ts_runner_dir, \"package.json\")))\n        print(\"Symlink exists:\", os.path.exists(\"/tmp/ts_runner/node_modules/.bin/tsc\"))\n        print(\"Build files exist:\", os.path.exists(\"/tmp/ts_runner/build/sdk/main.js\"))\n\n    # Launching a subprocess to run an npm script with specific configurations\n    ts_process = subprocess.Popen(\n        [\n            \"npm\",  # Command to run Node Package Manager\n            \"--loglevel=error\",  # Suppress npm warnings and info logs, only show errors\n            \"--prefix\",\n            self.ts_runner_dir,  # Specifies the directory where npm should look for package.json\n            \"run\",\n            \"start\",  # Runs the \"start\" script defined in package.json, this launches the agent\n            \"agents/agent.ts\",\n            json_params,  # Arguments passed to the \"start\" script to configure the agent\n        ],\n        stdout=subprocess.PIPE,  # Captures standard output from the process\n        stderr=subprocess.PIPE,  # Captures standard error\n        cwd=self.ts_runner_dir,  # Sets the current working directory for the process\n        env=env_vars,  # Provides custom environment variables to the subprocess\n    )\n\n    stdout, stderr = ts_process.communicate()\n\n    stdout = stdout.decode().strip()\n    if stdout and env_vars.get(\"DEBUG\"):\n        print(f\"TS AGENT STDOUT: {stdout}\")\n\n    stderr = stderr.decode().strip()\n    if stderr:\n        print(f\"TS AGENT STDERR: {stderr}\")\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.set_agent_metadata","title":"set_agent_metadata","text":"<pre><code>set_agent_metadata(metadata) -&gt; None\n</code></pre> <p>Set agent details from metadata.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>def set_agent_metadata(self, metadata) -&gt; None:\n    \"\"\"Set agent details from metadata.\"\"\"\n    try:\n        self.name = metadata[\"name\"]\n        self.version = metadata[\"version\"]\n    except KeyError as e:\n        raise ValueError(f\"Missing key in metadata: {e}\") from None\n\n    details = metadata.get(\"details\", {})\n    agent = details.get(\"agent\", {})\n    welcome = agent.get(\"welcome\", {})\n\n    self.env_vars = details.get(\"env_vars\", {})\n    self.welcome_title = welcome.get(\"title\")\n    self.welcome_description = welcome.get(\"description\")\n\n    if agent_metadata := details.get(\"agent\", None):\n        if defaults := agent_metadata.get(\"defaults\", None):\n            self.model = defaults.get(\"model\", self.model)\n            self.model_provider = defaults.get(\"model_provider\", self.model_provider)\n            self.model_temperature = defaults.get(\"model_temperature\", self.model_temperature)\n            self.model_max_tokens = defaults.get(\"model_max_tokens\", self.model_max_tokens)\n            self.max_iterations = defaults.get(\"max_iterations\", self.max_iterations)\n\n    if not self.version or not self.name:\n        raise ValueError(\"Both 'version' and 'name' must be non-empty in metadata.\")\n</code></pre>"},{"location":"api/#nearai.agents.agent.Agent.write_agent_files_to_temp","title":"write_agent_files_to_temp  <code>staticmethod</code>","text":"<pre><code>write_agent_files_to_temp(agent_files)\n</code></pre> <p>Write agent files to a temporary directory.</p> Source code in <code>nearai/agents/agent.py</code> <pre><code>@staticmethod\ndef write_agent_files_to_temp(agent_files):\n    \"\"\"Write agent files to a temporary directory.\"\"\"\n    unique_id = uuid.uuid4().hex\n    temp_dir = os.path.join(tempfile.gettempdir(), f\"agent_{unique_id}\")\n\n    if isinstance(agent_files, List):\n        os.makedirs(temp_dir, exist_ok=True)\n\n        for file_obj in agent_files:\n            file_path = os.path.join(temp_dir, file_obj[\"filename\"])\n\n            try:\n                if not os.path.exists(os.path.dirname(file_path)):\n                    os.makedirs(os.path.dirname(file_path))\n\n                content = file_obj[\"content\"]\n\n                if isinstance(content, dict) or isinstance(content, list):\n                    try:\n                        content = json.dumps(content)\n                    except Exception as e:\n                        print(f\"Error converting content to json: {e}\")\n                    content = str(content)\n\n                if isinstance(content, str):\n                    content = content.encode(\"utf-8\")\n\n                with open(file_path, \"wb\") as f:\n                    with io.BytesIO(content) as byte_stream:\n                        shutil.copyfileobj(byte_stream, f)\n            except Exception as e:\n                print(f\"Error writing file {file_path}: {e}\")\n                raise e\n\n    else:\n        # if agent files is a PosixPath, it is a path to the agent directory\n        # Copy all agent files including subfolders\n        shutil.copytree(agent_files, temp_dir, dirs_exist_ok=True)\n\n    return temp_dir\n</code></pre>"},{"location":"api/#nearai.agents.environment","title":"environment","text":""},{"location":"api/#nearai.agents.environment.Environment","title":"Environment","text":"<p>               Bases: <code>object</code></p> Source code in <code>nearai/agents/environment.py</code> <pre><code>class Environment(object):\n    def __init__(  # noqa: D107\n        self,\n        path: str,\n        agents: List[Agent],\n        client: InferenceClient,\n        hub_client: OpenAI,\n        thread_id: str,\n        run_id: str,\n        create_files: bool = True,\n        env_vars: Optional[Dict[str, Any]] = None,\n        tool_resources: Optional[Dict[str, Any]] = None,\n        print_system_log: bool = False,\n        agent_runner_user: Optional[str] = None,\n        fastnear_api_key: Optional[str] = None,\n        approvals=None,\n    ) -&gt; None:\n        # Warning: never expose `client` or `_hub_client` to agent's environment\n\n        self.base_url = client._config.base_url\n\n        # user_auth is used to authenticate the user in the ts_runner. It will be removed after that in\n        # `nearai/agents/agent.py`\n        self.user_auth = client._auth\n\n        # Placeholder for solver\n        self.client: Optional[InferenceClient] = None\n\n        self._path = path\n        self._agents = agents\n        self._done = False\n        self._pending_ext_agent = False\n        self.env_vars: Dict[str, Any] = env_vars if env_vars else {}\n        self._last_used_model = \"\"\n        self.tool_resources: Dict[str, Any] = tool_resources if tool_resources else {}\n        self.print_system_log = print_system_log\n        self.agent_runner_user = agent_runner_user\n        self._approvals = approvals if approvals else default_approvals\n        self._thread_id = thread_id\n        self._run_id = run_id\n        self._debug_mode: bool = any(\n            str(value).lower() in (\"true\", \"1\", \"yes\", \"on\")\n            for key, value in self.env_vars.items()\n            if key.lower() == \"debug\"\n        )\n        # Expose the NEAR account_id of a user that signs this request to run an agent.\n        self.signer_account_id: str = client._config.auth.account_id if client._config.auth else \"\"\n\n        if fastnear_api_key:\n            default_mainnet_rpc = f\"https://rpc.mainnet.fastnear.com?apiKey={fastnear_api_key}\"\n        else:\n            default_mainnet_rpc = \"https://rpc.mainnet.near.org\"\n\n        class NearAccount(Account):\n            async def view(\n                self,\n                contract_id: str,\n                method_name: str,\n                args: dict,\n                block_id: Optional[int] = None,\n                threshold: Optional[int] = None,\n                max_retries: int = 3,\n            ):\n                \"\"\"Wrapper for the view method of the Account class, adding multiple retry attempts.\n\n                Parameters\n                ----------\n                contract_id : str\n                    The ID of the contract to call.\n                method_name : str\n                    The name of the method to invoke on the contract.\n                args : dict\n                    The arguments to pass to the contract method.\n                block_id : Optional[int]\n                    The block ID to query at.\n                threshold : Optional[int]\n                    The threshold for the view function.\n                max_retries : int\n                    The maximum number of retry attempts.\n\n                Returns\n                -------\n                The result of the contract method call.\n\n                Raises\n                ------\n                Exception\n                    If all retry attempts fail, the exception is propagated.\n\n                \"\"\"\n                acc = Account(self.account_id, self.private_key, default_mainnet_rpc)\n                await acc.startup()\n                max_retries = min(max_retries, 10)\n\n                for attempt in range(1, max_retries + 1):\n                    try:\n                        # Attempt to read the contract view method\n                        return await acc.view_function(contract_id, method_name, args, block_id, threshold)\n                    except Exception as e:\n                        # Log the error message for the current attempt\n                        print(\n                            f\"Attempt {attempt}/{max_retries} to view method '{method_name}' on contract \"\n                            f\"'{contract_id}' failed with error: {e}\"\n                        )\n\n                        # If it's the last attempt, re-raise the exception\n                        if attempt == max_retries:\n                            raise\n\n            async def call(\n                self,\n                contract_id: str,\n                method_name: str,\n                args: dict,\n                gas: int = DEFAULT_ATTACHED_GAS,\n                amount: int = 0,\n                nowait: bool = False,\n                included=False,\n                max_retries: int = 1,\n            ):\n                \"\"\"Wrapper for the call method of the Account class, adding multiple retry attempts.\n\n                Parameters\n                ----------\n                contract_id : str\n                    The ID of the contract to call.\n                method_name : str\n                    The name of the method to invoke on the contract.\n                args : dict\n                    The arguments to pass to the contract method.\n                gas : int\n                    The amount of gas to attach to the call.\n                amount : int\n                    The amount of tokens to attach to the call.\n                nowait : bool\n                    If True, do not wait for the transaction to be confirmed.\n                included : bool\n                    If True, include the transaction in the block.\n                max_retries : int\n                    The maximum number of retry attempts.\n\n                Returns\n                -------\n                The result of the contract method call.\n\n                Raises\n                ------\n                Exception\n                    If all retry attempts fail, the exception is propagated.\n\n                \"\"\"\n                acc = Account(self.account_id, self.private_key, default_mainnet_rpc)\n                await acc.startup()\n                max_retries = min(max_retries, 10)\n\n                for attempt in range(1, max_retries + 1):\n                    try:\n                        # Attempt to call the contract method\n                        return await acc.function_call(contract_id, method_name, args, gas, amount, nowait, included)\n                    except Exception as e:\n                        # Log the error message for the current attempt\n                        print(\n                            f\"Attempt {attempt}/{max_retries} to call method '{method_name}' on contract \"\n                            f\"'{contract_id}' failed with error: {e}\"\n                        )\n\n                        # If it's the last attempt, re-raise the exception\n                        if attempt == max_retries:\n                            raise\n\n            async def get_balance(self, account_id: Optional[str] = None) -&gt; int:\n                \"\"\"Retrieves the balance of the specified NEAR account.\n\n                Parameters\n                ----------\n                account_id : Optional[str]\n                    The ID of the account to retrieve the balance for. If not provided, the balance of the current\n                    account is retrieved.\n\n                Returns\n                -------\n                int\n                    The balance of the specified account in yoctoNEAR.\n\n                Raises\n                ------\n                Exception\n                    If there is an error retrieving the balance.\n\n                \"\"\"\n                acc = Account(self.account_id, self.private_key, default_mainnet_rpc)\n                await acc.startup()\n                return await acc.get_balance(account_id)\n\n            def __init__(\n                self,\n                account_id: Optional[str] = None,\n                private_key: Optional[Union[List[Union[str, bytes]], str, bytes]] = None,\n                rpc_addr: Optional[str] = None,\n            ):\n                self.account_id = account_id\n                self.private_key = private_key\n                super().__init__(account_id, private_key, rpc_addr)\n\n        self.set_near = NearAccount\n\n        self._tools = ToolRegistry()\n\n        if create_files:\n            os.makedirs(self._path, exist_ok=True)\n            open(os.path.join(self._path, CHAT_FILENAME), \"a\").close()\n        os.chdir(self._path)\n\n        # Protected client methods\n        def query_vector_store(vector_store_id: str, query: str, full_files: bool = False):\n            \"\"\"Queries a vector store.\n\n            vector_store_id: The id of the vector store to query.\n            query: The query to search for.\n            \"\"\"\n            return client.query_vector_store(vector_store_id, query, full_files)\n\n        self.query_vector_store = query_vector_store\n\n        def upload_file(\n            file_content: str,\n            purpose: Literal[\"assistants\", \"batch\", \"fine-tune\", \"vision\"] = \"assistants\",\n            encoding: Optional[str] = \"utf-8\",\n            file_name: Optional[str] = \"file.txt\",\n            file_type: Optional[str] = \"text/plain\",\n        ):\n            \"\"\"Uploads a file to the registry.\"\"\"\n            return client.upload_file(\n                file_content, purpose, encoding=encoding, file_name=file_name, file_type=file_type\n            )\n\n        self.upload_file = upload_file\n\n        def remove_file(file_id: str):\n            \"\"\"Removes a file from the registry.\"\"\"\n            return client.remove_file(file_id)\n\n        self.remove_file = remove_file\n\n        def create_vector_store_from_source(\n            name: str,\n            source: Union[GitHubSource, GitLabSource],\n            source_auth: Optional[str] = None,\n            chunking_strategy: Optional[ChunkingStrategy] = None,\n            expires_after: Optional[ExpiresAfter] = None,\n            metadata: Optional[Dict[str, str]] = None,\n        ) -&gt; VectorStore:\n            \"\"\"Creates a vector store from the given source.\n\n            Args:\n            ----\n                name: The name of the vector store.\n                source: The source from which to create the vector store.\n                source_auth: The source authentication token.\n                chunking_strategy: The chunking strategy to use.\n                expires_after: The expiration policy.\n                metadata: Additional metadata.\n\n            Returns:\n            -------\n                VectorStore: The created vector store.\n\n            \"\"\"\n            return client.create_vector_store_from_source(\n                name=name,\n                source=source,\n                source_auth=source_auth,\n                chunking_strategy=chunking_strategy,\n                expires_after=expires_after,\n                metadata=metadata,\n            )\n\n        self.create_vector_store_from_source = create_vector_store_from_source\n\n        def add_file_to_vector_store(vector_store_id: str, file_id: str):\n            \"\"\"Adds a file to the vector store.\"\"\"\n            return client.add_file_to_vector_store(vector_store_id, file_id)\n\n        self.add_file_to_vector_store = add_file_to_vector_store\n\n        # positional arguments are not allowed because arguments list will be updated\n        def find_agents(*, owner_id: Optional[str] = None, with_capabilities: Optional[bool] = False):\n            \"\"\"Find agents based on various parameters.\"\"\"\n            return client.find_agents(owner_id, with_capabilities)\n\n        self.find_agents = find_agents\n\n        def create_vector_store(\n            name: str,\n            file_ids: list,\n            expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN,\n            chunking_strategy: Union[\n                AutoFileChunkingStrategyParam, StaticFileChunkingStrategyObjectParam, NotGiven\n            ] = NOT_GIVEN,\n            metadata: Optional[Dict[str, str]] = None,\n        ) -&gt; VectorStore:\n            \"\"\"Creates a vector store.\n\n            Args:\n            ----\n                name: The name of the vector store.\n                file_ids: List of file ids to create the vector store.\n                chunking_strategy: The chunking strategy to use.\n                expires_after: The expiration policy.\n                metadata: Additional metadata.\n\n            Returns:\n            -------\n                VectorStore: The created vector store.\n\n            \"\"\"\n            return client.create_vector_store(\n                name=name,\n                file_ids=file_ids,\n                chunking_strategy=chunking_strategy,\n                expires_after=expires_after,\n                metadata=metadata,\n            )\n\n        self.create_vector_store = create_vector_store\n\n        def get_vector_store(vector_store_id: str) -&gt; VectorStore:\n            \"\"\"Gets a vector store by id.\"\"\"\n            return client.get_vector_store(vector_store_id)\n\n        self.get_vector_store = get_vector_store\n\n        def get_vector_store_files(vector_store_id: str) -&gt; Optional[List[VectorStoreFile]]:\n            \"\"\"Gets a list of vector store files.\"\"\"\n            return client.get_vector_store_files(vector_store_id)\n\n        self.get_vector_store_files = get_vector_store_files\n\n        # Save cache of requested models for inference to avoid extra server calls\n        self.cached_models_for_inference: Dict[str, str] = {}\n\n        def get_model_for_inference(model: str = \"\") -&gt; str:\n            \"\"\"Returns 'provider::model_full_path'.\"\"\"\n            if self.cached_models_for_inference.get(model, None) is None:\n                provider = self.get_primary_agent().model_provider if self._agents else \"\"\n                if model == \"\":\n                    model = self.get_primary_agent().model if self._agents else \"\"\n                if model == \"\":\n                    return DEFAULT_PROVIDER_MODEL\n\n                _, model_for_inference = client.provider_models.match_provider_model(model, provider)\n\n                self.cached_models_for_inference[model] = model_for_inference\n\n            return self.cached_models_for_inference[model]\n\n        self.get_model_for_inference = get_model_for_inference\n\n        def _run_inference_completions(\n            messages: Union[Iterable[ChatCompletionMessageParam], str],\n            model: Union[Iterable[ChatCompletionMessageParam], str],\n            stream: bool,\n            **kwargs: Any,\n        ) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n            \"\"\"Run inference completions for given parameters.\"\"\"\n            params, kwargs = self.get_inference_parameters(messages, model, stream, **kwargs)\n\n            completions = client.completions(\n                params.model, params.messages, params.stream, params.temperature, params.max_tokens, **kwargs\n            )\n\n            return completions\n\n        self._run_inference_completions = _run_inference_completions\n\n        def get_agent_public_key():\n            \"\"\"Returns public key of the agent.\"\"\"\n            agent_name = self.get_primary_agent().get_full_name()\n\n            return client.get_agent_public_key(agent_name)\n\n        self.get_agent_public_key = get_agent_public_key\n\n        def run_agent(\n            agent_id: str,\n            query: Optional[str] = None,\n            thread_mode: ThreadMode = ThreadMode.FORK,\n            run_mode: RunMode = RunMode.SIMPLE,\n        ):\n            \"\"\"Runs a child agent on the thread.\"\"\"\n            child_thread_id = self._thread_id\n\n            if thread_mode == ThreadMode.SAME:\n                pass\n            elif thread_mode == ThreadMode.FORK:\n                child_thread_id = client.threads_fork(self._thread_id).id\n                self.add_system_log(f\"Forked thread {child_thread_id}\", logging.INFO)\n            elif thread_mode == ThreadMode.CHILD:\n                child_thread_id = client.create_subthread(self._thread_id).id\n                self.add_system_log(f\"Created subthread {child_thread_id}\", logging.INFO)\n\n            if query:\n                client.threads_messages_create(thread_id=child_thread_id, content=query, role=\"user\")\n\n            self.add_system_log(f\"Running agent {agent_id}\", logging.INFO)\n            client.run_agent(\n                parent_run_id=self._run_id,\n                run_on_thread_id=child_thread_id,\n                assistant_id=agent_id,\n                run_mode=run_mode,\n            )\n            self._pending_ext_agent = True\n\n            return child_thread_id\n\n        self.run_agent = run_agent\n\n        def schedule_run(\n            agent: str,\n            input_message: str,\n            run_at: datetime,\n            run_params: Optional[Dict[str, str]] = None,\n            thread_id: Optional[str] = None,\n        ):\n            \"\"\"Schedules a run.\"\"\"\n            return client.schedule_run(agent, input_message, thread_id, run_params, run_at)\n\n        self.schedule_run = schedule_run\n\n        # TODO(https://github.com/nearai/nearai/issues/549): Allow only a subset of agents to access/update user memory.\n        def add_user_memory(memory: str):\n            \"\"\"Add user memory.\"\"\"\n            return client.add_user_memory(memory)\n\n        self.add_user_memory = add_user_memory\n\n        def query_user_memory(query: str):\n            \"\"\"Query user memory.\"\"\"\n            return client.query_user_memory(query)\n\n        self.query_user_memory = query_user_memory\n\n        def generate_image(prompt: str):\n            \"\"\"Generate an image.\"\"\"\n            return client.generate_image(prompt)\n\n        self.generate_image = generate_image\n\n        def save_agent_data(key, data: Dict[str, Any]):\n            \"\"\"Save agent data.\"\"\"\n            return client.save_agent_data(key, data)\n\n        self.save_agent_data = save_agent_data\n\n        def get_agent_data():\n            \"\"\"Get agent data.\"\"\"\n            return client.get_agent_data()\n\n        self.get_agent_data = get_agent_data\n\n        def get_agent_data_by_key(key, default=None):\n            \"\"\"Get agent data by key.\"\"\"\n            namespace = self.get_primary_agent().namespace\n            name = self.get_primary_agent().name\n            result = client.get_agent_data_by_key(key)\n            return (\n                result\n                if result\n                else {\n                    \"value\": default,\n                    \"namespace\": namespace,\n                    \"key\": key,\n                    \"name\": name,\n                    \"updated_at\": \"\",\n                    \"created_at\": \"\",\n                }\n            )\n\n        self.get_agent_data_by_key = get_agent_data_by_key\n\n        # HubClient methods\n        def add_reply(\n            message: str,\n            attachments: Optional[Iterable[Attachment]] = None,\n            message_type: Optional[str] = None,\n            thread_id: str = self._thread_id,\n        ):\n            \"\"\"Assistant adds a message to the environment.\"\"\"\n            # NOTE: message from `user` are not stored in the memory\n\n            return hub_client.beta.threads.messages.create(\n                thread_id=thread_id,\n                role=\"assistant\",\n                content=message,\n                extra_body={\n                    \"assistant_id\": self.get_primary_agent().identifier,\n                    \"run_id\": self._run_id,\n                },\n                attachments=attachments,\n                metadata={\"message_type\": message_type} if message_type else None,\n            )\n\n        self.add_reply = add_reply\n\n        def get_thread(thread_id=self._thread_id):\n            \"\"\"Returns the current Thread object or the requested Thread.\"\"\"\n            return client.get_thread(thread_id)\n\n        self.get_thread = get_thread\n\n        def _add_message(\n            role: str,\n            message: str,\n            attachments: Optional[Iterable[Attachment]] = None,\n            **kwargs: Any,\n        ):\n            return hub_client.beta.threads.messages.create(\n                thread_id=self._thread_id,\n                role=role,  # type: ignore\n                content=message,\n                extra_body={\n                    \"assistant_id\": self.get_primary_agent().identifier,\n                    \"run_id\": self._run_id,\n                },\n                metadata=kwargs,\n                attachments=attachments,\n            )\n\n        self._add_message = _add_message\n\n        def _list_messages(\n            limit: Union[int, NotGiven] = NOT_GIVEN,\n            order: Literal[\"asc\", \"desc\"] = \"asc\",\n            thread_id: Optional[str] = None,\n        ) -&gt; List[Message]:\n            \"\"\"Returns messages from the environment.\"\"\"\n            messages = hub_client.beta.threads.messages.list(\n                thread_id=thread_id or self._thread_id, limit=limit, order=order\n            )\n            self.add_system_log(f\"Retrieved {len(messages.data)} messages from NEAR AI Hub\")\n            return messages.data\n\n        self._list_messages = _list_messages\n\n        def list_files_from_thread(\n            order: Literal[\"asc\", \"desc\"] = \"asc\", thread_id: Optional[str] = None\n        ) -&gt; List[FileObject]:\n            \"\"\"Lists files in the thread.\"\"\"\n            messages = self._list_messages(order=order)\n            # Extract attachments from messages\n            attachments = [a for m in messages if m.attachments for a in m.attachments]\n            # Extract files from attachments\n            file_ids = [a.file_id for a in attachments]\n            files = [hub_client.files.retrieve(f) for f in file_ids if f]\n            return files\n\n        self.list_files_from_thread = list_files_from_thread\n\n        def read_file_by_id(file_id: str):\n            \"\"\"Read a file from the thread.\"\"\"\n            content = hub_client.files.content(file_id).content.decode(\"utf-8\")\n            print(\"file content returned by api\", content)\n            return content\n\n        self.read_file_by_id = read_file_by_id\n\n        def write_file(\n            filename: str,\n            content: Union[str, bytes],\n            encoding: str = \"utf-8\",\n            filetype: str = \"text/plain\",\n            write_to_disk: bool = True,\n        ) -&gt; FileObject:\n            \"\"\"Writes a file to the environment.\n\n            filename: The name of the file to write to\n            content: The content to write to the file\n            encoding: The encoding to use when writing the file (default is utf-8)\n            filetype: The MIME type of the file (default is text/plain)\n            write_to_disk: If True, write locally to disk (default is True)\n            \"\"\"\n            if write_to_disk:\n                # Write locally\n                path = Path(self.get_primary_agent_temp_dir()) / filename\n                path.parent.mkdir(parents=True, exist_ok=True)\n                if isinstance(content, bytes):\n                    with open(path, \"wb\") as f:\n                        f.write(content)\n                else:\n                    with open(path, \"w\", encoding=encoding) as f:\n                        f.write(content)\n\n            if isinstance(content, bytes):\n                file_data = content\n            else:\n                file_data = io.BytesIO(content.encode(encoding))  # type:ignore\n\n            # Upload to Hub\n            file = hub_client.files.create(file=(filename, file_data, filetype), purpose=\"assistants\")\n            res = self.add_reply(\n                message=f\"Successfully wrote {len(content) if content else 0} characters to {filename}\",\n                attachments=[{\"file_id\": file.id, \"tools\": [{\"type\": \"file_search\"}]}],\n                message_type=\"system:file_write\",\n            )\n            self.add_system_log(\n                f\"Uploaded file {filename} with {len(content)} characters, id: {file.id}. Added in thread as: {res.id}\"\n            )\n            return file\n\n        self.write_file = write_file\n\n        def mark_done() -&gt; Run:  # noqa: D102\n            self._done = True\n            res = hub_client.beta.threads.runs.update(\n                thread_id=self._thread_id,\n                run_id=self._run_id,\n                extra_body={\n                    \"status\": \"completed\",\n                    \"completed_at\": datetime.now().isoformat(),\n                },\n            )\n            return res\n\n        self.mark_done = mark_done\n\n        def mark_failed() -&gt; Run:\n            \"\"\"Marks the environment run as failed.\"\"\"\n            self._done = True\n            self.add_system_log(\"Environment run failed\", logging.ERROR)\n            res = hub_client.beta.threads.runs.update(\n                thread_id=self._thread_id,\n                run_id=self._run_id,\n                extra_body={\"status\": \"failed\", \"failed_at\": datetime.now().isoformat()},\n            )\n            return res\n\n        self.mark_failed = mark_failed\n\n        def request_user_input() -&gt; Run:\n            \"\"\"Must be called to request input from the user.\"\"\"\n            return hub_client.beta.threads.runs.update(\n                thread_id=self._thread_id,\n                run_id=self._run_id,\n                extra_body={\"status\": \"requires_action\", \"required_action\": {\"type\": \"user_input\"}},\n            )\n\n        self.request_user_input = request_user_input\n\n        def request_agent_input() -&gt; Run:\n            \"\"\"Mark the run as ready for input from another agent.\"\"\"\n            return hub_client.beta.threads.runs.update(\n                thread_id=self._thread_id,\n                run_id=self._run_id,\n                extra_body={\"status\": \"requires_action\", \"required_action\": {\"type\": \"agent_input\"}},\n            )\n\n        self.request_agent_input = request_agent_input\n\n        # Must be placed after method definitions\n        self.register_standard_tools()\n\n    # end of protected client methods\n\n    def get_tool_registry(self, new: bool = False) -&gt; ToolRegistry:\n        \"\"\"Returns the tool registry, a dictionary of tools that can be called by the agent.\"\"\"\n        if new:\n            self._tools = ToolRegistry()\n        return self._tools\n\n    def register_standard_tools(self) -&gt; None:  # noqa: D102\n        reg = self.get_tool_registry()\n        reg.register_tool(self.exec_command)\n        reg.register_tool(self.read_file)\n        reg.register_tool(self.write_file)\n        reg.register_tool(self.request_user_input)\n        reg.register_tool(self.list_files)\n        reg.register_tool(self.query_vector_store)\n\n    def get_last_message(self, role: str = \"user\"):\n        \"\"\"Reads last message from the given role and returns it.\"\"\"\n        for message in reversed(self.list_messages()):\n            if message.get(\"role\") == role:\n                return message\n\n        return None\n\n    def add_message(\n        self,\n        role: str,\n        message: str,\n        attachments: Optional[Iterable[Attachment]] = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Deprecated. Please use `add_reply` instead. Assistant adds a message to the environment.\"\"\"\n        # Prevent agent to save messages on behalf of `user` to avoid adding false memory\n        role = \"assistant\"\n\n        return self._add_message(role, message, attachments, **kwargs)\n\n    def add_system_log(self, log: str, level: int = logging.INFO) -&gt; None:\n        \"\"\"Add system log with timestamp and log level.\"\"\"\n        logger = logging.getLogger(\"system_logger\")\n        if not logger.handlers:\n            # Configure the logger if it hasn't been set up yet\n            logger.setLevel(logging.DEBUG)\n            file_handler = logging.FileHandler(os.path.join(self._path, SYSTEM_LOG_FILENAME))\n            formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n            file_handler.setFormatter(formatter)\n            logger.addHandler(file_handler)\n\n            if self.print_system_log:\n                console_handler = logging.StreamHandler()\n                console_handler.setFormatter(formatter)\n                logger.addHandler(console_handler)\n\n            # Add Thread log handler\n            if self._debug_mode:\n                custom_handler = CustomLogHandler(self.add_reply, \"system\")\n                custom_handler.setFormatter(formatter)\n                logger.addHandler(custom_handler)\n\n        # Log the message\n        logger.log(level, log)\n        # Force the handler to write to disk\n        for handler in logger.handlers:\n            handler.flush()\n\n    def add_agent_log(self, log: str, level: int = logging.INFO) -&gt; None:\n        \"\"\"Add agent log with timestamp and log level.\"\"\"\n        logger = logging.getLogger(\"agent_logger\")\n        if not logger.handlers:\n            # Configure the logger if it hasn't been set up yet\n            logger.setLevel(logging.DEBUG)\n            file_handler = logging.FileHandler(os.path.join(self._path, AGENT_LOG_FILENAME))\n            formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n            file_handler.setFormatter(formatter)\n            logger.addHandler(file_handler)\n\n            # Add Thread log handler\n            if self._debug_mode:\n                custom_handler = CustomLogHandler(self.add_reply, \"agent\")\n                custom_handler.setFormatter(formatter)\n                logger.addHandler(custom_handler)\n\n        # Log the message\n        logger.log(level, log)\n        # Force the handler to write to disk\n        for handler in logger.handlers:\n            handler.flush()\n\n    def add_agent_start_system_log(self, agent_idx: int) -&gt; None:\n        \"\"\"Adds agent start system log.\"\"\"\n        agent = self._agents[agent_idx]\n        message = f\"Running agent {agent.name}\"\n        if agent.model != \"\":\n            model = self.get_model_for_inference(agent.model)\n            self._last_used_model = model\n            message += f\" that will connect to {model}\"\n            if agent.model_temperature:\n                message += f\", temperature={agent.model_temperature}\"\n            if agent.model_max_tokens:\n                message += f\", max_tokens={agent.model_max_tokens}\"\n        self.add_system_log(message)\n\n    def list_terminal_commands(self, filename: str = TERMINAL_FILENAME) -&gt; List[Any]:\n        \"\"\"Returns the terminal commands from the terminal file.\"\"\"\n        path = os.path.join(self._path, filename)\n\n        if not os.path.exists(path):\n            return []\n\n        with open(path, \"r\") as f:\n            return [json.loads(message) for message in f.read().split(DELIMITER) if message]\n\n    def list_messages(\n        self,\n        thread_id: Optional[str] = None,\n        limit: Union[int, NotGiven] = 200,  # api defaults to 20\n        order: Literal[\"asc\", \"desc\"] = \"asc\",\n    ):\n        \"\"\"Backwards compatibility for chat_completions messages.\"\"\"\n        messages = self._list_messages(thread_id=thread_id, limit=limit, order=order)\n\n        # Filter out system and agent log messages when running in debug mode. Agent behavior shouldn't change based on logs.  # noqa: E501\n        messages = [\n            m\n            for m in messages\n            if not (\n                m.metadata\n                and any(m.metadata.get(\"message_type\", \"\").startswith(prefix) for prefix in [\"system:\", \"agent:\"])\n            )\n        ]\n\n        legacy_messages = [\n            {\n                \"id\": m.id,\n                \"content\": \"\\n\".join([c.text.value for c in m.content]),  # type: ignore\n                \"role\": m.role,\n            }\n            for m in messages\n        ]\n        return legacy_messages\n\n    def verify_message(\n        self,\n        account_id: str,\n        public_key: str,\n        signature: str,\n        message: str,\n        nonce: str,\n        callback_url: str,\n    ) -&gt; near.SignatureVerificationResult:\n        \"\"\"Verifies that the user message is signed with NEAR Account.\"\"\"\n        return near.verify_signed_message(\n            account_id,\n            public_key,\n            signature,\n            message,\n            nonce,\n            self.get_primary_agent().name,\n            callback_url,\n        )\n\n    def list_files(self, path: str, order: Literal[\"asc\", \"desc\"] = \"asc\") -&gt; List[str]:\n        \"\"\"Lists files in the environment.\"\"\"\n        return os.listdir(os.path.join(self.get_primary_agent_temp_dir(), path))\n\n    def get_system_path(self) -&gt; Path:\n        \"\"\"Returns the system path where chat.txt &amp; system_log are stored.\"\"\"\n        return Path(self._path)\n\n    def get_agent_temp_path(self) -&gt; Path:\n        \"\"\"Returns temp dir for primary agent where execution happens.\"\"\"\n        return self.get_primary_agent_temp_dir()\n\n    def read_file(self, filename: str) -&gt; Optional[Union[bytes, str]]:\n        \"\"\"Reads a file from the environment or thread.\"\"\"\n        file_content: Optional[Union[bytes, str]] = None\n        # First try to read from local filesystem\n        local_path = os.path.join(self.get_primary_agent_temp_dir(), filename)\n        if os.path.exists(local_path):\n            try:\n                with open(local_path, \"rb\") as local_path_file:\n                    local_file_content = local_path_file.read()\n                    try:\n                        # Try to decode as text\n                        file_content = local_file_content.decode(\"utf-8\")\n                    except UnicodeDecodeError:\n                        # If decoding fails, store as binary\n                        file_content = local_file_content\n            except Exception as e:\n                print(f\"Error with read_file: {e}\")\n\n        if not file_content:\n            # Next check files written out by the agent.\n            # Agent output files take precedence over files packaged with the agent\n            thread_files = self.list_files_from_thread(order=\"desc\")\n\n            # Then try to read from thread, starting from the most recent\n            for f in thread_files:\n                if f.filename == filename:\n                    file_content = self.read_file_by_id(f.id)\n                    break\n\n            if not file_content:\n                # Next check agent file cache\n                # Agent output files &amp; thread files take precedence over cached files\n                file_cache = self.get_primary_agent().file_cache\n                if file_cache:\n                    file_content = file_cache.get(filename, None)\n\n            # Write the file content from the thread or cache to the local filesystem\n            # This allows exec_command to operate on the file\n            if file_content:\n                if not os.path.exists(os.path.dirname(local_path)):\n                    os.makedirs(os.path.dirname(local_path))\n\n                with open(local_path, \"wb\") as local_file:\n                    if isinstance(file_content, bytes):\n                        local_file.write(file_content)\n                    else:\n                        local_file.write(file_content.encode(\"utf-8\"))\n\n        if not file_content:\n            self.add_system_log(f\"Warn: File {filename} not found during read_file operation\")\n\n        return file_content\n\n    def exec_command(self, command: str) -&gt; Dict[str, Union[str, int]]:\n        \"\"\"Executes a command in the environment and logs the output.\n\n        The environment does not allow running interactive programs.\n        It will run a program for 1 second then will interrupt it if it is still running\n        or if it is waiting for user input.\n        command: The command to execute, like 'ls -l' or 'python3 tests.py'\n        \"\"\"\n        approval_function = self._approvals[\"confirm_execution\"] if self._approvals else None\n        if not approval_function:\n            return {\n                \"stderr\": \"Agent runner misconfiguration. No command execution approval function found.\",\n            }\n        if not approval_function(command):\n            return {\n                \"command\": command,\n                \"returncode\": 999,\n                \"stdout\": \"\",\n                \"stderr\": \"Command execution was not approved.\",\n            }\n\n        try:\n            process = subprocess.Popen(\n                shlex.split(command),\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                bufsize=0,\n                universal_newlines=True,\n                cwd=self._path,\n            )\n        except Exception as e:\n            return {\n                \"command\": command,\n                \"returncode\": 999,\n                \"stdout\": \"\",\n                \"stderr\": \"Failed to execute: \" + str(e),\n            }\n\n        msg = \"\"\n\n        def kill_process_tree(p: Any) -&gt; None:\n            nonlocal msg\n            msg = \"Killing process due to timeout\"\n\n            process = psutil.Process(p.pid)\n            for proc in process.children(recursive=True):\n                proc.kill()\n            process.kill()\n\n        timer = threading.Timer(2, kill_process_tree, (process,))\n        timer.start()\n        process.wait()\n        timer.cancel()\n\n        result = {\n            \"command\": command,\n            \"stdout\": process.stdout.read() if process.stdout and hasattr(process.stdout, \"read\") else \"\",\n            \"stderr\": process.stderr.read() if process.stderr and hasattr(process.stderr, \"read\") else \"\",\n            \"returncode\": process.returncode,\n            \"msg\": msg,\n        }\n        with open(os.path.join(self._path, TERMINAL_FILENAME), \"a\") as f:\n            f.write(json.dumps(result) + DELIMITER)\n        return result\n\n    def get_inference_parameters(\n        self,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str],\n        stream: bool,\n        **kwargs: Any,\n    ) -&gt; Tuple[InferenceParameters, Any]:\n        \"\"\"Run inference parameters to run completions.\"\"\"\n        if isinstance(messages, str):\n            self.add_system_log(\n                \"Deprecated completions call. Pass `messages` as a first parameter.\",\n                logging.WARNING,\n            )\n            messages_or_model = messages\n            model_or_messages = model\n            model = cast(str, messages_or_model)\n            messages = cast(Iterable[ChatCompletionMessageParam], model_or_messages)\n        else:\n            model = cast(str, model)\n            messages = cast(Iterable[ChatCompletionMessageParam], messages)\n        model = self.get_model_for_inference(model)\n        if model != self._last_used_model:\n            self._last_used_model = model\n            self.add_system_log(f\"Connecting to {model}\")\n\n        temperature = kwargs.pop(\"temperature\", self.get_primary_agent().model_temperature if self._agents else None)\n        max_tokens = kwargs.pop(\"max_tokens\", self.get_primary_agent().model_max_tokens if self._agents else None)\n\n        params = InferenceParameters(\n            model=model,\n            messages=messages,\n            stream=stream,\n            temperature=temperature,\n            max_tokens=max_tokens,\n        )\n\n        return params, kwargs\n\n    # TODO(286): `messages` may be model and `model` may be messages temporarily to support deprecated API.\n    def completions(\n        self,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n        stream: bool = False,\n        **kwargs: Any,\n    ) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"Returns all completions for given messages using the given model.\"\"\"\n        return self._run_inference_completions(messages, model, stream, **kwargs)\n\n    def verify_signed_message(\n        self,\n        completion: str,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        public_key: Union[str, None] = None,\n        signature: Union[str, None] = None,\n        model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n        **kwargs: Any,\n    ) -&gt; bool:\n        \"\"\"Verifies a signed message.\"\"\"\n        if public_key is None or signature is None:\n            return False\n\n        params, _ = self.get_inference_parameters(messages, model, False, **kwargs)\n\n        messages_without_ids = [{k: v for k, v in item.items() if k != \"id\"} for item in params.messages]\n        ordered_messages_without_ids = [\n            {\"role\": str(item[\"role\"]), \"content\": str(item[\"content\"])} for item in messages_without_ids\n        ]\n\n        return validate_completion_signature(\n            public_key,\n            signature,\n            CompletionSignaturePayload(\n                agent_name=self.get_primary_agent().get_full_name(),\n                completion=completion,\n                model=params.model,\n                messages=ordered_messages_without_ids,\n                temperature=params.temperature,\n                max_tokens=params.max_tokens,\n            ),\n        )\n\n    def completions_and_run_tools(\n        self,\n        messages: List[ChatCompletionMessageParam],\n        model: str = \"\",\n        tools: Optional[List] = None,\n        add_responses_to_messages: bool = True,\n        agent_role_name=\"assistant\",\n        tool_role_name=\"tool\",\n        **kwargs: Any,\n    ) -&gt; ModelResponse:\n        \"\"\"Returns all completions for given messages using the given model and runs tools.\"\"\"\n        if self._use_llama_tool_syntax(model, tools):\n            tool_prompt = self._llama_tool_prompt(tools)\n            messages.append({\"role\": \"system\", \"content\": tool_prompt})\n        raw_response = self._run_inference_completions(messages, model, stream=False, tools=tools, **kwargs)\n        assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n        response: ModelResponse = raw_response\n        assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n        choices: List[Choices] = response.choices  # type: ignore\n        response_message = choices[0].message\n\n        self._handle_tool_calls(response_message, add_responses_to_messages, agent_role_name, tool_role_name)\n\n        return response\n\n    def _handle_tool_calls(\n        self,\n        response_message,\n        add_responses_to_messages,\n        agent_role_name,\n        tool_role_name,\n    ):\n        (message_without_tool_call, tool_calls) = self._parse_tool_call(response_message)\n        if add_responses_to_messages and response_message.content:\n            self.add_message(agent_role_name, message_without_tool_call)\n        if tool_calls:\n            for tool_call in tool_calls:\n                function_name = tool_call.function.name\n                try:\n                    assert function_name, \"Tool call must have a function name\"\n                    function_signature = self.get_tool_registry().get_tool_definition(function_name)\n                    assert function_signature, f\"Tool {function_name} not found\"\n                    args = tool_call.function.arguments\n                    function_args = tool_json_helper.parse_json_args(function_signature, args)\n                    self.add_system_log(f\"Calling tool {function_name} with args {function_args}\")\n                    function_response = self._tools.call_tool(function_name, **function_args if function_args else {})\n\n                    if function_response:\n                        try:\n                            function_response_json = json.dumps(function_response) if function_response else \"\"\n                            if add_responses_to_messages:\n                                self.add_message(\n                                    tool_role_name,\n                                    function_response_json,\n                                    tool_call_id=tool_call.id,\n                                    name=function_name,\n                                )\n                        except Exception as e:\n                            # some tool responses may not be serializable\n                            error_message = f\"Unable to add tool output as a message {function_name}: {e}\"\n                            self.add_system_log(error_message, level=logging.INFO)\n                except Exception as e:\n                    error_message = f\"Error calling tool {function_name}: {e}\"\n                    self.add_system_log(error_message, level=logging.ERROR)\n                    if add_responses_to_messages:\n                        self.add_message(\n                            tool_role_name,\n                            error_message,\n                            tool_call_id=tool_call.id,\n                            name=function_name,\n                        )\n\n    @staticmethod\n    def _parse_tool_call(\n        response_message,\n    ) -&gt; Tuple[Optional[str], Optional[List[ChatCompletionMessageToolCall]]]:\n        if hasattr(response_message, \"tool_calls\") and response_message.tool_calls:\n            return response_message.content, response_message.tool_calls\n        if \"content\" not in response_message or response_message.content is None:\n            return None, None\n        content = response_message.content\n        llama_matches = LLAMA_TOOL_FORMAT_PATTERN.findall(content)\n        if llama_matches:\n            text = \"\"\n            tool_calls = []\n            for llama_match in llama_matches:\n                before_call_text, function_name, args, end_tag, after_call_text = llama_match\n                function = Function(name=function_name, arguments=args)\n                tool_call = ChatCompletionMessageToolCall(id=str(uuid.uuid4()), function=function)\n                text += before_call_text + after_call_text\n                tool_calls.append(tool_call)\n            return text, tool_calls\n\n        llama_matches = LLAMA_TOOL_FORMAT_PATTERN2.findall(content)\n        if llama_matches:\n            text = \"\"\n            tool_calls = []\n            for llama_match in llama_matches:\n                before_call_text, function_name_and_args, after_call_text = llama_match\n                try:\n                    parsed_function_name_and_args = json.loads(function_name_and_args)\n                    function_name = parsed_function_name_and_args.get(\"name\")\n                    args = parsed_function_name_and_args.get(\"arguments\")\n                    function = Function(name=function_name, arguments=args)\n                    tool_call = ChatCompletionMessageToolCall(id=str(uuid.uuid4()), function=function)\n                    text += before_call_text + after_call_text\n                    tool_calls.append(tool_call)\n                except json.JSONDecodeError:\n                    print(f\"Error parsing tool_call function name and args: {function_name_and_args}\")\n                    continue\n            return text, tool_calls\n\n        return content, None\n\n    @staticmethod\n    def _use_llama_tool_syntax(model: str, tools: Optional[List]) -&gt; bool:\n        return tools is not None and \"llama\" in model\n\n    @staticmethod\n    def _llama_tool_prompt(tools: Optional[List]) -&gt; str:\n        return (\n            \"\"\"Answer the user's question by making use of the following functions if needed.\n            If none of the function can be used, please say so.\n            Here is a list of functions in JSON format:\"\"\"\n            + json.dumps(tools)\n            + \"\"\"Think very carefully before calling functions.\n            If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n\n            &lt;function=example_function_name&gt;{\"example_name\": \"example_value\"}&lt;/function&gt;\n\n            Reminder:\n            - Function calls MUST follow the specified format, start with &lt;function= and end with &lt;/function&gt;\n            - Function arguments MUST be in JSON format using double quotes\n            - Required parameters MUST be specified\n            - Multiple functions can be called in one message as long as they are on separate lines.\n            - Put the entire function call reply on one line\n        \"\"\"\n        )\n\n    # TODO(286): `messages` may be model and `model` may be messages temporarily to support deprecated API.\n    def completion(\n        self,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Returns a completion for the given messages using the given model.\"\"\"\n        raw_response = self.completions(messages, model, **kwargs)\n        assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n        response: ModelResponse = raw_response\n        assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n        choices: List[Choices] = response.choices  # type: ignore\n        response_message = choices[0].message.content\n        assert response_message, \"No completions returned\"\n        return response_message\n\n    def signed_completion(\n        self,\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n        **kwargs: Any,\n    ) -&gt; Dict[str, str]:\n        \"\"\"Returns a completion for the given messages using the given model with the agent signature.\"\"\"\n        # TODO Return signed completions for non-latest versions only?\n        agent_name = self.get_primary_agent().get_full_name()\n        raw_response = self.completions(messages, model, agent_name=agent_name, **kwargs)\n        assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n        response: ModelResponse = raw_response\n\n        signature_data = json.loads(response.system_fingerprint) if response.system_fingerprint else {}\n\n        assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n        choices: List[Choices] = response.choices  # type: ignore\n        response_message = choices[0].message.content\n        assert response_message, \"No completions returned\"\n\n        return {\n            \"response\": response_message,\n            \"signature\": signature_data.get(\"signature\", None),\n            \"public_key\": signature_data.get(\"public_key\", None),\n        }\n\n    def completion_and_get_tools_calls(\n        self,\n        messages: List[ChatCompletionMessageParam],\n        model: str = \"\",\n        **kwargs: Any,\n    ) -&gt; SimpleNamespace:\n        \"\"\"Returns completion message and/or tool calls from OpenAI or Llama tool formats.\"\"\"\n        raw_response = self._run_inference_completions(messages, model, stream=False, **kwargs)\n\n        assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n        response: ModelResponse = raw_response\n        assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n        choices: List[Choices] = response.choices  # type: ignore\n\n        (message_without_tool_call, tool_calls) = self._parse_tool_call(choices[0].message)\n\n        if message_without_tool_call is None:\n            response_message = choices[0].message.content\n            message_without_tool_call = response_message\n\n        return SimpleNamespace(message=message_without_tool_call, tool_calls=tool_calls)\n\n    def completion_and_run_tools(\n        self,\n        messages: List[ChatCompletionMessageParam],\n        model: str = \"\",\n        tools: Optional[List] = None,\n        **kwargs: Any,\n    ) -&gt; Optional[str]:\n        \"\"\"Returns a completion for the given messages using the given model and runs tools.\"\"\"\n        completion_tools_response = self.completions_and_run_tools(messages, model, tools, **kwargs)\n        assert all(\n            map(\n                lambda choice: isinstance(choice, Choices),\n                completion_tools_response.choices,\n            )\n        ), \"Expected Choices\"\n        choices: List[Choices] = completion_tools_response.choices  # type: ignore\n        response_content = choices[0].message.content\n        return response_content\n\n    def call_agent(self, agent_index: int, task: str) -&gt; None:\n        \"\"\"Calls agent with given task.\"\"\"\n        self._agents[agent_index].run(self, task=task)\n\n    def get_agents(self) -&gt; List[Agent]:\n        \"\"\"Returns list of agents available in environment.\"\"\"\n        return self._agents\n\n    def get_primary_agent(self) -&gt; Agent:\n        \"\"\"Returns the agent that is invoked first.\"\"\"\n        return self._agents[0]\n\n    def get_primary_agent_temp_dir(self) -&gt; Path:\n        \"\"\"Returns temp dir for primary agent.\"\"\"\n        return self.get_primary_agent().temp_dir\n\n    def is_done(self) -&gt; bool:  # noqa: D102\n        return self._done\n\n    def create_snapshot(self) -&gt; bytes:\n        \"\"\"Create an in memory snapshot.\"\"\"\n        with tempfile.NamedTemporaryFile(suffix=\".tar.gz\") as f:\n            with tarfile.open(fileobj=f, mode=\"w:gz\") as tar:\n                tar.add(self._path, arcname=\".\")\n            f.flush()\n            f.seek(0)\n            snapshot = f.read()\n        return snapshot\n\n    def environment_run_info(self, base_id, run_type) -&gt; dict:\n        \"\"\"Returns the environment run information.\"\"\"\n        if not self._agents or not self.get_primary_agent():\n            raise ValueError(\"Agent not found\")\n        primary_agent = self.get_primary_agent()\n\n        full_agent_name = \"/\".join([primary_agent.namespace, primary_agent.name, primary_agent.version])\n        safe_agent_name = full_agent_name.replace(\"/\", \"_\")\n        uid = uuid.uuid4().hex\n        generated_name = f\"environment_run_{safe_agent_name}_{uid}\"\n        name = generated_name\n\n        timestamp = datetime.now(timezone.utc).isoformat()\n        return {\n            \"name\": name,\n            \"version\": \"0\",\n            \"description\": f\"Agent {run_type} {full_agent_name} {uid} {timestamp}\",\n            \"category\": \"environment\",\n            \"tags\": [\"environment\"],\n            \"details\": {\n                \"base_id\": base_id,\n                \"timestamp\": timestamp,\n                \"agents\": [agent.name for agent in self._agents],\n                \"primary_agent_namespace\": primary_agent.namespace,\n                \"primary_agent_name\": primary_agent.name,\n                \"primary_agent_version\": primary_agent.version,\n                \"run_id\": self._run_id,\n                \"run_type\": run_type,\n            },\n            \"show_entry\": True,\n        }\n\n    def load_snapshot(self, snapshot: bytes) -&gt; None:\n        \"\"\"Load Environment from Snapshot.\"\"\"\n        shutil.rmtree(self._path, ignore_errors=True)\n\n        with tempfile.NamedTemporaryFile(suffix=\".tar.gz\") as f:\n            f.write(snapshot)\n            f.flush()\n            f.seek(0)\n\n            with tarfile.open(fileobj=f, mode=\"r:gz\") as tar:\n                tar.extractall(self._path)\n\n    def __str__(self) -&gt; str:  # noqa: D105\n        return f\"Environment({self._path})\"\n\n    def clear_temp_agent_files(self, verbose=True) -&gt; None:\n        \"\"\"Remove temp agent files created to be used in `runpy`.\"\"\"\n        for agent in self._agents:\n            if os.path.exists(agent.temp_dir):\n                if verbose:\n                    print(\"removed agent.temp_files\", agent.temp_dir)\n                shutil.rmtree(agent.temp_dir)\n\n    def set_next_actor(self, who: str) -&gt; None:\n        \"\"\"Set the next actor / action in the dialogue.\"\"\"\n        next_action_fn = os.path.join(self._path, \".next_action\")\n        if who == \"agent\":\n            self._done = False\n\n        with open(next_action_fn, \"w\") as f:\n            f.write(who)\n\n    def get_next_actor(self) -&gt; str:  # noqa: D102\n        next_action_fn = os.path.join(self._path, \".next_action\")\n\n        if os.path.exists(next_action_fn):\n            with open(next_action_fn) as f:\n                return f.read().strip(\" \\n\")\n        else:\n            # By default the user starts the conversation.\n            return \"user\"\n\n    def run(\n        self,\n        new_message: Optional[str] = None,\n        max_iterations: int = 10,\n    ) -&gt; None:\n        \"\"\"Runs agent(s) against a new or previously created environment.\"\"\"\n        if new_message:\n            self._add_message(\"user\", new_message)\n\n        iteration = 0\n        self.set_next_actor(\"agent\")\n\n        while iteration &lt; max_iterations and not self.is_done() and self.get_next_actor() != \"user\":\n            iteration += 1\n            if max_iterations &gt; 1:\n                self.add_system_log(\n                    f\"Running agent, iteration {iteration}/{max_iterations}\",\n                    logging.INFO,\n                )\n            try:\n                error_message, traceback_message = self.get_primary_agent().run(self, task=new_message)\n                if self._debug_mode and (error_message or traceback_message):\n                    if self._debug_mode and (error_message or traceback_message):\n                        message_parts = []\n\n                        if error_message:\n                            message_parts.append(f\"Error: \\n ```\\n{error_message}\\n```\")\n\n                        if traceback_message:\n                            message_parts.append(f\"Error Traceback: \\n ```\\n{traceback_message}\\n```\")\n\n                        self.add_reply(\"\\n\\n\".join(message_parts), message_type=\"system:debug\")\n\n            except Exception as e:\n                self.add_system_log(f\"Environment run failed: {e}\", logging.ERROR)\n                self.mark_failed()\n                raise e\n\n        if not self._pending_ext_agent:\n            # If no external agent was called, mark the whole run as done.\n            # Else this environment will stop for now but this run will be continued later.\n            self.mark_done()\n\n    def generate_folder_hash_id(self, path: str) -&gt; str:\n        \"\"\"Returns hash based on files and their contents in path, including subfolders.\"\"\"  # noqa: E501\n        hash_obj = hashlib.md5()\n\n        for root, _dirs, files in os.walk(path):\n            for file in sorted(files):\n                file_path = os.path.join(root, file)\n                with open(file_path, \"rb\") as f:\n                    while chunk := f.read(8192):\n                        hash_obj.update(chunk)\n\n        return hash_obj.hexdigest()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.__init__","title":"__init__","text":"<pre><code>__init__(path: str, agents: List[Agent], client: InferenceClient, hub_client: OpenAI, thread_id: str, run_id: str, create_files: bool = True, env_vars: Optional[Dict[str, Any]] = None, tool_resources: Optional[Dict[str, Any]] = None, print_system_log: bool = False, agent_runner_user: Optional[str] = None, fastnear_api_key: Optional[str] = None, approvals=None) -&gt; None\n</code></pre> Source code in <code>nearai/agents/environment.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    path: str,\n    agents: List[Agent],\n    client: InferenceClient,\n    hub_client: OpenAI,\n    thread_id: str,\n    run_id: str,\n    create_files: bool = True,\n    env_vars: Optional[Dict[str, Any]] = None,\n    tool_resources: Optional[Dict[str, Any]] = None,\n    print_system_log: bool = False,\n    agent_runner_user: Optional[str] = None,\n    fastnear_api_key: Optional[str] = None,\n    approvals=None,\n) -&gt; None:\n    # Warning: never expose `client` or `_hub_client` to agent's environment\n\n    self.base_url = client._config.base_url\n\n    # user_auth is used to authenticate the user in the ts_runner. It will be removed after that in\n    # `nearai/agents/agent.py`\n    self.user_auth = client._auth\n\n    # Placeholder for solver\n    self.client: Optional[InferenceClient] = None\n\n    self._path = path\n    self._agents = agents\n    self._done = False\n    self._pending_ext_agent = False\n    self.env_vars: Dict[str, Any] = env_vars if env_vars else {}\n    self._last_used_model = \"\"\n    self.tool_resources: Dict[str, Any] = tool_resources if tool_resources else {}\n    self.print_system_log = print_system_log\n    self.agent_runner_user = agent_runner_user\n    self._approvals = approvals if approvals else default_approvals\n    self._thread_id = thread_id\n    self._run_id = run_id\n    self._debug_mode: bool = any(\n        str(value).lower() in (\"true\", \"1\", \"yes\", \"on\")\n        for key, value in self.env_vars.items()\n        if key.lower() == \"debug\"\n    )\n    # Expose the NEAR account_id of a user that signs this request to run an agent.\n    self.signer_account_id: str = client._config.auth.account_id if client._config.auth else \"\"\n\n    if fastnear_api_key:\n        default_mainnet_rpc = f\"https://rpc.mainnet.fastnear.com?apiKey={fastnear_api_key}\"\n    else:\n        default_mainnet_rpc = \"https://rpc.mainnet.near.org\"\n\n    class NearAccount(Account):\n        async def view(\n            self,\n            contract_id: str,\n            method_name: str,\n            args: dict,\n            block_id: Optional[int] = None,\n            threshold: Optional[int] = None,\n            max_retries: int = 3,\n        ):\n            \"\"\"Wrapper for the view method of the Account class, adding multiple retry attempts.\n\n            Parameters\n            ----------\n            contract_id : str\n                The ID of the contract to call.\n            method_name : str\n                The name of the method to invoke on the contract.\n            args : dict\n                The arguments to pass to the contract method.\n            block_id : Optional[int]\n                The block ID to query at.\n            threshold : Optional[int]\n                The threshold for the view function.\n            max_retries : int\n                The maximum number of retry attempts.\n\n            Returns\n            -------\n            The result of the contract method call.\n\n            Raises\n            ------\n            Exception\n                If all retry attempts fail, the exception is propagated.\n\n            \"\"\"\n            acc = Account(self.account_id, self.private_key, default_mainnet_rpc)\n            await acc.startup()\n            max_retries = min(max_retries, 10)\n\n            for attempt in range(1, max_retries + 1):\n                try:\n                    # Attempt to read the contract view method\n                    return await acc.view_function(contract_id, method_name, args, block_id, threshold)\n                except Exception as e:\n                    # Log the error message for the current attempt\n                    print(\n                        f\"Attempt {attempt}/{max_retries} to view method '{method_name}' on contract \"\n                        f\"'{contract_id}' failed with error: {e}\"\n                    )\n\n                    # If it's the last attempt, re-raise the exception\n                    if attempt == max_retries:\n                        raise\n\n        async def call(\n            self,\n            contract_id: str,\n            method_name: str,\n            args: dict,\n            gas: int = DEFAULT_ATTACHED_GAS,\n            amount: int = 0,\n            nowait: bool = False,\n            included=False,\n            max_retries: int = 1,\n        ):\n            \"\"\"Wrapper for the call method of the Account class, adding multiple retry attempts.\n\n            Parameters\n            ----------\n            contract_id : str\n                The ID of the contract to call.\n            method_name : str\n                The name of the method to invoke on the contract.\n            args : dict\n                The arguments to pass to the contract method.\n            gas : int\n                The amount of gas to attach to the call.\n            amount : int\n                The amount of tokens to attach to the call.\n            nowait : bool\n                If True, do not wait for the transaction to be confirmed.\n            included : bool\n                If True, include the transaction in the block.\n            max_retries : int\n                The maximum number of retry attempts.\n\n            Returns\n            -------\n            The result of the contract method call.\n\n            Raises\n            ------\n            Exception\n                If all retry attempts fail, the exception is propagated.\n\n            \"\"\"\n            acc = Account(self.account_id, self.private_key, default_mainnet_rpc)\n            await acc.startup()\n            max_retries = min(max_retries, 10)\n\n            for attempt in range(1, max_retries + 1):\n                try:\n                    # Attempt to call the contract method\n                    return await acc.function_call(contract_id, method_name, args, gas, amount, nowait, included)\n                except Exception as e:\n                    # Log the error message for the current attempt\n                    print(\n                        f\"Attempt {attempt}/{max_retries} to call method '{method_name}' on contract \"\n                        f\"'{contract_id}' failed with error: {e}\"\n                    )\n\n                    # If it's the last attempt, re-raise the exception\n                    if attempt == max_retries:\n                        raise\n\n        async def get_balance(self, account_id: Optional[str] = None) -&gt; int:\n            \"\"\"Retrieves the balance of the specified NEAR account.\n\n            Parameters\n            ----------\n            account_id : Optional[str]\n                The ID of the account to retrieve the balance for. If not provided, the balance of the current\n                account is retrieved.\n\n            Returns\n            -------\n            int\n                The balance of the specified account in yoctoNEAR.\n\n            Raises\n            ------\n            Exception\n                If there is an error retrieving the balance.\n\n            \"\"\"\n            acc = Account(self.account_id, self.private_key, default_mainnet_rpc)\n            await acc.startup()\n            return await acc.get_balance(account_id)\n\n        def __init__(\n            self,\n            account_id: Optional[str] = None,\n            private_key: Optional[Union[List[Union[str, bytes]], str, bytes]] = None,\n            rpc_addr: Optional[str] = None,\n        ):\n            self.account_id = account_id\n            self.private_key = private_key\n            super().__init__(account_id, private_key, rpc_addr)\n\n    self.set_near = NearAccount\n\n    self._tools = ToolRegistry()\n\n    if create_files:\n        os.makedirs(self._path, exist_ok=True)\n        open(os.path.join(self._path, CHAT_FILENAME), \"a\").close()\n    os.chdir(self._path)\n\n    # Protected client methods\n    def query_vector_store(vector_store_id: str, query: str, full_files: bool = False):\n        \"\"\"Queries a vector store.\n\n        vector_store_id: The id of the vector store to query.\n        query: The query to search for.\n        \"\"\"\n        return client.query_vector_store(vector_store_id, query, full_files)\n\n    self.query_vector_store = query_vector_store\n\n    def upload_file(\n        file_content: str,\n        purpose: Literal[\"assistants\", \"batch\", \"fine-tune\", \"vision\"] = \"assistants\",\n        encoding: Optional[str] = \"utf-8\",\n        file_name: Optional[str] = \"file.txt\",\n        file_type: Optional[str] = \"text/plain\",\n    ):\n        \"\"\"Uploads a file to the registry.\"\"\"\n        return client.upload_file(\n            file_content, purpose, encoding=encoding, file_name=file_name, file_type=file_type\n        )\n\n    self.upload_file = upload_file\n\n    def remove_file(file_id: str):\n        \"\"\"Removes a file from the registry.\"\"\"\n        return client.remove_file(file_id)\n\n    self.remove_file = remove_file\n\n    def create_vector_store_from_source(\n        name: str,\n        source: Union[GitHubSource, GitLabSource],\n        source_auth: Optional[str] = None,\n        chunking_strategy: Optional[ChunkingStrategy] = None,\n        expires_after: Optional[ExpiresAfter] = None,\n        metadata: Optional[Dict[str, str]] = None,\n    ) -&gt; VectorStore:\n        \"\"\"Creates a vector store from the given source.\n\n        Args:\n        ----\n            name: The name of the vector store.\n            source: The source from which to create the vector store.\n            source_auth: The source authentication token.\n            chunking_strategy: The chunking strategy to use.\n            expires_after: The expiration policy.\n            metadata: Additional metadata.\n\n        Returns:\n        -------\n            VectorStore: The created vector store.\n\n        \"\"\"\n        return client.create_vector_store_from_source(\n            name=name,\n            source=source,\n            source_auth=source_auth,\n            chunking_strategy=chunking_strategy,\n            expires_after=expires_after,\n            metadata=metadata,\n        )\n\n    self.create_vector_store_from_source = create_vector_store_from_source\n\n    def add_file_to_vector_store(vector_store_id: str, file_id: str):\n        \"\"\"Adds a file to the vector store.\"\"\"\n        return client.add_file_to_vector_store(vector_store_id, file_id)\n\n    self.add_file_to_vector_store = add_file_to_vector_store\n\n    # positional arguments are not allowed because arguments list will be updated\n    def find_agents(*, owner_id: Optional[str] = None, with_capabilities: Optional[bool] = False):\n        \"\"\"Find agents based on various parameters.\"\"\"\n        return client.find_agents(owner_id, with_capabilities)\n\n    self.find_agents = find_agents\n\n    def create_vector_store(\n        name: str,\n        file_ids: list,\n        expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN,\n        chunking_strategy: Union[\n            AutoFileChunkingStrategyParam, StaticFileChunkingStrategyObjectParam, NotGiven\n        ] = NOT_GIVEN,\n        metadata: Optional[Dict[str, str]] = None,\n    ) -&gt; VectorStore:\n        \"\"\"Creates a vector store.\n\n        Args:\n        ----\n            name: The name of the vector store.\n            file_ids: List of file ids to create the vector store.\n            chunking_strategy: The chunking strategy to use.\n            expires_after: The expiration policy.\n            metadata: Additional metadata.\n\n        Returns:\n        -------\n            VectorStore: The created vector store.\n\n        \"\"\"\n        return client.create_vector_store(\n            name=name,\n            file_ids=file_ids,\n            chunking_strategy=chunking_strategy,\n            expires_after=expires_after,\n            metadata=metadata,\n        )\n\n    self.create_vector_store = create_vector_store\n\n    def get_vector_store(vector_store_id: str) -&gt; VectorStore:\n        \"\"\"Gets a vector store by id.\"\"\"\n        return client.get_vector_store(vector_store_id)\n\n    self.get_vector_store = get_vector_store\n\n    def get_vector_store_files(vector_store_id: str) -&gt; Optional[List[VectorStoreFile]]:\n        \"\"\"Gets a list of vector store files.\"\"\"\n        return client.get_vector_store_files(vector_store_id)\n\n    self.get_vector_store_files = get_vector_store_files\n\n    # Save cache of requested models for inference to avoid extra server calls\n    self.cached_models_for_inference: Dict[str, str] = {}\n\n    def get_model_for_inference(model: str = \"\") -&gt; str:\n        \"\"\"Returns 'provider::model_full_path'.\"\"\"\n        if self.cached_models_for_inference.get(model, None) is None:\n            provider = self.get_primary_agent().model_provider if self._agents else \"\"\n            if model == \"\":\n                model = self.get_primary_agent().model if self._agents else \"\"\n            if model == \"\":\n                return DEFAULT_PROVIDER_MODEL\n\n            _, model_for_inference = client.provider_models.match_provider_model(model, provider)\n\n            self.cached_models_for_inference[model] = model_for_inference\n\n        return self.cached_models_for_inference[model]\n\n    self.get_model_for_inference = get_model_for_inference\n\n    def _run_inference_completions(\n        messages: Union[Iterable[ChatCompletionMessageParam], str],\n        model: Union[Iterable[ChatCompletionMessageParam], str],\n        stream: bool,\n        **kwargs: Any,\n    ) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"Run inference completions for given parameters.\"\"\"\n        params, kwargs = self.get_inference_parameters(messages, model, stream, **kwargs)\n\n        completions = client.completions(\n            params.model, params.messages, params.stream, params.temperature, params.max_tokens, **kwargs\n        )\n\n        return completions\n\n    self._run_inference_completions = _run_inference_completions\n\n    def get_agent_public_key():\n        \"\"\"Returns public key of the agent.\"\"\"\n        agent_name = self.get_primary_agent().get_full_name()\n\n        return client.get_agent_public_key(agent_name)\n\n    self.get_agent_public_key = get_agent_public_key\n\n    def run_agent(\n        agent_id: str,\n        query: Optional[str] = None,\n        thread_mode: ThreadMode = ThreadMode.FORK,\n        run_mode: RunMode = RunMode.SIMPLE,\n    ):\n        \"\"\"Runs a child agent on the thread.\"\"\"\n        child_thread_id = self._thread_id\n\n        if thread_mode == ThreadMode.SAME:\n            pass\n        elif thread_mode == ThreadMode.FORK:\n            child_thread_id = client.threads_fork(self._thread_id).id\n            self.add_system_log(f\"Forked thread {child_thread_id}\", logging.INFO)\n        elif thread_mode == ThreadMode.CHILD:\n            child_thread_id = client.create_subthread(self._thread_id).id\n            self.add_system_log(f\"Created subthread {child_thread_id}\", logging.INFO)\n\n        if query:\n            client.threads_messages_create(thread_id=child_thread_id, content=query, role=\"user\")\n\n        self.add_system_log(f\"Running agent {agent_id}\", logging.INFO)\n        client.run_agent(\n            parent_run_id=self._run_id,\n            run_on_thread_id=child_thread_id,\n            assistant_id=agent_id,\n            run_mode=run_mode,\n        )\n        self._pending_ext_agent = True\n\n        return child_thread_id\n\n    self.run_agent = run_agent\n\n    def schedule_run(\n        agent: str,\n        input_message: str,\n        run_at: datetime,\n        run_params: Optional[Dict[str, str]] = None,\n        thread_id: Optional[str] = None,\n    ):\n        \"\"\"Schedules a run.\"\"\"\n        return client.schedule_run(agent, input_message, thread_id, run_params, run_at)\n\n    self.schedule_run = schedule_run\n\n    # TODO(https://github.com/nearai/nearai/issues/549): Allow only a subset of agents to access/update user memory.\n    def add_user_memory(memory: str):\n        \"\"\"Add user memory.\"\"\"\n        return client.add_user_memory(memory)\n\n    self.add_user_memory = add_user_memory\n\n    def query_user_memory(query: str):\n        \"\"\"Query user memory.\"\"\"\n        return client.query_user_memory(query)\n\n    self.query_user_memory = query_user_memory\n\n    def generate_image(prompt: str):\n        \"\"\"Generate an image.\"\"\"\n        return client.generate_image(prompt)\n\n    self.generate_image = generate_image\n\n    def save_agent_data(key, data: Dict[str, Any]):\n        \"\"\"Save agent data.\"\"\"\n        return client.save_agent_data(key, data)\n\n    self.save_agent_data = save_agent_data\n\n    def get_agent_data():\n        \"\"\"Get agent data.\"\"\"\n        return client.get_agent_data()\n\n    self.get_agent_data = get_agent_data\n\n    def get_agent_data_by_key(key, default=None):\n        \"\"\"Get agent data by key.\"\"\"\n        namespace = self.get_primary_agent().namespace\n        name = self.get_primary_agent().name\n        result = client.get_agent_data_by_key(key)\n        return (\n            result\n            if result\n            else {\n                \"value\": default,\n                \"namespace\": namespace,\n                \"key\": key,\n                \"name\": name,\n                \"updated_at\": \"\",\n                \"created_at\": \"\",\n            }\n        )\n\n    self.get_agent_data_by_key = get_agent_data_by_key\n\n    # HubClient methods\n    def add_reply(\n        message: str,\n        attachments: Optional[Iterable[Attachment]] = None,\n        message_type: Optional[str] = None,\n        thread_id: str = self._thread_id,\n    ):\n        \"\"\"Assistant adds a message to the environment.\"\"\"\n        # NOTE: message from `user` are not stored in the memory\n\n        return hub_client.beta.threads.messages.create(\n            thread_id=thread_id,\n            role=\"assistant\",\n            content=message,\n            extra_body={\n                \"assistant_id\": self.get_primary_agent().identifier,\n                \"run_id\": self._run_id,\n            },\n            attachments=attachments,\n            metadata={\"message_type\": message_type} if message_type else None,\n        )\n\n    self.add_reply = add_reply\n\n    def get_thread(thread_id=self._thread_id):\n        \"\"\"Returns the current Thread object or the requested Thread.\"\"\"\n        return client.get_thread(thread_id)\n\n    self.get_thread = get_thread\n\n    def _add_message(\n        role: str,\n        message: str,\n        attachments: Optional[Iterable[Attachment]] = None,\n        **kwargs: Any,\n    ):\n        return hub_client.beta.threads.messages.create(\n            thread_id=self._thread_id,\n            role=role,  # type: ignore\n            content=message,\n            extra_body={\n                \"assistant_id\": self.get_primary_agent().identifier,\n                \"run_id\": self._run_id,\n            },\n            metadata=kwargs,\n            attachments=attachments,\n        )\n\n    self._add_message = _add_message\n\n    def _list_messages(\n        limit: Union[int, NotGiven] = NOT_GIVEN,\n        order: Literal[\"asc\", \"desc\"] = \"asc\",\n        thread_id: Optional[str] = None,\n    ) -&gt; List[Message]:\n        \"\"\"Returns messages from the environment.\"\"\"\n        messages = hub_client.beta.threads.messages.list(\n            thread_id=thread_id or self._thread_id, limit=limit, order=order\n        )\n        self.add_system_log(f\"Retrieved {len(messages.data)} messages from NEAR AI Hub\")\n        return messages.data\n\n    self._list_messages = _list_messages\n\n    def list_files_from_thread(\n        order: Literal[\"asc\", \"desc\"] = \"asc\", thread_id: Optional[str] = None\n    ) -&gt; List[FileObject]:\n        \"\"\"Lists files in the thread.\"\"\"\n        messages = self._list_messages(order=order)\n        # Extract attachments from messages\n        attachments = [a for m in messages if m.attachments for a in m.attachments]\n        # Extract files from attachments\n        file_ids = [a.file_id for a in attachments]\n        files = [hub_client.files.retrieve(f) for f in file_ids if f]\n        return files\n\n    self.list_files_from_thread = list_files_from_thread\n\n    def read_file_by_id(file_id: str):\n        \"\"\"Read a file from the thread.\"\"\"\n        content = hub_client.files.content(file_id).content.decode(\"utf-8\")\n        print(\"file content returned by api\", content)\n        return content\n\n    self.read_file_by_id = read_file_by_id\n\n    def write_file(\n        filename: str,\n        content: Union[str, bytes],\n        encoding: str = \"utf-8\",\n        filetype: str = \"text/plain\",\n        write_to_disk: bool = True,\n    ) -&gt; FileObject:\n        \"\"\"Writes a file to the environment.\n\n        filename: The name of the file to write to\n        content: The content to write to the file\n        encoding: The encoding to use when writing the file (default is utf-8)\n        filetype: The MIME type of the file (default is text/plain)\n        write_to_disk: If True, write locally to disk (default is True)\n        \"\"\"\n        if write_to_disk:\n            # Write locally\n            path = Path(self.get_primary_agent_temp_dir()) / filename\n            path.parent.mkdir(parents=True, exist_ok=True)\n            if isinstance(content, bytes):\n                with open(path, \"wb\") as f:\n                    f.write(content)\n            else:\n                with open(path, \"w\", encoding=encoding) as f:\n                    f.write(content)\n\n        if isinstance(content, bytes):\n            file_data = content\n        else:\n            file_data = io.BytesIO(content.encode(encoding))  # type:ignore\n\n        # Upload to Hub\n        file = hub_client.files.create(file=(filename, file_data, filetype), purpose=\"assistants\")\n        res = self.add_reply(\n            message=f\"Successfully wrote {len(content) if content else 0} characters to {filename}\",\n            attachments=[{\"file_id\": file.id, \"tools\": [{\"type\": \"file_search\"}]}],\n            message_type=\"system:file_write\",\n        )\n        self.add_system_log(\n            f\"Uploaded file {filename} with {len(content)} characters, id: {file.id}. Added in thread as: {res.id}\"\n        )\n        return file\n\n    self.write_file = write_file\n\n    def mark_done() -&gt; Run:  # noqa: D102\n        self._done = True\n        res = hub_client.beta.threads.runs.update(\n            thread_id=self._thread_id,\n            run_id=self._run_id,\n            extra_body={\n                \"status\": \"completed\",\n                \"completed_at\": datetime.now().isoformat(),\n            },\n        )\n        return res\n\n    self.mark_done = mark_done\n\n    def mark_failed() -&gt; Run:\n        \"\"\"Marks the environment run as failed.\"\"\"\n        self._done = True\n        self.add_system_log(\"Environment run failed\", logging.ERROR)\n        res = hub_client.beta.threads.runs.update(\n            thread_id=self._thread_id,\n            run_id=self._run_id,\n            extra_body={\"status\": \"failed\", \"failed_at\": datetime.now().isoformat()},\n        )\n        return res\n\n    self.mark_failed = mark_failed\n\n    def request_user_input() -&gt; Run:\n        \"\"\"Must be called to request input from the user.\"\"\"\n        return hub_client.beta.threads.runs.update(\n            thread_id=self._thread_id,\n            run_id=self._run_id,\n            extra_body={\"status\": \"requires_action\", \"required_action\": {\"type\": \"user_input\"}},\n        )\n\n    self.request_user_input = request_user_input\n\n    def request_agent_input() -&gt; Run:\n        \"\"\"Mark the run as ready for input from another agent.\"\"\"\n        return hub_client.beta.threads.runs.update(\n            thread_id=self._thread_id,\n            run_id=self._run_id,\n            extra_body={\"status\": \"requires_action\", \"required_action\": {\"type\": \"agent_input\"}},\n        )\n\n    self.request_agent_input = request_agent_input\n\n    # Must be placed after method definitions\n    self.register_standard_tools()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.add_agent_log","title":"add_agent_log","text":"<pre><code>add_agent_log(log: str, level: int = INFO) -&gt; None\n</code></pre> <p>Add agent log with timestamp and log level.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def add_agent_log(self, log: str, level: int = logging.INFO) -&gt; None:\n    \"\"\"Add agent log with timestamp and log level.\"\"\"\n    logger = logging.getLogger(\"agent_logger\")\n    if not logger.handlers:\n        # Configure the logger if it hasn't been set up yet\n        logger.setLevel(logging.DEBUG)\n        file_handler = logging.FileHandler(os.path.join(self._path, AGENT_LOG_FILENAME))\n        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n        # Add Thread log handler\n        if self._debug_mode:\n            custom_handler = CustomLogHandler(self.add_reply, \"agent\")\n            custom_handler.setFormatter(formatter)\n            logger.addHandler(custom_handler)\n\n    # Log the message\n    logger.log(level, log)\n    # Force the handler to write to disk\n    for handler in logger.handlers:\n        handler.flush()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.add_agent_start_system_log","title":"add_agent_start_system_log","text":"<pre><code>add_agent_start_system_log(agent_idx: int) -&gt; None\n</code></pre> <p>Adds agent start system log.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def add_agent_start_system_log(self, agent_idx: int) -&gt; None:\n    \"\"\"Adds agent start system log.\"\"\"\n    agent = self._agents[agent_idx]\n    message = f\"Running agent {agent.name}\"\n    if agent.model != \"\":\n        model = self.get_model_for_inference(agent.model)\n        self._last_used_model = model\n        message += f\" that will connect to {model}\"\n        if agent.model_temperature:\n            message += f\", temperature={agent.model_temperature}\"\n        if agent.model_max_tokens:\n            message += f\", max_tokens={agent.model_max_tokens}\"\n    self.add_system_log(message)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.add_message","title":"add_message","text":"<pre><code>add_message(role: str, message: str, attachments: Optional[Iterable[Attachment]] = None, **kwargs: Any)\n</code></pre> <p>Deprecated. Please use <code>add_reply</code> instead. Assistant adds a message to the environment.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def add_message(\n    self,\n    role: str,\n    message: str,\n    attachments: Optional[Iterable[Attachment]] = None,\n    **kwargs: Any,\n):\n    \"\"\"Deprecated. Please use `add_reply` instead. Assistant adds a message to the environment.\"\"\"\n    # Prevent agent to save messages on behalf of `user` to avoid adding false memory\n    role = \"assistant\"\n\n    return self._add_message(role, message, attachments, **kwargs)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.add_system_log","title":"add_system_log","text":"<pre><code>add_system_log(log: str, level: int = INFO) -&gt; None\n</code></pre> <p>Add system log with timestamp and log level.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def add_system_log(self, log: str, level: int = logging.INFO) -&gt; None:\n    \"\"\"Add system log with timestamp and log level.\"\"\"\n    logger = logging.getLogger(\"system_logger\")\n    if not logger.handlers:\n        # Configure the logger if it hasn't been set up yet\n        logger.setLevel(logging.DEBUG)\n        file_handler = logging.FileHandler(os.path.join(self._path, SYSTEM_LOG_FILENAME))\n        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n        if self.print_system_log:\n            console_handler = logging.StreamHandler()\n            console_handler.setFormatter(formatter)\n            logger.addHandler(console_handler)\n\n        # Add Thread log handler\n        if self._debug_mode:\n            custom_handler = CustomLogHandler(self.add_reply, \"system\")\n            custom_handler.setFormatter(formatter)\n            logger.addHandler(custom_handler)\n\n    # Log the message\n    logger.log(level, log)\n    # Force the handler to write to disk\n    for handler in logger.handlers:\n        handler.flush()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.call_agent","title":"call_agent","text":"<pre><code>call_agent(agent_index: int, task: str) -&gt; None\n</code></pre> <p>Calls agent with given task.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def call_agent(self, agent_index: int, task: str) -&gt; None:\n    \"\"\"Calls agent with given task.\"\"\"\n    self._agents[agent_index].run(self, task=task)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.clear_temp_agent_files","title":"clear_temp_agent_files","text":"<pre><code>clear_temp_agent_files(verbose=True) -&gt; None\n</code></pre> <p>Remove temp agent files created to be used in <code>runpy</code>.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def clear_temp_agent_files(self, verbose=True) -&gt; None:\n    \"\"\"Remove temp agent files created to be used in `runpy`.\"\"\"\n    for agent in self._agents:\n        if os.path.exists(agent.temp_dir):\n            if verbose:\n                print(\"removed agent.temp_files\", agent.temp_dir)\n            shutil.rmtree(agent.temp_dir)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.completion","title":"completion","text":"<pre><code>completion(messages: Union[Iterable[ChatCompletionMessageParam], str], model: Union[Iterable[ChatCompletionMessageParam], str] = '', **kwargs: Any) -&gt; str\n</code></pre> <p>Returns a completion for the given messages using the given model.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def completion(\n    self,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Returns a completion for the given messages using the given model.\"\"\"\n    raw_response = self.completions(messages, model, **kwargs)\n    assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n    response: ModelResponse = raw_response\n    assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n    choices: List[Choices] = response.choices  # type: ignore\n    response_message = choices[0].message.content\n    assert response_message, \"No completions returned\"\n    return response_message\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.completion_and_get_tools_calls","title":"completion_and_get_tools_calls","text":"<pre><code>completion_and_get_tools_calls(messages: List[ChatCompletionMessageParam], model: str = '', **kwargs: Any) -&gt; SimpleNamespace\n</code></pre> <p>Returns completion message and/or tool calls from OpenAI or Llama tool formats.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def completion_and_get_tools_calls(\n    self,\n    messages: List[ChatCompletionMessageParam],\n    model: str = \"\",\n    **kwargs: Any,\n) -&gt; SimpleNamespace:\n    \"\"\"Returns completion message and/or tool calls from OpenAI or Llama tool formats.\"\"\"\n    raw_response = self._run_inference_completions(messages, model, stream=False, **kwargs)\n\n    assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n    response: ModelResponse = raw_response\n    assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n    choices: List[Choices] = response.choices  # type: ignore\n\n    (message_without_tool_call, tool_calls) = self._parse_tool_call(choices[0].message)\n\n    if message_without_tool_call is None:\n        response_message = choices[0].message.content\n        message_without_tool_call = response_message\n\n    return SimpleNamespace(message=message_without_tool_call, tool_calls=tool_calls)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.completion_and_run_tools","title":"completion_and_run_tools","text":"<pre><code>completion_and_run_tools(messages: List[ChatCompletionMessageParam], model: str = '', tools: Optional[List] = None, **kwargs: Any) -&gt; Optional[str]\n</code></pre> <p>Returns a completion for the given messages using the given model and runs tools.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def completion_and_run_tools(\n    self,\n    messages: List[ChatCompletionMessageParam],\n    model: str = \"\",\n    tools: Optional[List] = None,\n    **kwargs: Any,\n) -&gt; Optional[str]:\n    \"\"\"Returns a completion for the given messages using the given model and runs tools.\"\"\"\n    completion_tools_response = self.completions_and_run_tools(messages, model, tools, **kwargs)\n    assert all(\n        map(\n            lambda choice: isinstance(choice, Choices),\n            completion_tools_response.choices,\n        )\n    ), \"Expected Choices\"\n    choices: List[Choices] = completion_tools_response.choices  # type: ignore\n    response_content = choices[0].message.content\n    return response_content\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.completions","title":"completions","text":"<pre><code>completions(messages: Union[Iterable[ChatCompletionMessageParam], str], model: Union[Iterable[ChatCompletionMessageParam], str] = '', stream: bool = False, **kwargs: Any) -&gt; Union[ModelResponse, CustomStreamWrapper]\n</code></pre> <p>Returns all completions for given messages using the given model.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def completions(\n    self,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n    stream: bool = False,\n    **kwargs: Any,\n) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n    \"\"\"Returns all completions for given messages using the given model.\"\"\"\n    return self._run_inference_completions(messages, model, stream, **kwargs)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.completions_and_run_tools","title":"completions_and_run_tools","text":"<pre><code>completions_and_run_tools(messages: List[ChatCompletionMessageParam], model: str = '', tools: Optional[List] = None, add_responses_to_messages: bool = True, agent_role_name='assistant', tool_role_name='tool', **kwargs: Any) -&gt; ModelResponse\n</code></pre> <p>Returns all completions for given messages using the given model and runs tools.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def completions_and_run_tools(\n    self,\n    messages: List[ChatCompletionMessageParam],\n    model: str = \"\",\n    tools: Optional[List] = None,\n    add_responses_to_messages: bool = True,\n    agent_role_name=\"assistant\",\n    tool_role_name=\"tool\",\n    **kwargs: Any,\n) -&gt; ModelResponse:\n    \"\"\"Returns all completions for given messages using the given model and runs tools.\"\"\"\n    if self._use_llama_tool_syntax(model, tools):\n        tool_prompt = self._llama_tool_prompt(tools)\n        messages.append({\"role\": \"system\", \"content\": tool_prompt})\n    raw_response = self._run_inference_completions(messages, model, stream=False, tools=tools, **kwargs)\n    assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n    response: ModelResponse = raw_response\n    assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n    choices: List[Choices] = response.choices  # type: ignore\n    response_message = choices[0].message\n\n    self._handle_tool_calls(response_message, add_responses_to_messages, agent_role_name, tool_role_name)\n\n    return response\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.create_snapshot","title":"create_snapshot","text":"<pre><code>create_snapshot() -&gt; bytes\n</code></pre> <p>Create an in memory snapshot.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def create_snapshot(self) -&gt; bytes:\n    \"\"\"Create an in memory snapshot.\"\"\"\n    with tempfile.NamedTemporaryFile(suffix=\".tar.gz\") as f:\n        with tarfile.open(fileobj=f, mode=\"w:gz\") as tar:\n            tar.add(self._path, arcname=\".\")\n        f.flush()\n        f.seek(0)\n        snapshot = f.read()\n    return snapshot\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.environment_run_info","title":"environment_run_info","text":"<pre><code>environment_run_info(base_id, run_type) -&gt; dict\n</code></pre> <p>Returns the environment run information.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def environment_run_info(self, base_id, run_type) -&gt; dict:\n    \"\"\"Returns the environment run information.\"\"\"\n    if not self._agents or not self.get_primary_agent():\n        raise ValueError(\"Agent not found\")\n    primary_agent = self.get_primary_agent()\n\n    full_agent_name = \"/\".join([primary_agent.namespace, primary_agent.name, primary_agent.version])\n    safe_agent_name = full_agent_name.replace(\"/\", \"_\")\n    uid = uuid.uuid4().hex\n    generated_name = f\"environment_run_{safe_agent_name}_{uid}\"\n    name = generated_name\n\n    timestamp = datetime.now(timezone.utc).isoformat()\n    return {\n        \"name\": name,\n        \"version\": \"0\",\n        \"description\": f\"Agent {run_type} {full_agent_name} {uid} {timestamp}\",\n        \"category\": \"environment\",\n        \"tags\": [\"environment\"],\n        \"details\": {\n            \"base_id\": base_id,\n            \"timestamp\": timestamp,\n            \"agents\": [agent.name for agent in self._agents],\n            \"primary_agent_namespace\": primary_agent.namespace,\n            \"primary_agent_name\": primary_agent.name,\n            \"primary_agent_version\": primary_agent.version,\n            \"run_id\": self._run_id,\n            \"run_type\": run_type,\n        },\n        \"show_entry\": True,\n    }\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.exec_command","title":"exec_command","text":"<pre><code>exec_command(command: str) -&gt; Dict[str, Union[str, int]]\n</code></pre> <p>Executes a command in the environment and logs the output.</p> <p>The environment does not allow running interactive programs. It will run a program for 1 second then will interrupt it if it is still running or if it is waiting for user input. command: The command to execute, like 'ls -l' or 'python3 tests.py'</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def exec_command(self, command: str) -&gt; Dict[str, Union[str, int]]:\n    \"\"\"Executes a command in the environment and logs the output.\n\n    The environment does not allow running interactive programs.\n    It will run a program for 1 second then will interrupt it if it is still running\n    or if it is waiting for user input.\n    command: The command to execute, like 'ls -l' or 'python3 tests.py'\n    \"\"\"\n    approval_function = self._approvals[\"confirm_execution\"] if self._approvals else None\n    if not approval_function:\n        return {\n            \"stderr\": \"Agent runner misconfiguration. No command execution approval function found.\",\n        }\n    if not approval_function(command):\n        return {\n            \"command\": command,\n            \"returncode\": 999,\n            \"stdout\": \"\",\n            \"stderr\": \"Command execution was not approved.\",\n        }\n\n    try:\n        process = subprocess.Popen(\n            shlex.split(command),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            bufsize=0,\n            universal_newlines=True,\n            cwd=self._path,\n        )\n    except Exception as e:\n        return {\n            \"command\": command,\n            \"returncode\": 999,\n            \"stdout\": \"\",\n            \"stderr\": \"Failed to execute: \" + str(e),\n        }\n\n    msg = \"\"\n\n    def kill_process_tree(p: Any) -&gt; None:\n        nonlocal msg\n        msg = \"Killing process due to timeout\"\n\n        process = psutil.Process(p.pid)\n        for proc in process.children(recursive=True):\n            proc.kill()\n        process.kill()\n\n    timer = threading.Timer(2, kill_process_tree, (process,))\n    timer.start()\n    process.wait()\n    timer.cancel()\n\n    result = {\n        \"command\": command,\n        \"stdout\": process.stdout.read() if process.stdout and hasattr(process.stdout, \"read\") else \"\",\n        \"stderr\": process.stderr.read() if process.stderr and hasattr(process.stderr, \"read\") else \"\",\n        \"returncode\": process.returncode,\n        \"msg\": msg,\n    }\n    with open(os.path.join(self._path, TERMINAL_FILENAME), \"a\") as f:\n        f.write(json.dumps(result) + DELIMITER)\n    return result\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.generate_folder_hash_id","title":"generate_folder_hash_id","text":"<pre><code>generate_folder_hash_id(path: str) -&gt; str\n</code></pre> <p>Returns hash based on files and their contents in path, including subfolders.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def generate_folder_hash_id(self, path: str) -&gt; str:\n    \"\"\"Returns hash based on files and their contents in path, including subfolders.\"\"\"  # noqa: E501\n    hash_obj = hashlib.md5()\n\n    for root, _dirs, files in os.walk(path):\n        for file in sorted(files):\n            file_path = os.path.join(root, file)\n            with open(file_path, \"rb\") as f:\n                while chunk := f.read(8192):\n                    hash_obj.update(chunk)\n\n    return hash_obj.hexdigest()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_agent_temp_path","title":"get_agent_temp_path","text":"<pre><code>get_agent_temp_path() -&gt; Path\n</code></pre> <p>Returns temp dir for primary agent where execution happens.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_agent_temp_path(self) -&gt; Path:\n    \"\"\"Returns temp dir for primary agent where execution happens.\"\"\"\n    return self.get_primary_agent_temp_dir()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_agents","title":"get_agents","text":"<pre><code>get_agents() -&gt; List[Agent]\n</code></pre> <p>Returns list of agents available in environment.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_agents(self) -&gt; List[Agent]:\n    \"\"\"Returns list of agents available in environment.\"\"\"\n    return self._agents\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_inference_parameters","title":"get_inference_parameters","text":"<pre><code>get_inference_parameters(messages: Union[Iterable[ChatCompletionMessageParam], str], model: Union[Iterable[ChatCompletionMessageParam], str], stream: bool, **kwargs: Any) -&gt; Tuple[InferenceParameters, Any]\n</code></pre> <p>Run inference parameters to run completions.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_inference_parameters(\n    self,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    model: Union[Iterable[ChatCompletionMessageParam], str],\n    stream: bool,\n    **kwargs: Any,\n) -&gt; Tuple[InferenceParameters, Any]:\n    \"\"\"Run inference parameters to run completions.\"\"\"\n    if isinstance(messages, str):\n        self.add_system_log(\n            \"Deprecated completions call. Pass `messages` as a first parameter.\",\n            logging.WARNING,\n        )\n        messages_or_model = messages\n        model_or_messages = model\n        model = cast(str, messages_or_model)\n        messages = cast(Iterable[ChatCompletionMessageParam], model_or_messages)\n    else:\n        model = cast(str, model)\n        messages = cast(Iterable[ChatCompletionMessageParam], messages)\n    model = self.get_model_for_inference(model)\n    if model != self._last_used_model:\n        self._last_used_model = model\n        self.add_system_log(f\"Connecting to {model}\")\n\n    temperature = kwargs.pop(\"temperature\", self.get_primary_agent().model_temperature if self._agents else None)\n    max_tokens = kwargs.pop(\"max_tokens\", self.get_primary_agent().model_max_tokens if self._agents else None)\n\n    params = InferenceParameters(\n        model=model,\n        messages=messages,\n        stream=stream,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n\n    return params, kwargs\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_last_message","title":"get_last_message","text":"<pre><code>get_last_message(role: str = 'user')\n</code></pre> <p>Reads last message from the given role and returns it.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_last_message(self, role: str = \"user\"):\n    \"\"\"Reads last message from the given role and returns it.\"\"\"\n    for message in reversed(self.list_messages()):\n        if message.get(\"role\") == role:\n            return message\n\n    return None\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_primary_agent","title":"get_primary_agent","text":"<pre><code>get_primary_agent() -&gt; Agent\n</code></pre> <p>Returns the agent that is invoked first.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_primary_agent(self) -&gt; Agent:\n    \"\"\"Returns the agent that is invoked first.\"\"\"\n    return self._agents[0]\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_primary_agent_temp_dir","title":"get_primary_agent_temp_dir","text":"<pre><code>get_primary_agent_temp_dir() -&gt; Path\n</code></pre> <p>Returns temp dir for primary agent.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_primary_agent_temp_dir(self) -&gt; Path:\n    \"\"\"Returns temp dir for primary agent.\"\"\"\n    return self.get_primary_agent().temp_dir\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_system_path","title":"get_system_path","text":"<pre><code>get_system_path() -&gt; Path\n</code></pre> <p>Returns the system path where chat.txt &amp; system_log are stored.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_system_path(self) -&gt; Path:\n    \"\"\"Returns the system path where chat.txt &amp; system_log are stored.\"\"\"\n    return Path(self._path)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.get_tool_registry","title":"get_tool_registry","text":"<pre><code>get_tool_registry(new: bool = False) -&gt; ToolRegistry\n</code></pre> <p>Returns the tool registry, a dictionary of tools that can be called by the agent.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def get_tool_registry(self, new: bool = False) -&gt; ToolRegistry:\n    \"\"\"Returns the tool registry, a dictionary of tools that can be called by the agent.\"\"\"\n    if new:\n        self._tools = ToolRegistry()\n    return self._tools\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.list_files","title":"list_files","text":"<pre><code>list_files(path: str, order: Literal['asc', 'desc'] = 'asc') -&gt; List[str]\n</code></pre> <p>Lists files in the environment.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def list_files(self, path: str, order: Literal[\"asc\", \"desc\"] = \"asc\") -&gt; List[str]:\n    \"\"\"Lists files in the environment.\"\"\"\n    return os.listdir(os.path.join(self.get_primary_agent_temp_dir(), path))\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.list_messages","title":"list_messages","text":"<pre><code>list_messages(thread_id: Optional[str] = None, limit: Union[int, NotGiven] = 200, order: Literal['asc', 'desc'] = 'asc')\n</code></pre> <p>Backwards compatibility for chat_completions messages.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def list_messages(\n    self,\n    thread_id: Optional[str] = None,\n    limit: Union[int, NotGiven] = 200,  # api defaults to 20\n    order: Literal[\"asc\", \"desc\"] = \"asc\",\n):\n    \"\"\"Backwards compatibility for chat_completions messages.\"\"\"\n    messages = self._list_messages(thread_id=thread_id, limit=limit, order=order)\n\n    # Filter out system and agent log messages when running in debug mode. Agent behavior shouldn't change based on logs.  # noqa: E501\n    messages = [\n        m\n        for m in messages\n        if not (\n            m.metadata\n            and any(m.metadata.get(\"message_type\", \"\").startswith(prefix) for prefix in [\"system:\", \"agent:\"])\n        )\n    ]\n\n    legacy_messages = [\n        {\n            \"id\": m.id,\n            \"content\": \"\\n\".join([c.text.value for c in m.content]),  # type: ignore\n            \"role\": m.role,\n        }\n        for m in messages\n    ]\n    return legacy_messages\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.list_terminal_commands","title":"list_terminal_commands","text":"<pre><code>list_terminal_commands(filename: str = TERMINAL_FILENAME) -&gt; List[Any]\n</code></pre> <p>Returns the terminal commands from the terminal file.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def list_terminal_commands(self, filename: str = TERMINAL_FILENAME) -&gt; List[Any]:\n    \"\"\"Returns the terminal commands from the terminal file.\"\"\"\n    path = os.path.join(self._path, filename)\n\n    if not os.path.exists(path):\n        return []\n\n    with open(path, \"r\") as f:\n        return [json.loads(message) for message in f.read().split(DELIMITER) if message]\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.load_snapshot","title":"load_snapshot","text":"<pre><code>load_snapshot(snapshot: bytes) -&gt; None\n</code></pre> <p>Load Environment from Snapshot.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def load_snapshot(self, snapshot: bytes) -&gt; None:\n    \"\"\"Load Environment from Snapshot.\"\"\"\n    shutil.rmtree(self._path, ignore_errors=True)\n\n    with tempfile.NamedTemporaryFile(suffix=\".tar.gz\") as f:\n        f.write(snapshot)\n        f.flush()\n        f.seek(0)\n\n        with tarfile.open(fileobj=f, mode=\"r:gz\") as tar:\n            tar.extractall(self._path)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.read_file","title":"read_file","text":"<pre><code>read_file(filename: str) -&gt; Optional[Union[bytes, str]]\n</code></pre> <p>Reads a file from the environment or thread.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def read_file(self, filename: str) -&gt; Optional[Union[bytes, str]]:\n    \"\"\"Reads a file from the environment or thread.\"\"\"\n    file_content: Optional[Union[bytes, str]] = None\n    # First try to read from local filesystem\n    local_path = os.path.join(self.get_primary_agent_temp_dir(), filename)\n    if os.path.exists(local_path):\n        try:\n            with open(local_path, \"rb\") as local_path_file:\n                local_file_content = local_path_file.read()\n                try:\n                    # Try to decode as text\n                    file_content = local_file_content.decode(\"utf-8\")\n                except UnicodeDecodeError:\n                    # If decoding fails, store as binary\n                    file_content = local_file_content\n        except Exception as e:\n            print(f\"Error with read_file: {e}\")\n\n    if not file_content:\n        # Next check files written out by the agent.\n        # Agent output files take precedence over files packaged with the agent\n        thread_files = self.list_files_from_thread(order=\"desc\")\n\n        # Then try to read from thread, starting from the most recent\n        for f in thread_files:\n            if f.filename == filename:\n                file_content = self.read_file_by_id(f.id)\n                break\n\n        if not file_content:\n            # Next check agent file cache\n            # Agent output files &amp; thread files take precedence over cached files\n            file_cache = self.get_primary_agent().file_cache\n            if file_cache:\n                file_content = file_cache.get(filename, None)\n\n        # Write the file content from the thread or cache to the local filesystem\n        # This allows exec_command to operate on the file\n        if file_content:\n            if not os.path.exists(os.path.dirname(local_path)):\n                os.makedirs(os.path.dirname(local_path))\n\n            with open(local_path, \"wb\") as local_file:\n                if isinstance(file_content, bytes):\n                    local_file.write(file_content)\n                else:\n                    local_file.write(file_content.encode(\"utf-8\"))\n\n    if not file_content:\n        self.add_system_log(f\"Warn: File {filename} not found during read_file operation\")\n\n    return file_content\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.run","title":"run","text":"<pre><code>run(new_message: Optional[str] = None, max_iterations: int = 10) -&gt; None\n</code></pre> <p>Runs agent(s) against a new or previously created environment.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def run(\n    self,\n    new_message: Optional[str] = None,\n    max_iterations: int = 10,\n) -&gt; None:\n    \"\"\"Runs agent(s) against a new or previously created environment.\"\"\"\n    if new_message:\n        self._add_message(\"user\", new_message)\n\n    iteration = 0\n    self.set_next_actor(\"agent\")\n\n    while iteration &lt; max_iterations and not self.is_done() and self.get_next_actor() != \"user\":\n        iteration += 1\n        if max_iterations &gt; 1:\n            self.add_system_log(\n                f\"Running agent, iteration {iteration}/{max_iterations}\",\n                logging.INFO,\n            )\n        try:\n            error_message, traceback_message = self.get_primary_agent().run(self, task=new_message)\n            if self._debug_mode and (error_message or traceback_message):\n                if self._debug_mode and (error_message or traceback_message):\n                    message_parts = []\n\n                    if error_message:\n                        message_parts.append(f\"Error: \\n ```\\n{error_message}\\n```\")\n\n                    if traceback_message:\n                        message_parts.append(f\"Error Traceback: \\n ```\\n{traceback_message}\\n```\")\n\n                    self.add_reply(\"\\n\\n\".join(message_parts), message_type=\"system:debug\")\n\n        except Exception as e:\n            self.add_system_log(f\"Environment run failed: {e}\", logging.ERROR)\n            self.mark_failed()\n            raise e\n\n    if not self._pending_ext_agent:\n        # If no external agent was called, mark the whole run as done.\n        # Else this environment will stop for now but this run will be continued later.\n        self.mark_done()\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.set_next_actor","title":"set_next_actor","text":"<pre><code>set_next_actor(who: str) -&gt; None\n</code></pre> <p>Set the next actor / action in the dialogue.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def set_next_actor(self, who: str) -&gt; None:\n    \"\"\"Set the next actor / action in the dialogue.\"\"\"\n    next_action_fn = os.path.join(self._path, \".next_action\")\n    if who == \"agent\":\n        self._done = False\n\n    with open(next_action_fn, \"w\") as f:\n        f.write(who)\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.signed_completion","title":"signed_completion","text":"<pre><code>signed_completion(messages: Union[Iterable[ChatCompletionMessageParam], str], model: Union[Iterable[ChatCompletionMessageParam], str] = '', **kwargs: Any) -&gt; Dict[str, str]\n</code></pre> <p>Returns a completion for the given messages using the given model with the agent signature.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def signed_completion(\n    self,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n    **kwargs: Any,\n) -&gt; Dict[str, str]:\n    \"\"\"Returns a completion for the given messages using the given model with the agent signature.\"\"\"\n    # TODO Return signed completions for non-latest versions only?\n    agent_name = self.get_primary_agent().get_full_name()\n    raw_response = self.completions(messages, model, agent_name=agent_name, **kwargs)\n    assert isinstance(raw_response, ModelResponse), \"Expected ModelResponse\"\n    response: ModelResponse = raw_response\n\n    signature_data = json.loads(response.system_fingerprint) if response.system_fingerprint else {}\n\n    assert all(map(lambda choice: isinstance(choice, Choices), response.choices)), \"Expected Choices\"\n    choices: List[Choices] = response.choices  # type: ignore\n    response_message = choices[0].message.content\n    assert response_message, \"No completions returned\"\n\n    return {\n        \"response\": response_message,\n        \"signature\": signature_data.get(\"signature\", None),\n        \"public_key\": signature_data.get(\"public_key\", None),\n    }\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.verify_message","title":"verify_message","text":"<pre><code>verify_message(account_id: str, public_key: str, signature: str, message: str, nonce: str, callback_url: str) -&gt; SignatureVerificationResult\n</code></pre> <p>Verifies that the user message is signed with NEAR Account.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def verify_message(\n    self,\n    account_id: str,\n    public_key: str,\n    signature: str,\n    message: str,\n    nonce: str,\n    callback_url: str,\n) -&gt; near.SignatureVerificationResult:\n    \"\"\"Verifies that the user message is signed with NEAR Account.\"\"\"\n    return near.verify_signed_message(\n        account_id,\n        public_key,\n        signature,\n        message,\n        nonce,\n        self.get_primary_agent().name,\n        callback_url,\n    )\n</code></pre>"},{"location":"api/#nearai.agents.environment.Environment.verify_signed_message","title":"verify_signed_message","text":"<pre><code>verify_signed_message(completion: str, messages: Union[Iterable[ChatCompletionMessageParam], str], public_key: Union[str, None] = None, signature: Union[str, None] = None, model: Union[Iterable[ChatCompletionMessageParam], str] = '', **kwargs: Any) -&gt; bool\n</code></pre> <p>Verifies a signed message.</p> Source code in <code>nearai/agents/environment.py</code> <pre><code>def verify_signed_message(\n    self,\n    completion: str,\n    messages: Union[Iterable[ChatCompletionMessageParam], str],\n    public_key: Union[str, None] = None,\n    signature: Union[str, None] = None,\n    model: Union[Iterable[ChatCompletionMessageParam], str] = \"\",\n    **kwargs: Any,\n) -&gt; bool:\n    \"\"\"Verifies a signed message.\"\"\"\n    if public_key is None or signature is None:\n        return False\n\n    params, _ = self.get_inference_parameters(messages, model, False, **kwargs)\n\n    messages_without_ids = [{k: v for k, v in item.items() if k != \"id\"} for item in params.messages]\n    ordered_messages_without_ids = [\n        {\"role\": str(item[\"role\"]), \"content\": str(item[\"content\"])} for item in messages_without_ids\n    ]\n\n    return validate_completion_signature(\n        public_key,\n        signature,\n        CompletionSignaturePayload(\n            agent_name=self.get_primary_agent().get_full_name(),\n            completion=completion,\n            model=params.model,\n            messages=ordered_messages_without_ids,\n            temperature=params.temperature,\n            max_tokens=params.max_tokens,\n        ),\n    )\n</code></pre>"},{"location":"api/#nearai.agents.tool_json_helper","title":"tool_json_helper","text":""},{"location":"api/#nearai.agents.tool_json_helper.parse_json_args","title":"parse_json_args","text":"<pre><code>parse_json_args(signature: dict, args: str)\n</code></pre> <p>Parses LLM generated JSON args, trying various repair strategies if args are not valid JSON.</p> Source code in <code>nearai/agents/tool_json_helper.py</code> <pre><code>def parse_json_args(signature: dict, args: str):\n    \"\"\"Parses LLM generated JSON args, trying various repair strategies if args are not valid JSON.\"\"\"\n    # if args is empty or an empty json object check if the function has no arguments\n    if not args or args == \"{}\":\n        if not signature[\"function\"][\"parameters\"][\"required\"]:\n            return {}\n        else:\n            raise ValueError(\"Function requires arguments\")\n\n    transforms = [\n        lambda x: json.loads(x),\n        _ending_transform,\n        lambda x: parse_json_args_based_on_signature(signature, x),\n    ]\n\n    for transform in transforms:\n        try:\n            result = transform(args)\n            # check that all result keys are valid properties in the signature\n            for key in result.keys():\n                if key not in signature[\"function\"][\"parameters\"][\"properties\"]:\n                    raise json.JSONDecodeError(f\"Unknown parameter {key}\", args, 0)\n            return result\n        except json.JSONDecodeError:\n            continue\n        except Exception as err:\n            raise json.JSONDecodeError(\"Error parsing function args\", args, 0) from err\n</code></pre>"},{"location":"api/#nearai.agents.tool_json_helper.parse_json_args_based_on_signature","title":"parse_json_args_based_on_signature","text":"<pre><code>parse_json_args_based_on_signature(signature: dict, args: str)\n</code></pre> <p>Finds parameter names based on the signature and tries to extract the values in between from the args string.</p> Source code in <code>nearai/agents/tool_json_helper.py</code> <pre><code>def parse_json_args_based_on_signature(signature: dict, args: str):\n    \"\"\"Finds parameter names based on the signature and tries to extract the values in between from the args string.\"\"\"\n    parameter_names = list(signature[\"function\"][\"parameters\"][\"properties\"].keys())\n    # find each parameter name in the args string\n    #   assuming each parameter name is surrounded by \"s, followed by a colon and optionally preceded by a comma,\n    #   extract the intervening values as values\n    parameter_positions = {}\n    parameter_values = {}\n    for param in parameter_names:\n        match = re.search(f',?\\\\s*\"({param})\"\\\\s*:', args)\n        if not match:\n            raise ValueError(f\"Parameter {param} not found in args {args}\")\n        parameter_positions[param] = (match.start(), match.end())\n    # sort the parameter positions by start position\n    sorted_positions = sorted(parameter_positions.items(), key=lambda x: x[1][0])\n    # for each parameter, extract the value from the args string\n    for i, (param, (start, end)) in enumerate(sorted_positions):  # noqa B007\n        # if this is the last parameter, extract the value from the start position to the end of the string\n        if i == len(sorted_positions) - 1:\n            raw_value = args[end:-1]\n            if raw_value.endswith(\"}\"):\n                raw_value = raw_value[:-1]\n        # otherwise, extract the value from the start position to the start position of the next parameter\n        else:\n            next_start = sorted_positions[i + 1][1][0]\n            raw_value = args[end:next_start]\n        raw_value = raw_value.strip()\n        if raw_value.startswith('\"') and raw_value.endswith('\"'):\n            raw_value = raw_value[1:-1]\n        parameter_values[param] = raw_value\n    return parameter_values\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry","title":"tool_registry","text":""},{"location":"api/#nearai.agents.tool_registry.ToolRegistry","title":"ToolRegistry","text":"<p>A registry for tools that can be called by the agent.</p> <p>Tool definitions follow this structure:</p> <pre><code>{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n                },\n                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n            },\n            \"required\": [\"location\"],\n        },\n    },\n}\n</code></pre> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>class ToolRegistry:\n    \"\"\"A registry for tools that can be called by the agent.\n\n    Tool definitions follow this structure:\n\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_current_weather\",\n                \"description\": \"Get the current weather in a given location\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                    },\n                    \"required\": [\"location\"],\n                },\n            },\n        }\n\n    \"\"\"\n\n    def __init__(self) -&gt; None:  # noqa: D107\n        self.tools: Dict[str, Callable] = {}\n\n    def register_tool(self, tool: Callable) -&gt; None:  # noqa: D102\n        \"\"\"Register a tool.\"\"\"\n        self.tools[tool.__name__] = tool\n\n    def get_tool(self, name: str) -&gt; Optional[Callable]:  # noqa: D102\n        \"\"\"Get a tool by name.\"\"\"\n        return self.tools.get(name)\n\n    def get_all_tools(self) -&gt; Dict[str, Callable]:  # noqa: D102\n        \"\"\"Get all tools.\"\"\"\n        return self.tools\n\n    def call_tool(self, name: str, **kwargs: Any) -&gt; Any:  # noqa: D102\n        \"\"\"Call a tool by name.\"\"\"\n        tool = self.get_tool(name)\n        if tool is None:\n            raise ValueError(f\"Tool '{name}' not found.\")\n        return tool(**kwargs)\n\n    def get_tool_definition(self, name: str) -&gt; Optional[Dict]:  # noqa: D102\n        \"\"\"Get the definition of a tool by name.\"\"\"\n        tool = self.get_tool(name)\n        if tool is None:\n            return None\n\n        assert tool.__doc__ is not None, f\"Docstring missing for tool '{name}'.\"\n        docstring = tool.__doc__.strip().split(\"\\n\")\n\n        # The first line of the docstring is the function description\n        function_description = docstring[0].strip()\n\n        # The rest of the lines contain parameter descriptions\n        param_descriptions = docstring[1:]\n\n        # Extract parameter names and types\n        signature = inspect.signature(tool)\n        type_hints = get_type_hints(tool)\n\n        parameters: Dict[str, Any] = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n\n        # Iterate through function parameters\n        for param in signature.parameters.values():\n            param_name = param.name\n            param_type = type_hints.get(param_name, str)  # Default to str if type hint is missing\n            param_description = \"\"\n\n            # Find the parameter description in the docstring\n            for line in param_descriptions:\n                if line.strip().startswith(param_name):\n                    param_description = line.strip().split(\":\", 1)[1].strip()\n                    break\n\n            # Convert type hint to JSON Schema type\n            if isinstance(param_type, _GenericAlias) and param_type.__origin__ is Literal:\n                json_type = \"string\"\n            else:\n                json_type = param_type.__name__.lower()\n\n            if json_type == \"union\":\n                json_type = [t.__name__.lower() for t in param_type.__args__][0]\n\n            json_type = {\"int\": \"integer\", \"float\": \"number\", \"str\": \"string\", \"bool\": \"boolean\"}.get(\n                json_type, \"string\"\n            )\n\n            # Add parameter to the definition\n            parameters[\"properties\"][param_name] = {\"description\": param_description, \"type\": json_type}\n\n            # Params without default values are required params\n            if param.default == inspect.Parameter.empty:\n                parameters[\"required\"].append(param_name)\n\n        return {\n            \"type\": \"function\",\n            \"function\": {\"name\": tool.__name__, \"description\": function_description, \"parameters\": parameters},\n        }\n\n    def get_all_tool_definitions(self) -&gt; list[Dict]:  # noqa: D102\n        definitions = []\n        for tool_name, _tool in self.tools.items():\n            definition = self.get_tool_definition(tool_name)\n            if definition is not None:\n                definitions.append(definition)\n        return definitions\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.call_tool","title":"call_tool","text":"<pre><code>call_tool(name: str, **kwargs: Any) -&gt; Any\n</code></pre> <p>Call a tool by name.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def call_tool(self, name: str, **kwargs: Any) -&gt; Any:  # noqa: D102\n    \"\"\"Call a tool by name.\"\"\"\n    tool = self.get_tool(name)\n    if tool is None:\n        raise ValueError(f\"Tool '{name}' not found.\")\n    return tool(**kwargs)\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.get_all_tools","title":"get_all_tools","text":"<pre><code>get_all_tools() -&gt; Dict[str, Callable]\n</code></pre> <p>Get all tools.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def get_all_tools(self) -&gt; Dict[str, Callable]:  # noqa: D102\n    \"\"\"Get all tools.\"\"\"\n    return self.tools\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.get_tool","title":"get_tool","text":"<pre><code>get_tool(name: str) -&gt; Optional[Callable]\n</code></pre> <p>Get a tool by name.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def get_tool(self, name: str) -&gt; Optional[Callable]:  # noqa: D102\n    \"\"\"Get a tool by name.\"\"\"\n    return self.tools.get(name)\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.get_tool_definition","title":"get_tool_definition","text":"<pre><code>get_tool_definition(name: str) -&gt; Optional[Dict]\n</code></pre> <p>Get the definition of a tool by name.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def get_tool_definition(self, name: str) -&gt; Optional[Dict]:  # noqa: D102\n    \"\"\"Get the definition of a tool by name.\"\"\"\n    tool = self.get_tool(name)\n    if tool is None:\n        return None\n\n    assert tool.__doc__ is not None, f\"Docstring missing for tool '{name}'.\"\n    docstring = tool.__doc__.strip().split(\"\\n\")\n\n    # The first line of the docstring is the function description\n    function_description = docstring[0].strip()\n\n    # The rest of the lines contain parameter descriptions\n    param_descriptions = docstring[1:]\n\n    # Extract parameter names and types\n    signature = inspect.signature(tool)\n    type_hints = get_type_hints(tool)\n\n    parameters: Dict[str, Any] = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n\n    # Iterate through function parameters\n    for param in signature.parameters.values():\n        param_name = param.name\n        param_type = type_hints.get(param_name, str)  # Default to str if type hint is missing\n        param_description = \"\"\n\n        # Find the parameter description in the docstring\n        for line in param_descriptions:\n            if line.strip().startswith(param_name):\n                param_description = line.strip().split(\":\", 1)[1].strip()\n                break\n\n        # Convert type hint to JSON Schema type\n        if isinstance(param_type, _GenericAlias) and param_type.__origin__ is Literal:\n            json_type = \"string\"\n        else:\n            json_type = param_type.__name__.lower()\n\n        if json_type == \"union\":\n            json_type = [t.__name__.lower() for t in param_type.__args__][0]\n\n        json_type = {\"int\": \"integer\", \"float\": \"number\", \"str\": \"string\", \"bool\": \"boolean\"}.get(\n            json_type, \"string\"\n        )\n\n        # Add parameter to the definition\n        parameters[\"properties\"][param_name] = {\"description\": param_description, \"type\": json_type}\n\n        # Params without default values are required params\n        if param.default == inspect.Parameter.empty:\n            parameters[\"required\"].append(param_name)\n\n    return {\n        \"type\": \"function\",\n        \"function\": {\"name\": tool.__name__, \"description\": function_description, \"parameters\": parameters},\n    }\n</code></pre>"},{"location":"api/#nearai.agents.tool_registry.ToolRegistry.register_tool","title":"register_tool","text":"<pre><code>register_tool(tool: Callable) -&gt; None\n</code></pre> <p>Register a tool.</p> Source code in <code>nearai/agents/tool_registry.py</code> <pre><code>def register_tool(self, tool: Callable) -&gt; None:  # noqa: D102\n    \"\"\"Register a tool.\"\"\"\n    self.tools[tool.__name__] = tool\n</code></pre>"},{"location":"api/#nearai.cli","title":"cli","text":""},{"location":"api/#nearai.cli.AgentCli","title":"AgentCli","text":"Source code in <code>nearai/cli.py</code> <pre><code>class AgentCli:\n    def dev(self) -&gt; int:\n        \"\"\"Run local UI for development of agents that have their own UI.\"\"\"\n        if not os.path.exists(\"hub/demo/.env\"):\n            shutil.copy(\"hub/demo/.env.example\", \"hub/demo/.env\")\n\n        ret_val = os.system(\"npm install --prefix hub/demo\")\n        if ret_val != 0:\n            print(\"Node.js is required to run the development server.\")\n            print(\"Please install Node.js from https://nodejs.org/\")\n        ret_val = os.system(\"npm run dev --prefix hub/demo\")\n        return ret_val\n\n    def inspect(self, path: str) -&gt; None:\n        \"\"\"Inspect environment from given path.\"\"\"\n        import subprocess\n\n        filename = Path(os.path.abspath(__file__)).parent / \"streamlit_inspect.py\"\n        subprocess.call([\"streamlit\", \"run\", filename, \"--\", path])\n\n    def interactive(\n        self,\n        agent: Optional[str] = None,\n        thread_id: Optional[str] = None,\n        tool_resources: Optional[Dict[str, Any]] = None,\n        local: bool = False,\n        verbose: bool = False,\n        env_vars: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"Runs agent interactively.\n\n        Args:\n        ----\n            agent: Optional path to the agent directory. If not provided, will show agent selection menu\n            thread_id: Optional thread ID to continue an existing conversation\n            tool_resources: Optional tool resources to pass to the agent\n            local: Whether to run the agent locally (default: False)\n            verbose: Whether to show detailed debug information during execution\n            env_vars: Optional environment variables to pass to the agent\n\n        \"\"\"\n        assert_user_auth()\n\n        if agent is None:\n            # List available agents in the registry folder\n            registry_path = Path(get_registry_folder())\n            if not registry_path.exists():\n                print(\"Error: Registry folder not found. Please create an agent first.\")\n                return\n\n            agents = []\n            # Walk through registry to find agents\n            for namespace in registry_path.iterdir():\n                if namespace.is_dir():\n                    for agent_name in namespace.iterdir():\n                        if agent_name.is_dir():\n                            for version in agent_name.iterdir():\n                                if version.is_dir():\n                                    agents.append(version)\n\n            if not agents:\n                print(\"No agents found. Please create an agent first with 'nearai agent create'\")\n                return\n\n            # Sort agents by namespace then name\n            agents = sorted(agents, key=lambda x: (x.parts[-3], x.parts[-2]))\n            display_agents_in_columns(agents)\n\n            while True:\n                try:\n                    choice = int(Prompt.ask(\"[blue bold]Select an agent (enter number)\")) - 1\n                    if 0 &lt;= choice &lt; len(agents):\n                        agent = str(agents[choice])\n                        break\n                    print(\"Invalid selection. Please try again.\")\n                except ValueError:\n                    print(\"Please enter a valid number.\")\n                except KeyboardInterrupt:\n                    print(\"\\nOperation cancelled.\")\n                    return\n\n        # Convert agent path to Path object if it's a string\n        agent_path = Path(agent)\n        if not agent_path.exists():\n            print(f\"Error: Agent not found at path: {agent_path}\")\n            return\n\n        try:\n            # Get the last 3 parts of the path (namespace/name/version)\n            parts = agent_path.parts[-3:]\n            agent_id = \"/\".join(parts)\n        except IndexError:\n            print(\"Error: Invalid agent path. Expected format: path/to/namespace/name/version\")\n            print(\"Example: ~/.nearai/registry/namespace/agent-name/0.0.1\")\n            return\n\n        last_message_id = None\n        print(f\"\\n=== Starting interactive session with agent: {agent_id} ===\")\n        print(\"Type 'exit' to end the session\\n\")\n\n        while True:\n            new_message = input(\"&gt; \")\n            if new_message.lower() == \"exit\":\n                break\n\n            last_message_id = self._task(\n                agent=agent_id,\n                task=new_message,\n                thread_id=thread_id,\n                tool_resources=tool_resources,\n                last_message_id=last_message_id,\n                local=local,\n                verbose=verbose,\n                env_vars=env_vars,\n            )\n\n            # Update thread_id for the next iteration\n            if thread_id is None:\n                thread_id = self.last_thread_id\n\n    def task(\n        self,\n        agent: str,\n        task: str,\n        thread_id: Optional[str] = None,\n        tool_resources: Optional[Dict[str, Any]] = None,\n        file_ids: Optional[List[str]] = None,\n        local: bool = False,\n        verbose: bool = False,\n        env_vars: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"CLI wrapper for the _task method.\"\"\"\n        last_message_id = self._task(\n            agent=agent,\n            task=task,\n            thread_id=thread_id,\n            tool_resources=tool_resources,\n            file_ids=file_ids,\n            local=local,\n            verbose=verbose,\n            env_vars=env_vars,\n        )\n        if last_message_id:\n            print(f\"Task completed. Thread ID: {self.last_thread_id}\")\n            print(f\"Last message ID: {last_message_id}\")\n\n    def _task(\n        self,\n        agent: str,\n        task: str,\n        thread_id: Optional[str] = None,\n        tool_resources: Optional[Dict[str, Any]] = None,\n        file_ids: Optional[List[str]] = None,\n        last_message_id: Optional[str] = None,\n        local: bool = False,\n        verbose: bool = False,\n        env_vars: Optional[Dict[str, Any]] = None,\n    ) -&gt; Optional[str]:\n        \"\"\"Runs agent non-interactively with a single task.\"\"\"\n        assert_user_auth()\n\n        hub_client = get_hub_client()\n        if thread_id:\n            thread = hub_client.beta.threads.retrieve(thread_id)\n        else:\n            thread = hub_client.beta.threads.create(\n                tool_resources=tool_resources,\n            )\n\n        hub_client.beta.threads.messages.create(\n            thread_id=thread.id,\n            role=\"user\",\n            content=task,\n            attachments=[Attachment(file_id=file_id) for file_id in file_ids] if file_ids else None,\n        )\n\n        if not local:\n            hub_client.beta.threads.runs.create_and_poll(\n                thread_id=thread.id,\n                assistant_id=agent,\n            )\n        else:\n            run = hub_client.beta.threads.runs.create(\n                thread_id=thread.id,\n                assistant_id=agent,\n                extra_body={\"delegate_execution\": True},\n            )\n            params = {\n                \"api_url\": CONFIG.api_url,\n                \"tool_resources\": run.tools,\n                \"data_source\": \"local_files\",\n                \"user_env_vars\": env_vars,\n                \"agent_env_vars\": {},\n                \"verbose\": verbose,\n            }\n            auth = CONFIG.auth\n            assert auth is not None\n            LocalRunner(agent, agent, thread.id, run.id, auth, params)\n\n        # List new messages\n        messages = hub_client.beta.threads.messages.list(thread_id=thread.id, after=last_message_id, order=\"asc\")\n        message_list = list(messages)\n        if message_list:\n            for msg in message_list:\n                if msg.metadata and msg.metadata.get(\"message_type\"):\n                    continue\n                if msg.role == \"assistant\":\n                    print(f\"Assistant: {msg.content[0].text.value}\")\n            last_message_id = message_list[-1].id\n        else:\n            print(\"No new messages\")\n\n        # Store the thread_id for potential use in interactive mode\n        self.last_thread_id = thread.id\n\n        return last_message_id\n\n    def create(self, name: Optional[str] = None, description: Optional[str] = None, fork: Optional[str] = None) -&gt; None:\n        \"\"\"Create a new agent or fork an existing one.\n\n        Usage:\n          nearai agent create  # Enters interactive mode\n          nearai agent create --name &lt;agent_name&gt; --description &lt;description&gt;\n          nearai agent create --fork &lt;namespace/agent_name/version&gt; [--name &lt;new_agent_name&gt;]\n\n        Options:\n          --name          Name of the new agent (optional).\n          --description   Description of the new agent (optional).\n          --fork          Fork an existing agent specified by namespace/agent_name/version.\n\n        Examples\n        --------\n          nearai agent create   # Enters interactive mode\n          nearai agent create --name my_agent --description \"My new agent\"\n          nearai agent create --fork agentic.near/summary/0.0.3 --name new_summary_agent\n\n        \"\"\"\n        # Check if the user is authenticated\n        if CONFIG.auth is None or CONFIG.auth.namespace is None:\n            print(\"Please login with `nearai login` before creating an agent.\")\n            return\n\n        namespace = CONFIG.auth.namespace\n\n        if fork:\n            # Fork an existing agent\n            self._fork_agent(fork, namespace, name)\n        else:\n            # Create a new agent from scratch\n            self._create_new_agent(namespace, name, description)\n\n    def _create_new_agent(self, namespace: str, name: Optional[str], description: Optional[str]) -&gt; None:\n        \"\"\"Create a new agent from scratch.\"\"\"\n        # If no name/description provided, use interactive prompts\n        init_instructions = \"\"\n        if name is None and description is None:\n            _, name, description, init_instructions = self._prompt_agent_details()\n\n        # Set the agent path\n        registry_folder = get_registry_folder()\n        if registry_folder is None:\n            raise ValueError(\"Registry folder path cannot be None\")\n\n        # Narrow the type of namespace &amp; name from Optional[str] to str\n        namespace_str: str = namespace if namespace is not None else \"\"\n        if namespace_str == \"\":\n            raise ValueError(\"Namespace cannot be None or empty\")\n\n        name_str: str = name if name is not None else \"\"\n        if name_str == \"\":\n            raise ValueError(\"Name cannot be None or empty\")\n\n        agent_path = registry_folder / namespace_str / name_str / \"0.0.1\"\n        agent_path.mkdir(parents=True, exist_ok=True)\n\n        metadata: Dict[str, Any] = {\n            \"name\": name_str,\n            \"version\": \"0.0.1\",\n            \"description\": description or \"\",\n            \"category\": \"agent\",\n            \"tags\": [],\n            \"details\": {\n                \"agent\": {\n                    \"defaults\": {\n                        \"model\": DEFAULT_MODEL,\n                        \"model_provider\": DEFAULT_PROVIDER,\n                        \"model_temperature\": DEFAULT_MODEL_TEMPERATURE,\n                        \"model_max_tokens\": DEFAULT_MODEL_MAX_TOKENS,\n                    }\n                }\n            },\n            \"show_entry\": True,\n        }\n\n        metadata_path = agent_path / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n\n        # Create a default agent.py with the provided initial\n        agent_py_content = f\"\"\"from nearai.agents.environment import Environment\n\n\ndef run(env: Environment):\n    # Your agent code here\n    prompt = {{\"role\": \"system\", \"content\": \"{init_instructions}\"}}\n    result = env.completion([prompt] + env.list_messages())\n    env.add_reply(result)\n    env.request_user_input()\n\nrun(env)\n\n\"\"\"\n        agent_py_path = agent_path / \"agent.py\"\n        with open(agent_py_path, \"w\") as f:\n            f.write(agent_py_content)\n\n        # Create success message\n        console = Console()\n        success_title = Text(\" \ud83c\udf89 SUCCESS!\", style=\"bold green\")\n        path_text = Text.assemble((\"\\n  \u2022 New AI Agent created at: \", \"bold green\"), (f\"{agent_path}\", \"bold\"))\n\n        files_panel = Panel(\n            Text.assemble(\n                (\"Edit agent code here:\\n\\n\", \"yellow\"),\n                (f\"\ud83d\udcc4 - {agent_path}/agent.py\\n\", \"bold blue\"),\n                (f\"\ud83d\udcc4 - {agent_path}/metadata.json\", \"bold blue\"),\n            ),\n            title=\"Agent Files\",\n            border_style=\"yellow\",\n        )\n\n        commands_panel = Panel(\n            Text.assemble(\n                (\"Run this agent locally:\\n\", \"light_green\"),\n                (f\"  nearai agent interactive {agent_path} --local\\n\\n\", \"bold\"),\n                (\"Upload this agent to NEAR AI's public registry:\\n\", \"light_green\"),\n                (f\"  nearai registry upload {agent_path}\\n\\n\", \"bold\"),\n                (\"Run ANY agent from your local registry:\\n\", \"light_green\"),\n                (\"  nearai agent interactive --local\", \"bold\"),\n            ),\n            title=\"Useful Commands\",\n            border_style=\"green\",\n        )\n\n        console.print(\"\\n\")\n        console.print(success_title)\n        console.print(path_text)\n        console.print(\"\\n\")\n        console.print(files_panel)\n        console.print(\"\\n\")\n        console.print(commands_panel)\n        console.print(\"\\n\")\n\n    def _fork_agent(self, fork: str, namespace: str, new_name: Optional[str]) -&gt; None:\n        \"\"\"Fork an existing agent.\"\"\"\n        import shutil\n\n        # Parse the fork parameter\n        try:\n            entry_location = parse_location(fork)\n            fork_namespace = entry_location.namespace\n            fork_name = entry_location.name\n            fork_version = entry_location.version\n        except ValueError:\n            print(\"Invalid fork parameter format. Expected format: &lt;namespace&gt;/&lt;agent-name&gt;/&lt;version&gt;\")\n            return\n\n        # Download the agent from the registry\n        agent_location = f\"{fork_namespace}/{fork_name}/{fork_version}\"\n        print(f\"Downloading agent '{agent_location}'...\")\n        registry.download(agent_location, force=False, show_progress=True)\n        source_path = get_registry_folder() / fork_namespace / fork_name / fork_version\n\n        # Prompt for the new agent name if not provided\n        if not new_name:\n            new_name = input(\"Enter the new agent name: \").strip()\n            if not new_name:\n                print(\"Agent name cannot be empty.\")\n                return\n\n            # confirm pattern is ok\n            identifier_pattern = re.compile(r\"^[a-zA-Z0-9_\\-.]+$\")\n            if identifier_pattern.match(new_name) is None:\n                print(\"Invalid Name, please choose something different\")\n                return\n\n        # Set the destination path\n        dest_path = get_registry_folder() / namespace / new_name / \"0.0.1\"\n\n        # Copy the agent files\n        shutil.copytree(source_path, dest_path)\n\n        # Update metadata.json\n        metadata_path = dest_path / \"metadata.json\"\n        with open(metadata_path, \"r\") as file:\n            metadata = json.load(file)\n\n        metadata[\"name\"] = new_name\n        metadata[\"version\"] = \"0.0.1\"\n\n        with open(metadata_path, \"w\") as file:\n            json.dump(metadata, file, indent=2)\n\n        print(f\"\\nForked agent '{agent_location}' to '{dest_path}'\")\n        print(f\"Agent '{new_name}' created at '{dest_path}' with updated metadata.\")\n        print(\"\\nUseful commands:\")\n        print(f\"  &gt; nearai agent interactive {new_name} --local\")\n        print(f\"  &gt; nearai registry upload {dest_path}\")\n\n    def _prompt_agent_details(self) -&gt; Tuple[str, str, str, str]:\n        console = Console()\n\n        # Get namespace from CONFIG, with null check\n        if CONFIG.auth is None:\n            raise ValueError(\"Not logged in. Please run 'nearai login' first.\")\n        namespace = CONFIG.auth.namespace\n\n        # Welcome message\n        console.print(NEAR_AI_BANNER)\n        welcome_panel = Panel(\n            Text.assemble(\n                (\"Let's create a new agent! \ud83e\uddbe \\n\", \"bold green\"),\n                (\"We'll need some basic information to get started.\", \"dim\"),\n            ),\n            title=\"Agent Creator\",\n            border_style=\"green\",\n        )\n        console.print(welcome_panel)\n        console.print(\"\\n\")\n\n        # Name prompt with explanation\n        name_info = Panel(\n            Text.assemble(\n                (\"Choose a unique name for your agent using only:\\n\\n\", \"\"),\n                (\"\u2022 letters\\n\", \"dim\"),\n                (\"\u2022 numbers\\n\", \"dim\"),\n                (\"\u2022 dots (.)\\n\", \"dim\"),\n                (\"\u2022 hyphens (-)\\n\", \"dim\"),\n                (\"\u2022 underscores (_)\\n\\n\", \"dim\"),\n                (\"Examples: 'code-reviewer', 'data.analyzer', 'text_summarizer'\", \"green\"),\n            ),\n            title=\"Agent Name Rules\",\n            border_style=\"blue\",\n        )\n        console.print(name_info)\n\n        while True:\n            name = Prompt.ask(\"[bold blue]Enter agent name\").strip()\n            # Validate name format\n            if not re.match(r\"^[a-zA-Z0-9][a-zA-Z0-9._-]*$\", name):\n                console.print(\n                    \"[red]\u274c Invalid name format. \" \"Please use only letters, numbers, dots, hyphens, or underscores.\"\n                )\n                continue\n            if \" \" in name:\n                console.print(\"[red]\u274c Spaces are not allowed. Use dots, hyphens, or underscores instead.\")\n                continue\n            break\n\n        console.print(\"\\n\")\n\n        # Description prompt\n        description_info = Panel(\n            \"Describe what your agent will do in a few words...\", title=\"Description Info\", border_style=\"blue\"\n        )\n        console.print(description_info)\n        description = Prompt.ask(\"[bold blue]Enter description\")\n\n        console.print(\"\\n\")\n\n        # Initial instructions prompt\n        init_instructions_info = Panel(\n            Text.assemble(\n                (\"Provide initial instructions for your AI agent...\\n\\n\", \"\"),\n                (\"This will be used as the system message to guide the agent's behavior.\\n\", \"dim\"),\n                (\"You can edit these instructions later in the `agent.py` file.\\n\\n\", \"dim\"),\n                (\n                    \"Example: You are a helpful humorous assistant. Use puns or jokes to make the user smile.\",\n                    \"green\",\n                ),\n            ),\n            title=\"Instructions\",\n            border_style=\"blue\",\n        )\n        console.print(init_instructions_info)\n        init_instructions = Prompt.ask(\"[bold blue]Enter instructions\")\n\n        # Confirmation\n        console.print(\"\\n\")\n        summary_panel = Panel(\n            Text.assemble(\n                (\"Summary of your new agent:\\n\\n\", \"bold\"),\n                (\"Namespace/Account:    \", \"dim\"),\n                (f\"{namespace}\\n\", \"green\"),\n                (\"Agent Name:           \", \"dim\"),\n                (f\"{name}\\n\", \"green\"),\n                (\"Description:          \", \"dim\"),\n                (f\"{description}\\n\", \"green\"),\n                (\"Instructions:         \", \"dim\"),\n                (f\"{init_instructions}\", \"green\"),\n            ),\n            title=\"\ud83d\udccb Review\",\n            border_style=\"green\",\n        )\n        console.print(summary_panel)\n        console.print(\"\\n\")\n\n        if not Confirm.ask(\"[bold]Would you like to proceed?\", default=True):\n            console.print(\"[red]\u274c Agent creation cancelled\")\n            sys.exit(0)\n        return namespace, name, description, init_instructions\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli._create_new_agent","title":"_create_new_agent","text":"<pre><code>_create_new_agent(namespace: str, name: Optional[str], description: Optional[str]) -&gt; None\n</code></pre> <p>Create a new agent from scratch.</p> Source code in <code>nearai/cli.py</code> <pre><code>    def _create_new_agent(self, namespace: str, name: Optional[str], description: Optional[str]) -&gt; None:\n        \"\"\"Create a new agent from scratch.\"\"\"\n        # If no name/description provided, use interactive prompts\n        init_instructions = \"\"\n        if name is None and description is None:\n            _, name, description, init_instructions = self._prompt_agent_details()\n\n        # Set the agent path\n        registry_folder = get_registry_folder()\n        if registry_folder is None:\n            raise ValueError(\"Registry folder path cannot be None\")\n\n        # Narrow the type of namespace &amp; name from Optional[str] to str\n        namespace_str: str = namespace if namespace is not None else \"\"\n        if namespace_str == \"\":\n            raise ValueError(\"Namespace cannot be None or empty\")\n\n        name_str: str = name if name is not None else \"\"\n        if name_str == \"\":\n            raise ValueError(\"Name cannot be None or empty\")\n\n        agent_path = registry_folder / namespace_str / name_str / \"0.0.1\"\n        agent_path.mkdir(parents=True, exist_ok=True)\n\n        metadata: Dict[str, Any] = {\n            \"name\": name_str,\n            \"version\": \"0.0.1\",\n            \"description\": description or \"\",\n            \"category\": \"agent\",\n            \"tags\": [],\n            \"details\": {\n                \"agent\": {\n                    \"defaults\": {\n                        \"model\": DEFAULT_MODEL,\n                        \"model_provider\": DEFAULT_PROVIDER,\n                        \"model_temperature\": DEFAULT_MODEL_TEMPERATURE,\n                        \"model_max_tokens\": DEFAULT_MODEL_MAX_TOKENS,\n                    }\n                }\n            },\n            \"show_entry\": True,\n        }\n\n        metadata_path = agent_path / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n\n        # Create a default agent.py with the provided initial\n        agent_py_content = f\"\"\"from nearai.agents.environment import Environment\n\n\ndef run(env: Environment):\n    # Your agent code here\n    prompt = {{\"role\": \"system\", \"content\": \"{init_instructions}\"}}\n    result = env.completion([prompt] + env.list_messages())\n    env.add_reply(result)\n    env.request_user_input()\n\nrun(env)\n\n\"\"\"\n        agent_py_path = agent_path / \"agent.py\"\n        with open(agent_py_path, \"w\") as f:\n            f.write(agent_py_content)\n\n        # Create success message\n        console = Console()\n        success_title = Text(\" \ud83c\udf89 SUCCESS!\", style=\"bold green\")\n        path_text = Text.assemble((\"\\n  \u2022 New AI Agent created at: \", \"bold green\"), (f\"{agent_path}\", \"bold\"))\n\n        files_panel = Panel(\n            Text.assemble(\n                (\"Edit agent code here:\\n\\n\", \"yellow\"),\n                (f\"\ud83d\udcc4 - {agent_path}/agent.py\\n\", \"bold blue\"),\n                (f\"\ud83d\udcc4 - {agent_path}/metadata.json\", \"bold blue\"),\n            ),\n            title=\"Agent Files\",\n            border_style=\"yellow\",\n        )\n\n        commands_panel = Panel(\n            Text.assemble(\n                (\"Run this agent locally:\\n\", \"light_green\"),\n                (f\"  nearai agent interactive {agent_path} --local\\n\\n\", \"bold\"),\n                (\"Upload this agent to NEAR AI's public registry:\\n\", \"light_green\"),\n                (f\"  nearai registry upload {agent_path}\\n\\n\", \"bold\"),\n                (\"Run ANY agent from your local registry:\\n\", \"light_green\"),\n                (\"  nearai agent interactive --local\", \"bold\"),\n            ),\n            title=\"Useful Commands\",\n            border_style=\"green\",\n        )\n\n        console.print(\"\\n\")\n        console.print(success_title)\n        console.print(path_text)\n        console.print(\"\\n\")\n        console.print(files_panel)\n        console.print(\"\\n\")\n        console.print(commands_panel)\n        console.print(\"\\n\")\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli._fork_agent","title":"_fork_agent","text":"<pre><code>_fork_agent(fork: str, namespace: str, new_name: Optional[str]) -&gt; None\n</code></pre> <p>Fork an existing agent.</p> Source code in <code>nearai/cli.py</code> <pre><code>def _fork_agent(self, fork: str, namespace: str, new_name: Optional[str]) -&gt; None:\n    \"\"\"Fork an existing agent.\"\"\"\n    import shutil\n\n    # Parse the fork parameter\n    try:\n        entry_location = parse_location(fork)\n        fork_namespace = entry_location.namespace\n        fork_name = entry_location.name\n        fork_version = entry_location.version\n    except ValueError:\n        print(\"Invalid fork parameter format. Expected format: &lt;namespace&gt;/&lt;agent-name&gt;/&lt;version&gt;\")\n        return\n\n    # Download the agent from the registry\n    agent_location = f\"{fork_namespace}/{fork_name}/{fork_version}\"\n    print(f\"Downloading agent '{agent_location}'...\")\n    registry.download(agent_location, force=False, show_progress=True)\n    source_path = get_registry_folder() / fork_namespace / fork_name / fork_version\n\n    # Prompt for the new agent name if not provided\n    if not new_name:\n        new_name = input(\"Enter the new agent name: \").strip()\n        if not new_name:\n            print(\"Agent name cannot be empty.\")\n            return\n\n        # confirm pattern is ok\n        identifier_pattern = re.compile(r\"^[a-zA-Z0-9_\\-.]+$\")\n        if identifier_pattern.match(new_name) is None:\n            print(\"Invalid Name, please choose something different\")\n            return\n\n    # Set the destination path\n    dest_path = get_registry_folder() / namespace / new_name / \"0.0.1\"\n\n    # Copy the agent files\n    shutil.copytree(source_path, dest_path)\n\n    # Update metadata.json\n    metadata_path = dest_path / \"metadata.json\"\n    with open(metadata_path, \"r\") as file:\n        metadata = json.load(file)\n\n    metadata[\"name\"] = new_name\n    metadata[\"version\"] = \"0.0.1\"\n\n    with open(metadata_path, \"w\") as file:\n        json.dump(metadata, file, indent=2)\n\n    print(f\"\\nForked agent '{agent_location}' to '{dest_path}'\")\n    print(f\"Agent '{new_name}' created at '{dest_path}' with updated metadata.\")\n    print(\"\\nUseful commands:\")\n    print(f\"  &gt; nearai agent interactive {new_name} --local\")\n    print(f\"  &gt; nearai registry upload {dest_path}\")\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli._task","title":"_task","text":"<pre><code>_task(agent: str, task: str, thread_id: Optional[str] = None, tool_resources: Optional[Dict[str, Any]] = None, file_ids: Optional[List[str]] = None, last_message_id: Optional[str] = None, local: bool = False, verbose: bool = False, env_vars: Optional[Dict[str, Any]] = None) -&gt; Optional[str]\n</code></pre> <p>Runs agent non-interactively with a single task.</p> Source code in <code>nearai/cli.py</code> <pre><code>def _task(\n    self,\n    agent: str,\n    task: str,\n    thread_id: Optional[str] = None,\n    tool_resources: Optional[Dict[str, Any]] = None,\n    file_ids: Optional[List[str]] = None,\n    last_message_id: Optional[str] = None,\n    local: bool = False,\n    verbose: bool = False,\n    env_vars: Optional[Dict[str, Any]] = None,\n) -&gt; Optional[str]:\n    \"\"\"Runs agent non-interactively with a single task.\"\"\"\n    assert_user_auth()\n\n    hub_client = get_hub_client()\n    if thread_id:\n        thread = hub_client.beta.threads.retrieve(thread_id)\n    else:\n        thread = hub_client.beta.threads.create(\n            tool_resources=tool_resources,\n        )\n\n    hub_client.beta.threads.messages.create(\n        thread_id=thread.id,\n        role=\"user\",\n        content=task,\n        attachments=[Attachment(file_id=file_id) for file_id in file_ids] if file_ids else None,\n    )\n\n    if not local:\n        hub_client.beta.threads.runs.create_and_poll(\n            thread_id=thread.id,\n            assistant_id=agent,\n        )\n    else:\n        run = hub_client.beta.threads.runs.create(\n            thread_id=thread.id,\n            assistant_id=agent,\n            extra_body={\"delegate_execution\": True},\n        )\n        params = {\n            \"api_url\": CONFIG.api_url,\n            \"tool_resources\": run.tools,\n            \"data_source\": \"local_files\",\n            \"user_env_vars\": env_vars,\n            \"agent_env_vars\": {},\n            \"verbose\": verbose,\n        }\n        auth = CONFIG.auth\n        assert auth is not None\n        LocalRunner(agent, agent, thread.id, run.id, auth, params)\n\n    # List new messages\n    messages = hub_client.beta.threads.messages.list(thread_id=thread.id, after=last_message_id, order=\"asc\")\n    message_list = list(messages)\n    if message_list:\n        for msg in message_list:\n            if msg.metadata and msg.metadata.get(\"message_type\"):\n                continue\n            if msg.role == \"assistant\":\n                print(f\"Assistant: {msg.content[0].text.value}\")\n        last_message_id = message_list[-1].id\n    else:\n        print(\"No new messages\")\n\n    # Store the thread_id for potential use in interactive mode\n    self.last_thread_id = thread.id\n\n    return last_message_id\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.create","title":"create","text":"<pre><code>create(name: Optional[str] = None, description: Optional[str] = None, fork: Optional[str] = None) -&gt; None\n</code></pre> <p>Create a new agent or fork an existing one.</p> Usage <p>nearai agent create  # Enters interactive mode nearai agent create --name  --description  nearai agent create --fork  [--name ] Options <p>--name          Name of the new agent (optional). --description   Description of the new agent (optional). --fork          Fork an existing agent specified by namespace/agent_name/version.</p>"},{"location":"api/#nearai.cli.AgentCli.create--examples","title":"Examples","text":"<p>nearai agent create   # Enters interactive mode   nearai agent create --name my_agent --description \"My new agent\"   nearai agent create --fork agentic.near/summary/0.0.3 --name new_summary_agent</p> Source code in <code>nearai/cli.py</code> <pre><code>def create(self, name: Optional[str] = None, description: Optional[str] = None, fork: Optional[str] = None) -&gt; None:\n    \"\"\"Create a new agent or fork an existing one.\n\n    Usage:\n      nearai agent create  # Enters interactive mode\n      nearai agent create --name &lt;agent_name&gt; --description &lt;description&gt;\n      nearai agent create --fork &lt;namespace/agent_name/version&gt; [--name &lt;new_agent_name&gt;]\n\n    Options:\n      --name          Name of the new agent (optional).\n      --description   Description of the new agent (optional).\n      --fork          Fork an existing agent specified by namespace/agent_name/version.\n\n    Examples\n    --------\n      nearai agent create   # Enters interactive mode\n      nearai agent create --name my_agent --description \"My new agent\"\n      nearai agent create --fork agentic.near/summary/0.0.3 --name new_summary_agent\n\n    \"\"\"\n    # Check if the user is authenticated\n    if CONFIG.auth is None or CONFIG.auth.namespace is None:\n        print(\"Please login with `nearai login` before creating an agent.\")\n        return\n\n    namespace = CONFIG.auth.namespace\n\n    if fork:\n        # Fork an existing agent\n        self._fork_agent(fork, namespace, name)\n    else:\n        # Create a new agent from scratch\n        self._create_new_agent(namespace, name, description)\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.dev","title":"dev","text":"<pre><code>dev() -&gt; int\n</code></pre> <p>Run local UI for development of agents that have their own UI.</p> Source code in <code>nearai/cli.py</code> <pre><code>def dev(self) -&gt; int:\n    \"\"\"Run local UI for development of agents that have their own UI.\"\"\"\n    if not os.path.exists(\"hub/demo/.env\"):\n        shutil.copy(\"hub/demo/.env.example\", \"hub/demo/.env\")\n\n    ret_val = os.system(\"npm install --prefix hub/demo\")\n    if ret_val != 0:\n        print(\"Node.js is required to run the development server.\")\n        print(\"Please install Node.js from https://nodejs.org/\")\n    ret_val = os.system(\"npm run dev --prefix hub/demo\")\n    return ret_val\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.inspect","title":"inspect","text":"<pre><code>inspect(path: str) -&gt; None\n</code></pre> <p>Inspect environment from given path.</p> Source code in <code>nearai/cli.py</code> <pre><code>def inspect(self, path: str) -&gt; None:\n    \"\"\"Inspect environment from given path.\"\"\"\n    import subprocess\n\n    filename = Path(os.path.abspath(__file__)).parent / \"streamlit_inspect.py\"\n    subprocess.call([\"streamlit\", \"run\", filename, \"--\", path])\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.interactive","title":"interactive","text":"<pre><code>interactive(agent: Optional[str] = None, thread_id: Optional[str] = None, tool_resources: Optional[Dict[str, Any]] = None, local: bool = False, verbose: bool = False, env_vars: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Runs agent interactively.</p> <pre><code>agent: Optional path to the agent directory. If not provided, will show agent selection menu\nthread_id: Optional thread ID to continue an existing conversation\ntool_resources: Optional tool resources to pass to the agent\nlocal: Whether to run the agent locally (default: False)\nverbose: Whether to show detailed debug information during execution\nenv_vars: Optional environment variables to pass to the agent\n</code></pre> Source code in <code>nearai/cli.py</code> <pre><code>def interactive(\n    self,\n    agent: Optional[str] = None,\n    thread_id: Optional[str] = None,\n    tool_resources: Optional[Dict[str, Any]] = None,\n    local: bool = False,\n    verbose: bool = False,\n    env_vars: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"Runs agent interactively.\n\n    Args:\n    ----\n        agent: Optional path to the agent directory. If not provided, will show agent selection menu\n        thread_id: Optional thread ID to continue an existing conversation\n        tool_resources: Optional tool resources to pass to the agent\n        local: Whether to run the agent locally (default: False)\n        verbose: Whether to show detailed debug information during execution\n        env_vars: Optional environment variables to pass to the agent\n\n    \"\"\"\n    assert_user_auth()\n\n    if agent is None:\n        # List available agents in the registry folder\n        registry_path = Path(get_registry_folder())\n        if not registry_path.exists():\n            print(\"Error: Registry folder not found. Please create an agent first.\")\n            return\n\n        agents = []\n        # Walk through registry to find agents\n        for namespace in registry_path.iterdir():\n            if namespace.is_dir():\n                for agent_name in namespace.iterdir():\n                    if agent_name.is_dir():\n                        for version in agent_name.iterdir():\n                            if version.is_dir():\n                                agents.append(version)\n\n        if not agents:\n            print(\"No agents found. Please create an agent first with 'nearai agent create'\")\n            return\n\n        # Sort agents by namespace then name\n        agents = sorted(agents, key=lambda x: (x.parts[-3], x.parts[-2]))\n        display_agents_in_columns(agents)\n\n        while True:\n            try:\n                choice = int(Prompt.ask(\"[blue bold]Select an agent (enter number)\")) - 1\n                if 0 &lt;= choice &lt; len(agents):\n                    agent = str(agents[choice])\n                    break\n                print(\"Invalid selection. Please try again.\")\n            except ValueError:\n                print(\"Please enter a valid number.\")\n            except KeyboardInterrupt:\n                print(\"\\nOperation cancelled.\")\n                return\n\n    # Convert agent path to Path object if it's a string\n    agent_path = Path(agent)\n    if not agent_path.exists():\n        print(f\"Error: Agent not found at path: {agent_path}\")\n        return\n\n    try:\n        # Get the last 3 parts of the path (namespace/name/version)\n        parts = agent_path.parts[-3:]\n        agent_id = \"/\".join(parts)\n    except IndexError:\n        print(\"Error: Invalid agent path. Expected format: path/to/namespace/name/version\")\n        print(\"Example: ~/.nearai/registry/namespace/agent-name/0.0.1\")\n        return\n\n    last_message_id = None\n    print(f\"\\n=== Starting interactive session with agent: {agent_id} ===\")\n    print(\"Type 'exit' to end the session\\n\")\n\n    while True:\n        new_message = input(\"&gt; \")\n        if new_message.lower() == \"exit\":\n            break\n\n        last_message_id = self._task(\n            agent=agent_id,\n            task=new_message,\n            thread_id=thread_id,\n            tool_resources=tool_resources,\n            last_message_id=last_message_id,\n            local=local,\n            verbose=verbose,\n            env_vars=env_vars,\n        )\n\n        # Update thread_id for the next iteration\n        if thread_id is None:\n            thread_id = self.last_thread_id\n</code></pre>"},{"location":"api/#nearai.cli.AgentCli.task","title":"task","text":"<pre><code>task(agent: str, task: str, thread_id: Optional[str] = None, tool_resources: Optional[Dict[str, Any]] = None, file_ids: Optional[List[str]] = None, local: bool = False, verbose: bool = False, env_vars: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>CLI wrapper for the _task method.</p> Source code in <code>nearai/cli.py</code> <pre><code>def task(\n    self,\n    agent: str,\n    task: str,\n    thread_id: Optional[str] = None,\n    tool_resources: Optional[Dict[str, Any]] = None,\n    file_ids: Optional[List[str]] = None,\n    local: bool = False,\n    verbose: bool = False,\n    env_vars: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"CLI wrapper for the _task method.\"\"\"\n    last_message_id = self._task(\n        agent=agent,\n        task=task,\n        thread_id=thread_id,\n        tool_resources=tool_resources,\n        file_ids=file_ids,\n        local=local,\n        verbose=verbose,\n        env_vars=env_vars,\n    )\n    if last_message_id:\n        print(f\"Task completed. Thread ID: {self.last_thread_id}\")\n        print(f\"Last message ID: {last_message_id}\")\n</code></pre>"},{"location":"api/#nearai.cli.BenchmarkCli","title":"BenchmarkCli","text":"Source code in <code>nearai/cli.py</code> <pre><code>class BenchmarkCli:\n    def __init__(self):\n        \"\"\"Initialize Benchmark API.\"\"\"\n        self.client = BenchmarkApi()\n\n    def _get_or_create_benchmark(self, benchmark_name: str, solver_name: str, args: Dict[str, Any], force: bool) -&gt; int:\n        if CONFIG.auth is None:\n            print(\"Please login with `nearai login`\")\n            exit(1)\n        namespace = CONFIG.auth.namespace\n\n        # Sort the args to have a consistent representation.\n        solver_args = json.dumps(OrderedDict(sorted(args.items())))\n\n        benchmark_id = self.client.get_benchmark_v1_benchmark_get_get(\n            namespace=namespace,\n            benchmark_name=benchmark_name,\n            solver_name=solver_name,\n            solver_args=solver_args,\n        )\n\n        if benchmark_id == -1 or force:\n            benchmark_id = self.client.create_benchmark_v1_benchmark_create_get(\n                benchmark_name=benchmark_name,\n                solver_name=solver_name,\n                solver_args=solver_args,\n            )\n\n        assert benchmark_id != -1\n        return benchmark_id\n\n    def run(\n        self,\n        dataset: str,\n        solver_strategy: str,\n        max_concurrent: int = 2,\n        force: bool = False,\n        subset: Optional[str] = None,\n        check_compatibility: bool = True,\n        record: bool = False,\n        num_inference_retries: int = 10,\n        **solver_args: Any,\n    ) -&gt; None:\n        \"\"\"Run benchmark on a dataset with a solver strategy.\n\n        It will cache the results in the database and subsequent runs will pull the results from the cache.\n        If force is set to True, it will run the benchmark again and update the cache.\n        \"\"\"\n        from nearai.benchmark import BenchmarkExecutor, DatasetInfo\n        from nearai.dataset import get_dataset, load_dataset\n        from nearai.solvers import SolverScoringMethod, SolverStrategy, SolverStrategyRegistry\n\n        CONFIG.num_inference_retries = num_inference_retries\n\n        args = dict(solver_args)\n        if subset is not None:\n            args[\"subset\"] = subset\n\n        benchmark_id = self._get_or_create_benchmark(\n            benchmark_name=dataset,\n            solver_name=solver_strategy,\n            args=args,\n            force=force,\n        )\n\n        solver_strategy_class: Union[SolverStrategy, None] = SolverStrategyRegistry.get(solver_strategy, None)\n        assert (\n            solver_strategy_class\n        ), f\"Solver strategy {solver_strategy} not found. Available strategies: {list(SolverStrategyRegistry.keys())}\"\n\n        name = dataset\n        if solver_strategy_class.scoring_method == SolverScoringMethod.Custom:\n            dataset = str(get_dataset(dataset))\n        else:\n            dataset = load_dataset(dataset)\n\n        solver_strategy_obj: SolverStrategy = solver_strategy_class(dataset_ref=dataset, **solver_args)  # type: ignore\n        if check_compatibility:\n            assert name in solver_strategy_obj.compatible_datasets() or any(\n                map(lambda n: n in name, solver_strategy_obj.compatible_datasets())\n            ), f\"Solver strategy {solver_strategy} is not compatible with dataset {name}\"\n\n        dest_path = get_registry_folder() / name\n        metadata_path = dest_path / \"metadata.json\"\n        with open(metadata_path, \"r\") as file:\n            metadata = json.load(file)\n\n        be = BenchmarkExecutor(\n            DatasetInfo(name, subset, dataset, metadata), solver_strategy_obj, benchmark_id=benchmark_id\n        )\n\n        cpu_count = os.cpu_count()\n        max_concurrent = (cpu_count if cpu_count is not None else 1) if max_concurrent &lt; 0 else max_concurrent\n        be.run(max_concurrent=max_concurrent, record=record)\n\n    def list(\n        self,\n        namespace: Optional[str] = None,\n        benchmark: Optional[str] = None,\n        solver: Optional[str] = None,\n        args: Optional[str] = None,\n        total: int = 32,\n        offset: int = 0,\n    ):\n        \"\"\"List all executed benchmarks.\"\"\"\n        result = self.client.list_benchmarks_v1_benchmark_list_get(\n            namespace=namespace,\n            benchmark_name=benchmark,\n            solver_name=solver,\n            solver_args=args,\n            total=total,\n            offset=offset,\n        )\n\n        header = [\"id\", \"namespace\", \"benchmark\", \"solver\", \"args\", \"score\", \"solved\", \"total\"]\n        table = []\n        for benchmark_output in result:\n            score = 100 * benchmark_output.solved / benchmark_output.total\n            table.append(\n                [\n                    fill(str(benchmark_output.id)),\n                    fill(benchmark_output.namespace),\n                    fill(benchmark_output.benchmark),\n                    fill(benchmark_output.solver),\n                    fill(benchmark_output.args),\n                    fill(f\"{score:.2f}%\"),\n                    fill(str(benchmark_output.solved)),\n                    fill(str(benchmark_output.total)),\n                ]\n            )\n\n        print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n</code></pre>"},{"location":"api/#nearai.cli.BenchmarkCli.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize Benchmark API.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize Benchmark API.\"\"\"\n    self.client = BenchmarkApi()\n</code></pre>"},{"location":"api/#nearai.cli.BenchmarkCli.list","title":"list","text":"<pre><code>list(namespace: Optional[str] = None, benchmark: Optional[str] = None, solver: Optional[str] = None, args: Optional[str] = None, total: int = 32, offset: int = 0)\n</code></pre> <p>List all executed benchmarks.</p> Source code in <code>nearai/cli.py</code> <pre><code>def list(\n    self,\n    namespace: Optional[str] = None,\n    benchmark: Optional[str] = None,\n    solver: Optional[str] = None,\n    args: Optional[str] = None,\n    total: int = 32,\n    offset: int = 0,\n):\n    \"\"\"List all executed benchmarks.\"\"\"\n    result = self.client.list_benchmarks_v1_benchmark_list_get(\n        namespace=namespace,\n        benchmark_name=benchmark,\n        solver_name=solver,\n        solver_args=args,\n        total=total,\n        offset=offset,\n    )\n\n    header = [\"id\", \"namespace\", \"benchmark\", \"solver\", \"args\", \"score\", \"solved\", \"total\"]\n    table = []\n    for benchmark_output in result:\n        score = 100 * benchmark_output.solved / benchmark_output.total\n        table.append(\n            [\n                fill(str(benchmark_output.id)),\n                fill(benchmark_output.namespace),\n                fill(benchmark_output.benchmark),\n                fill(benchmark_output.solver),\n                fill(benchmark_output.args),\n                fill(f\"{score:.2f}%\"),\n                fill(str(benchmark_output.solved)),\n                fill(str(benchmark_output.total)),\n            ]\n        )\n\n    print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n</code></pre>"},{"location":"api/#nearai.cli.BenchmarkCli.run","title":"run","text":"<pre><code>run(dataset: str, solver_strategy: str, max_concurrent: int = 2, force: bool = False, subset: Optional[str] = None, check_compatibility: bool = True, record: bool = False, num_inference_retries: int = 10, **solver_args: Any) -&gt; None\n</code></pre> <p>Run benchmark on a dataset with a solver strategy.</p> <p>It will cache the results in the database and subsequent runs will pull the results from the cache. If force is set to True, it will run the benchmark again and update the cache.</p> Source code in <code>nearai/cli.py</code> <pre><code>def run(\n    self,\n    dataset: str,\n    solver_strategy: str,\n    max_concurrent: int = 2,\n    force: bool = False,\n    subset: Optional[str] = None,\n    check_compatibility: bool = True,\n    record: bool = False,\n    num_inference_retries: int = 10,\n    **solver_args: Any,\n) -&gt; None:\n    \"\"\"Run benchmark on a dataset with a solver strategy.\n\n    It will cache the results in the database and subsequent runs will pull the results from the cache.\n    If force is set to True, it will run the benchmark again and update the cache.\n    \"\"\"\n    from nearai.benchmark import BenchmarkExecutor, DatasetInfo\n    from nearai.dataset import get_dataset, load_dataset\n    from nearai.solvers import SolverScoringMethod, SolverStrategy, SolverStrategyRegistry\n\n    CONFIG.num_inference_retries = num_inference_retries\n\n    args = dict(solver_args)\n    if subset is not None:\n        args[\"subset\"] = subset\n\n    benchmark_id = self._get_or_create_benchmark(\n        benchmark_name=dataset,\n        solver_name=solver_strategy,\n        args=args,\n        force=force,\n    )\n\n    solver_strategy_class: Union[SolverStrategy, None] = SolverStrategyRegistry.get(solver_strategy, None)\n    assert (\n        solver_strategy_class\n    ), f\"Solver strategy {solver_strategy} not found. Available strategies: {list(SolverStrategyRegistry.keys())}\"\n\n    name = dataset\n    if solver_strategy_class.scoring_method == SolverScoringMethod.Custom:\n        dataset = str(get_dataset(dataset))\n    else:\n        dataset = load_dataset(dataset)\n\n    solver_strategy_obj: SolverStrategy = solver_strategy_class(dataset_ref=dataset, **solver_args)  # type: ignore\n    if check_compatibility:\n        assert name in solver_strategy_obj.compatible_datasets() or any(\n            map(lambda n: n in name, solver_strategy_obj.compatible_datasets())\n        ), f\"Solver strategy {solver_strategy} is not compatible with dataset {name}\"\n\n    dest_path = get_registry_folder() / name\n    metadata_path = dest_path / \"metadata.json\"\n    with open(metadata_path, \"r\") as file:\n        metadata = json.load(file)\n\n    be = BenchmarkExecutor(\n        DatasetInfo(name, subset, dataset, metadata), solver_strategy_obj, benchmark_id=benchmark_id\n    )\n\n    cpu_count = os.cpu_count()\n    max_concurrent = (cpu_count if cpu_count is not None else 1) if max_concurrent &lt; 0 else max_concurrent\n    be.run(max_concurrent=max_concurrent, record=record)\n</code></pre>"},{"location":"api/#nearai.cli.CLI","title":"CLI","text":"Source code in <code>nearai/cli.py</code> <pre><code>class CLI:\n    def __init__(self) -&gt; None:  # noqa: D107\n        self.registry = RegistryCli()\n        self.login = LoginCLI()\n        self.logout = LogoutCLI()\n        self.hub = HubCLI()\n        self.log = LogCLI()\n\n        self.config = ConfigCli()\n        self.benchmark = BenchmarkCli()\n        self.evaluation = EvaluationCli()\n        self.agent = AgentCli()\n        self.finetune = FinetuneCli()\n        self.tensorboard = TensorboardCli()\n        self.vllm = VllmCli()\n        self.permission = PermissionCli()\n\n    def submit(self, path: Optional[str] = None, worker_kind: str = WorkerKind.GPU_8_A100.value):\n        \"\"\"Submit a task to be executed by a worker.\"\"\"\n        if path is None:\n            path = os.getcwd()\n\n        worker_kind_t = WorkerKind(worker_kind)\n\n        location = self.registry.upload(path)\n\n        delegation_api = DelegationApi()\n        delegation_api.delegate_v1_delegation_delegate_post(\n            delegate_account_id=CONFIG.scheduler_account_id,\n            expires_at=datetime.now() + timedelta(days=1),\n        )\n\n        try:\n            client = JobsApi()\n            client.add_job_v1_jobs_add_job_post(\n                worker_kind_t,\n                BodyAddJobV1JobsAddJobPost(entry_location=location),\n            )\n        except Exception as e:\n            print(\"Error: \", e)\n            delegation_api.revoke_delegation_v1_delegation_revoke_delegation_post(\n                delegate_account_id=CONFIG.scheduler_account_id,\n            )\n\n    def location(self) -&gt; None:  # noqa: D102\n        \"\"\"Show location where nearai is installed.\"\"\"\n        from nearai import cli_path\n\n        print(cli_path())\n\n    def version(self):\n        \"\"\"Show nearai version.\"\"\"\n        print(importlib.metadata.version(\"nearai\"))\n\n    def task(self, *args, **kwargs):\n        \"\"\"CLI command for running a single task.\"\"\"\n        self.agent.task_cli(*args, **kwargs)\n</code></pre>"},{"location":"api/#nearai.cli.CLI.location","title":"location","text":"<pre><code>location() -&gt; None\n</code></pre> <p>Show location where nearai is installed.</p> Source code in <code>nearai/cli.py</code> <pre><code>def location(self) -&gt; None:  # noqa: D102\n    \"\"\"Show location where nearai is installed.\"\"\"\n    from nearai import cli_path\n\n    print(cli_path())\n</code></pre>"},{"location":"api/#nearai.cli.CLI.submit","title":"submit","text":"<pre><code>submit(path: Optional[str] = None, worker_kind: str = GPU_8_A100.value)\n</code></pre> <p>Submit a task to be executed by a worker.</p> Source code in <code>nearai/cli.py</code> <pre><code>def submit(self, path: Optional[str] = None, worker_kind: str = WorkerKind.GPU_8_A100.value):\n    \"\"\"Submit a task to be executed by a worker.\"\"\"\n    if path is None:\n        path = os.getcwd()\n\n    worker_kind_t = WorkerKind(worker_kind)\n\n    location = self.registry.upload(path)\n\n    delegation_api = DelegationApi()\n    delegation_api.delegate_v1_delegation_delegate_post(\n        delegate_account_id=CONFIG.scheduler_account_id,\n        expires_at=datetime.now() + timedelta(days=1),\n    )\n\n    try:\n        client = JobsApi()\n        client.add_job_v1_jobs_add_job_post(\n            worker_kind_t,\n            BodyAddJobV1JobsAddJobPost(entry_location=location),\n        )\n    except Exception as e:\n        print(\"Error: \", e)\n        delegation_api.revoke_delegation_v1_delegation_revoke_delegation_post(\n            delegate_account_id=CONFIG.scheduler_account_id,\n        )\n</code></pre>"},{"location":"api/#nearai.cli.CLI.task","title":"task","text":"<pre><code>task(*args, **kwargs)\n</code></pre> <p>CLI command for running a single task.</p> Source code in <code>nearai/cli.py</code> <pre><code>def task(self, *args, **kwargs):\n    \"\"\"CLI command for running a single task.\"\"\"\n    self.agent.task_cli(*args, **kwargs)\n</code></pre>"},{"location":"api/#nearai.cli.CLI.version","title":"version","text":"<pre><code>version()\n</code></pre> <p>Show nearai version.</p> Source code in <code>nearai/cli.py</code> <pre><code>def version(self):\n    \"\"\"Show nearai version.\"\"\"\n    print(importlib.metadata.version(\"nearai\"))\n</code></pre>"},{"location":"api/#nearai.cli.ConfigCli","title":"ConfigCli","text":"Source code in <code>nearai/cli.py</code> <pre><code>class ConfigCli:\n    def set(self, key: str, value: str, local: bool = False) -&gt; None:\n        \"\"\"Add key-value pair to the config file.\"\"\"\n        update_config(key, value, local)\n\n    def get(self, key: str) -&gt; None:\n        \"\"\"Get value of a key in the config file.\"\"\"\n        print(CONFIG.get(key))\n\n    def show(self) -&gt; None:  # noqa: D102\n        for key, value in asdict(CONFIG).items():\n            print(f\"{key}: {value}\")\n</code></pre>"},{"location":"api/#nearai.cli.ConfigCli.get","title":"get","text":"<pre><code>get(key: str) -&gt; None\n</code></pre> <p>Get value of a key in the config file.</p> Source code in <code>nearai/cli.py</code> <pre><code>def get(self, key: str) -&gt; None:\n    \"\"\"Get value of a key in the config file.\"\"\"\n    print(CONFIG.get(key))\n</code></pre>"},{"location":"api/#nearai.cli.ConfigCli.set","title":"set","text":"<pre><code>set(key: str, value: str, local: bool = False) -&gt; None\n</code></pre> <p>Add key-value pair to the config file.</p> Source code in <code>nearai/cli.py</code> <pre><code>def set(self, key: str, value: str, local: bool = False) -&gt; None:\n    \"\"\"Add key-value pair to the config file.\"\"\"\n    update_config(key, value, local)\n</code></pre>"},{"location":"api/#nearai.cli.EvaluationCli","title":"EvaluationCli","text":"Source code in <code>nearai/cli.py</code> <pre><code>class EvaluationCli:\n    def table(\n        self,\n        all_key_columns: bool = False,\n        all_metrics: bool = False,\n        num_columns: int = 6,\n        metric_name_max_length: int = 30,\n    ) -&gt; None:\n        \"\"\"Prints table of evaluations.\"\"\"\n        from nearai.evaluation import print_evaluation_table\n\n        api = EvaluationApi()\n        table = api.table_v1_evaluation_table_get()\n\n        print_evaluation_table(\n            table.rows,\n            table.columns,\n            table.important_columns,\n            all_key_columns,\n            all_metrics,\n            num_columns,\n            metric_name_max_length,\n        )\n\n    def read_solutions(self, entry: str, status: Optional[bool] = None, verbose: bool = False) -&gt; None:\n        \"\"\"Reads solutions.json from evaluation entry.\"\"\"\n        entry_path = registry.download(entry)\n        solutions_file = entry_path / \"solutions.json\"\n\n        if not solutions_file.exists():\n            print(f\"No solutions file found for entry: {entry}\")\n            return\n\n        try:\n            with open(solutions_file) as f:\n                solutions = json.load(f)\n        except json.JSONDecodeError:\n            print(f\"Error reading solutions file for entry: {entry}\")\n            return\n\n        # Filter solutions if status is specified\n        if status is not None:\n            solutions = [s for s in solutions if s.get(\"status\") == status]\n        if not solutions:\n            print(\"No solutions found matching criteria\")\n            return\n        print(f\"\\nFound {len(solutions)} solutions{' with status=' + str(status) if status is not None else ''}\")\n\n        for i, solution in enumerate(solutions, 1):\n            print(\"-\" * 80)\n            print(f\"\\nSolution {i}/{len(solutions)}:\")\n            datum = solution.get(\"datum\")\n            print(f\"datum: {json.dumps(datum, indent=2, ensure_ascii=False)}\")\n            status = solution.get(\"status\")\n            print(f\"status: {status}\")\n            info: dict = solution.get(\"info\", {})\n            if not verbose:\n                info.pop(\"verbose\")\n            print(f\"info: {json.dumps(info, indent=2, ensure_ascii=False)}\")\n            if i == 1:\n                print(\"Enter to continue, type 'exit' to quit.\")\n            new_message = input(\"&gt; \")\n            if new_message.lower() == \"exit\":\n                break\n</code></pre>"},{"location":"api/#nearai.cli.EvaluationCli.read_solutions","title":"read_solutions","text":"<pre><code>read_solutions(entry: str, status: Optional[bool] = None, verbose: bool = False) -&gt; None\n</code></pre> <p>Reads solutions.json from evaluation entry.</p> Source code in <code>nearai/cli.py</code> <pre><code>def read_solutions(self, entry: str, status: Optional[bool] = None, verbose: bool = False) -&gt; None:\n    \"\"\"Reads solutions.json from evaluation entry.\"\"\"\n    entry_path = registry.download(entry)\n    solutions_file = entry_path / \"solutions.json\"\n\n    if not solutions_file.exists():\n        print(f\"No solutions file found for entry: {entry}\")\n        return\n\n    try:\n        with open(solutions_file) as f:\n            solutions = json.load(f)\n    except json.JSONDecodeError:\n        print(f\"Error reading solutions file for entry: {entry}\")\n        return\n\n    # Filter solutions if status is specified\n    if status is not None:\n        solutions = [s for s in solutions if s.get(\"status\") == status]\n    if not solutions:\n        print(\"No solutions found matching criteria\")\n        return\n    print(f\"\\nFound {len(solutions)} solutions{' with status=' + str(status) if status is not None else ''}\")\n\n    for i, solution in enumerate(solutions, 1):\n        print(\"-\" * 80)\n        print(f\"\\nSolution {i}/{len(solutions)}:\")\n        datum = solution.get(\"datum\")\n        print(f\"datum: {json.dumps(datum, indent=2, ensure_ascii=False)}\")\n        status = solution.get(\"status\")\n        print(f\"status: {status}\")\n        info: dict = solution.get(\"info\", {})\n        if not verbose:\n            info.pop(\"verbose\")\n        print(f\"info: {json.dumps(info, indent=2, ensure_ascii=False)}\")\n        if i == 1:\n            print(\"Enter to continue, type 'exit' to quit.\")\n        new_message = input(\"&gt; \")\n        if new_message.lower() == \"exit\":\n            break\n</code></pre>"},{"location":"api/#nearai.cli.EvaluationCli.table","title":"table","text":"<pre><code>table(all_key_columns: bool = False, all_metrics: bool = False, num_columns: int = 6, metric_name_max_length: int = 30) -&gt; None\n</code></pre> <p>Prints table of evaluations.</p> Source code in <code>nearai/cli.py</code> <pre><code>def table(\n    self,\n    all_key_columns: bool = False,\n    all_metrics: bool = False,\n    num_columns: int = 6,\n    metric_name_max_length: int = 30,\n) -&gt; None:\n    \"\"\"Prints table of evaluations.\"\"\"\n    from nearai.evaluation import print_evaluation_table\n\n    api = EvaluationApi()\n    table = api.table_v1_evaluation_table_get()\n\n    print_evaluation_table(\n        table.rows,\n        table.columns,\n        table.important_columns,\n        all_key_columns,\n        all_metrics,\n        num_columns,\n        metric_name_max_length,\n    )\n</code></pre>"},{"location":"api/#nearai.cli.HubCLI","title":"HubCLI","text":"Source code in <code>nearai/cli.py</code> <pre><code>class HubCLI:\n    def chat(self, **kwargs):\n        \"\"\"Chat with model from NEAR AI hub.\n\n        Args:\n        ----\n            query (str): User's query to model\n            endpoint (str): NEAR AI HUB's url\n            model (str): Name of a model\n            provider (str): Name of a provider\n            info (bool): Display system info\n            kwargs (Dict[str, Any]): All cli keyword arguments\n\n        \"\"\"\n        from nearai.hub import Hub\n\n        hub = Hub(CONFIG)\n        hub.chat(kwargs)\n</code></pre>"},{"location":"api/#nearai.cli.HubCLI.chat","title":"chat","text":"<pre><code>chat(**kwargs)\n</code></pre> <p>Chat with model from NEAR AI hub.</p> <pre><code>query (str): User's query to model\nendpoint (str): NEAR AI HUB's url\nmodel (str): Name of a model\nprovider (str): Name of a provider\ninfo (bool): Display system info\nkwargs (Dict[str, Any]): All cli keyword arguments\n</code></pre> Source code in <code>nearai/cli.py</code> <pre><code>def chat(self, **kwargs):\n    \"\"\"Chat with model from NEAR AI hub.\n\n    Args:\n    ----\n        query (str): User's query to model\n        endpoint (str): NEAR AI HUB's url\n        model (str): Name of a model\n        provider (str): Name of a provider\n        info (bool): Display system info\n        kwargs (Dict[str, Any]): All cli keyword arguments\n\n    \"\"\"\n    from nearai.hub import Hub\n\n    hub = Hub(CONFIG)\n    hub.chat(kwargs)\n</code></pre>"},{"location":"api/#nearai.cli.LoginCLI","title":"LoginCLI","text":"Source code in <code>nearai/cli.py</code> <pre><code>class LoginCLI:\n    def __call__(self, **kwargs):\n        \"\"\"Login with NEAR Mainnet account.\n\n        Args:\n        ----\n            remote (bool): Remote login allows signing message with NEAR Account on a remote machine\n            auth_url (str): Url to the auth portal\n            accountId (str): AccountId in .near-credentials folder to signMessage\n            privateKey (str): Private Key to sign a message\n            kwargs (Dict[str, Any]): All cli keyword arguments\n\n        \"\"\"\n        from nearai.login import generate_and_save_signature, login_with_file_credentials, login_with_near_auth\n\n        remote = kwargs.get(\"remote\", False)\n        account_id = kwargs.get(\"accountId\", None)\n        private_key = kwargs.get(\"privateKey\", None)\n\n        if not remote and account_id and private_key:\n            generate_and_save_signature(account_id, private_key)\n        elif not remote and account_id:\n            login_with_file_credentials(account_id)\n        else:\n            auth_url = kwargs.get(\"auth_url\", \"https://auth.near.ai\")\n            login_with_near_auth(remote, auth_url)\n\n    def status(self):\n        \"\"\"Load NEAR account authorization data.\"\"\"\n        from nearai.login import print_login_status\n\n        print_login_status()\n\n    def save(self, **kwargs):\n        \"\"\"Save NEAR account authorization data.\n\n        Args:\n        ----\n            accountId (str): Near Account\n            signature (str): Signature\n            publicKey (str): Public Key used to sign\n            callbackUrl (str): Callback Url\n            nonce (str): nonce\n            kwargs (Dict[str, Any]): All cli keyword arguments\n\n        \"\"\"\n        from nearai.login import update_auth_config\n\n        account_id = kwargs.get(\"accountId\")\n        signature = kwargs.get(\"signature\")\n        public_key = kwargs.get(\"publicKey\")\n        callback_url = kwargs.get(\"callbackUrl\")\n        nonce = kwargs.get(\"nonce\")\n\n        if account_id and signature and public_key and callback_url and nonce:\n            update_auth_config(account_id, signature, public_key, callback_url, nonce)\n        else:\n            print(\"Missing data\")\n</code></pre>"},{"location":"api/#nearai.cli.LoginCLI.__call__","title":"__call__","text":"<pre><code>__call__(**kwargs)\n</code></pre> <p>Login with NEAR Mainnet account.</p> <pre><code>remote (bool): Remote login allows signing message with NEAR Account on a remote machine\nauth_url (str): Url to the auth portal\naccountId (str): AccountId in .near-credentials folder to signMessage\nprivateKey (str): Private Key to sign a message\nkwargs (Dict[str, Any]): All cli keyword arguments\n</code></pre> Source code in <code>nearai/cli.py</code> <pre><code>def __call__(self, **kwargs):\n    \"\"\"Login with NEAR Mainnet account.\n\n    Args:\n    ----\n        remote (bool): Remote login allows signing message with NEAR Account on a remote machine\n        auth_url (str): Url to the auth portal\n        accountId (str): AccountId in .near-credentials folder to signMessage\n        privateKey (str): Private Key to sign a message\n        kwargs (Dict[str, Any]): All cli keyword arguments\n\n    \"\"\"\n    from nearai.login import generate_and_save_signature, login_with_file_credentials, login_with_near_auth\n\n    remote = kwargs.get(\"remote\", False)\n    account_id = kwargs.get(\"accountId\", None)\n    private_key = kwargs.get(\"privateKey\", None)\n\n    if not remote and account_id and private_key:\n        generate_and_save_signature(account_id, private_key)\n    elif not remote and account_id:\n        login_with_file_credentials(account_id)\n    else:\n        auth_url = kwargs.get(\"auth_url\", \"https://auth.near.ai\")\n        login_with_near_auth(remote, auth_url)\n</code></pre>"},{"location":"api/#nearai.cli.LoginCLI.save","title":"save","text":"<pre><code>save(**kwargs)\n</code></pre> <p>Save NEAR account authorization data.</p> <pre><code>accountId (str): Near Account\nsignature (str): Signature\npublicKey (str): Public Key used to sign\ncallbackUrl (str): Callback Url\nnonce (str): nonce\nkwargs (Dict[str, Any]): All cli keyword arguments\n</code></pre> Source code in <code>nearai/cli.py</code> <pre><code>def save(self, **kwargs):\n    \"\"\"Save NEAR account authorization data.\n\n    Args:\n    ----\n        accountId (str): Near Account\n        signature (str): Signature\n        publicKey (str): Public Key used to sign\n        callbackUrl (str): Callback Url\n        nonce (str): nonce\n        kwargs (Dict[str, Any]): All cli keyword arguments\n\n    \"\"\"\n    from nearai.login import update_auth_config\n\n    account_id = kwargs.get(\"accountId\")\n    signature = kwargs.get(\"signature\")\n    public_key = kwargs.get(\"publicKey\")\n    callback_url = kwargs.get(\"callbackUrl\")\n    nonce = kwargs.get(\"nonce\")\n\n    if account_id and signature and public_key and callback_url and nonce:\n        update_auth_config(account_id, signature, public_key, callback_url, nonce)\n    else:\n        print(\"Missing data\")\n</code></pre>"},{"location":"api/#nearai.cli.LoginCLI.status","title":"status","text":"<pre><code>status()\n</code></pre> <p>Load NEAR account authorization data.</p> Source code in <code>nearai/cli.py</code> <pre><code>def status(self):\n    \"\"\"Load NEAR account authorization data.\"\"\"\n    from nearai.login import print_login_status\n\n    print_login_status()\n</code></pre>"},{"location":"api/#nearai.cli.LogoutCLI","title":"LogoutCLI","text":"Source code in <code>nearai/cli.py</code> <pre><code>class LogoutCLI:\n    def __call__(self, **kwargs):\n        \"\"\"Clear NEAR account auth data.\"\"\"\n        from nearai.config import load_config_file, save_config_file\n\n        config = load_config_file()\n        if not config.get(\"auth\") or not config[\"auth\"].get(\"account_id\"):\n            print(\"Auth data does not exist.\")\n        else:\n            config.pop(\"auth\", None)\n            save_config_file(config)\n            print(\"Auth data removed\")\n</code></pre>"},{"location":"api/#nearai.cli.LogoutCLI.__call__","title":"__call__","text":"<pre><code>__call__(**kwargs)\n</code></pre> <p>Clear NEAR account auth data.</p> Source code in <code>nearai/cli.py</code> <pre><code>def __call__(self, **kwargs):\n    \"\"\"Clear NEAR account auth data.\"\"\"\n    from nearai.config import load_config_file, save_config_file\n\n    config = load_config_file()\n    if not config.get(\"auth\") or not config[\"auth\"].get(\"account_id\"):\n        print(\"Auth data does not exist.\")\n    else:\n        config.pop(\"auth\", None)\n        save_config_file(config)\n        print(\"Auth data removed\")\n</code></pre>"},{"location":"api/#nearai.cli.PermissionCli","title":"PermissionCli","text":"Source code in <code>nearai/cli.py</code> <pre><code>class PermissionCli:\n    def __init__(self) -&gt; None:  # noqa: D107\n        self.client = PermissionsApi()\n\n    def grant(self, account_id: str, permission: str):\n        \"\"\"Grant permission to an account.\"\"\"\n        self.client.grant_permission_v1_permissions_grant_permission_post(account_id, permission)\n\n    def revoke(self, account_id: str, permission: str = \"\"):\n        \"\"\"Revoke permission from an account. If permission is empty all permissions are revoked.\"\"\"\n        self.client.revoke_permission_v1_permissions_revoke_permission_post(account_id, permission)\n</code></pre>"},{"location":"api/#nearai.cli.PermissionCli.grant","title":"grant","text":"<pre><code>grant(account_id: str, permission: str)\n</code></pre> <p>Grant permission to an account.</p> Source code in <code>nearai/cli.py</code> <pre><code>def grant(self, account_id: str, permission: str):\n    \"\"\"Grant permission to an account.\"\"\"\n    self.client.grant_permission_v1_permissions_grant_permission_post(account_id, permission)\n</code></pre>"},{"location":"api/#nearai.cli.PermissionCli.revoke","title":"revoke","text":"<pre><code>revoke(account_id: str, permission: str = '')\n</code></pre> <p>Revoke permission from an account. If permission is empty all permissions are revoked.</p> Source code in <code>nearai/cli.py</code> <pre><code>def revoke(self, account_id: str, permission: str = \"\"):\n    \"\"\"Revoke permission from an account. If permission is empty all permissions are revoked.\"\"\"\n    self.client.revoke_permission_v1_permissions_revoke_permission_post(account_id, permission)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli","title":"RegistryCli","text":"Source code in <code>nearai/cli.py</code> <pre><code>class RegistryCli:\n    def info(self, entry: str) -&gt; None:\n        \"\"\"Show information about an item.\"\"\"\n        entry_location = parse_location(entry)\n        metadata = registry.info(entry_location)\n\n        if metadata is None:\n            print(f\"Entry {entry} not found.\")\n            return\n\n        print(metadata.model_dump_json(indent=2))\n        if metadata.category == \"model\":\n            available_provider_matches = ProviderModels(CONFIG.get_client_config()).available_provider_matches(\n                NamespacedName(name=metadata.name, namespace=entry_location.namespace)\n            )\n            if len(available_provider_matches) &gt; 0:\n                header = [\"provider\", \"name\"]\n\n                table = []\n                for provider, name in available_provider_matches.items():\n                    table.append(\n                        [\n                            fill(provider),\n                            fill(name),\n                        ]\n                    )\n                print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n\n    def metadata_template(self, local_path: str = \".\", category: str = \"\", description: str = \"\"):\n        \"\"\"Create a metadata template.\"\"\"\n        path = Path(local_path)\n\n        metadata_path = path / \"metadata.json\"\n\n        version = path.name\n        pattern = r\"^(0|[1-9]\\d*)\\.(0|[1-9]\\d*)\\.(0|[1-9]\\d*)(?:-((?:0|[1-9]\\d*|\\d*[a-zA-Z-][0-9a-zA-Z-]*)(?:\\.(?:0|[1-9]\\d*|\\d*[a-zA-Z-][0-9a-zA-Z-]*))*))?(?:\\+([0-9a-zA-Z-]+(?:\\.[0-9a-zA-Z-]+)*))?$\"  # noqa: E501\n        assert re.match(pattern, version), f\"Invalid semantic version format: {version}\"\n        name = path.parent.name\n        assert not re.match(pattern, name), f\"Invalid agent name: {name}\"\n\n        with open(metadata_path, \"w\") as f:\n            metadata: Dict[str, Any] = {\n                \"name\": name,\n                \"version\": version,\n                \"description\": description,\n                \"category\": category,\n                \"tags\": [],\n                \"details\": {},\n                \"show_entry\": True,\n            }\n\n            if category == \"agent\":\n                metadata[\"details\"][\"agent\"] = {}\n                metadata[\"details\"][\"agent\"][\"welcome\"] = {\n                    \"title\": name,\n                    \"description\": description,\n                }\n                metadata[\"details\"][\"agent\"][\"defaults\"] = {\n                    \"model\": DEFAULT_MODEL,\n                    \"model_provider\": DEFAULT_PROVIDER,\n                    \"model_temperature\": DEFAULT_MODEL_TEMPERATURE,\n                    \"model_max_tokens\": DEFAULT_MODEL_MAX_TOKENS,\n                    \"max_iterations\": 1,\n                }\n                metadata[\"details\"][\"agent\"][\"framework\"] = \"base\"\n\n            json.dump(metadata, f, indent=2)\n\n    def list(\n        self,\n        namespace: str = \"\",\n        category: str = \"\",\n        tags: str = \"\",\n        total: int = 32,\n        offset: int = 0,\n        show_all: bool = False,\n        show_latest_version: bool = True,\n        star: str = \"\",\n    ) -&gt; None:\n        \"\"\"List available items.\"\"\"\n        # Make sure tags is a comma-separated list of tags\n        tags_l = parse_tags(tags)\n        tags = \",\".join(tags_l)\n\n        entries = registry.list(\n            namespace=namespace,\n            category=category,\n            tags=tags,\n            total=total + 1,\n            offset=offset,\n            show_all=show_all,\n            show_latest_version=show_latest_version,\n            starred_by=star,\n        )\n\n        more_rows = len(entries) &gt; total\n        entries = entries[:total]\n\n        header = [\"entry\", \"category\", \"description\", \"tags\"]\n\n        table = []\n        for entry in entries:\n            table.append(\n                [\n                    fill(f\"{entry.namespace}/{entry.name}/{entry.version}\"),\n                    fill(entry.category, 20),\n                    fill(entry.description, 50),\n                    fill(\", \".join(entry.tags), 20),\n                ]\n            )\n\n        if more_rows:\n            table.append([\"...\", \"...\", \"...\", \"...\"])\n\n        print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n\n        if category == \"model\" and len(entries) &lt; total and namespace == \"\" and tags == \"\" and star == \"\":\n            unregistered_common_provider_models = ProviderModels(\n                CONFIG.get_client_config()\n            ).get_unregistered_common_provider_models(registry.dict_models())\n            if len(unregistered_common_provider_models):\n                print(\n                    f\"There are unregistered common provider models: {unregistered_common_provider_models}. Run 'nearai registry upload-unregistered-common-provider-models' to update registry.\"  # noqa: E501\n                )\n\n    def update(self, local_path: str = \".\") -&gt; None:\n        \"\"\"Update metadata of a registry item.\"\"\"\n        path = Path(local_path)\n\n        if CONFIG.auth is None:\n            print(\"Please login with `nearai login`\")\n            exit(1)\n\n        metadata_path = path / \"metadata.json\"\n        check_metadata(metadata_path)\n\n        with open(metadata_path) as f:\n            metadata: Dict[str, Any] = json.load(f)\n\n        namespace = CONFIG.auth.namespace\n\n        entry_location = EntryLocation.model_validate(\n            dict(\n                namespace=namespace,\n                name=metadata.pop(\"name\"),\n                version=metadata.pop(\"version\"),\n            )\n        )\n\n        entry_metadata = EntryMetadataInput.model_validate(metadata)\n        result = registry.update(entry_location, entry_metadata)\n        print(json.dumps(result, indent=2))\n\n    def upload_unregistered_common_provider_models(self, dry_run: bool = True) -&gt; None:\n        \"\"\"Creates new registry items for unregistered common provider models.\"\"\"\n        provider_matches_list = ProviderModels(CONFIG.get_client_config()).get_unregistered_common_provider_models(\n            registry.dict_models()\n        )\n        if len(provider_matches_list) == 0:\n            print(\"No new models to upload.\")\n            return\n\n        print(\"Going to create new registry items:\")\n        header = [\"entry\", \"description\"]\n        table = []\n        paths = []\n        for provider_matches in provider_matches_list:\n            provider_model = provider_matches.get(DEFAULT_PROVIDER) or next(iter(provider_matches.values()))\n            _, model = get_provider_namespaced_model(provider_model)\n            assert model.namespace == \"\"\n            model.name = create_registry_name(model.name)\n            model.namespace = DEFAULT_NAMESPACE\n            version = \"1.0.0\"\n            description = \" &amp; \".join(provider_matches.values())\n            table.append(\n                [\n                    fill(f\"{model.namespace}/{model.name}/{version}\"),\n                    fill(description, 50),\n                ]\n            )\n\n            path = get_registry_folder() / model.namespace / model.name / version\n            path.mkdir(parents=True, exist_ok=True)\n            paths.append(path)\n            metadata_path = path / \"metadata.json\"\n            with open(metadata_path, \"w\") as f:\n                metadata: Dict[str, Any] = {\n                    \"name\": model.name,\n                    \"version\": version,\n                    \"description\": description,\n                    \"category\": \"model\",\n                    \"tags\": [],\n                    \"details\": {},\n                    \"show_entry\": True,\n                }\n                json.dump(metadata, f, indent=2)\n\n        print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n        if dry_run:\n            print(\"Please verify, then repeat the command with --dry_run=False\")\n        else:\n            for path in paths:\n                self.upload(str(path))\n\n    def upload(self, local_path: str = \".\") -&gt; EntryLocation:\n        \"\"\"Upload item to the registry.\"\"\"\n        return registry.upload(Path(local_path), show_progress=True)\n\n    def download(self, entry_location: str, force: bool = False) -&gt; None:\n        \"\"\"Download item.\"\"\"\n        registry.download(entry_location, force=force, show_progress=True)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.download","title":"download","text":"<pre><code>download(entry_location: str, force: bool = False) -&gt; None\n</code></pre> <p>Download item.</p> Source code in <code>nearai/cli.py</code> <pre><code>def download(self, entry_location: str, force: bool = False) -&gt; None:\n    \"\"\"Download item.\"\"\"\n    registry.download(entry_location, force=force, show_progress=True)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.info","title":"info","text":"<pre><code>info(entry: str) -&gt; None\n</code></pre> <p>Show information about an item.</p> Source code in <code>nearai/cli.py</code> <pre><code>def info(self, entry: str) -&gt; None:\n    \"\"\"Show information about an item.\"\"\"\n    entry_location = parse_location(entry)\n    metadata = registry.info(entry_location)\n\n    if metadata is None:\n        print(f\"Entry {entry} not found.\")\n        return\n\n    print(metadata.model_dump_json(indent=2))\n    if metadata.category == \"model\":\n        available_provider_matches = ProviderModels(CONFIG.get_client_config()).available_provider_matches(\n            NamespacedName(name=metadata.name, namespace=entry_location.namespace)\n        )\n        if len(available_provider_matches) &gt; 0:\n            header = [\"provider\", \"name\"]\n\n            table = []\n            for provider, name in available_provider_matches.items():\n                table.append(\n                    [\n                        fill(provider),\n                        fill(name),\n                    ]\n                )\n            print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.list","title":"list","text":"<pre><code>list(namespace: str = '', category: str = '', tags: str = '', total: int = 32, offset: int = 0, show_all: bool = False, show_latest_version: bool = True, star: str = '') -&gt; None\n</code></pre> <p>List available items.</p> Source code in <code>nearai/cli.py</code> <pre><code>def list(\n    self,\n    namespace: str = \"\",\n    category: str = \"\",\n    tags: str = \"\",\n    total: int = 32,\n    offset: int = 0,\n    show_all: bool = False,\n    show_latest_version: bool = True,\n    star: str = \"\",\n) -&gt; None:\n    \"\"\"List available items.\"\"\"\n    # Make sure tags is a comma-separated list of tags\n    tags_l = parse_tags(tags)\n    tags = \",\".join(tags_l)\n\n    entries = registry.list(\n        namespace=namespace,\n        category=category,\n        tags=tags,\n        total=total + 1,\n        offset=offset,\n        show_all=show_all,\n        show_latest_version=show_latest_version,\n        starred_by=star,\n    )\n\n    more_rows = len(entries) &gt; total\n    entries = entries[:total]\n\n    header = [\"entry\", \"category\", \"description\", \"tags\"]\n\n    table = []\n    for entry in entries:\n        table.append(\n            [\n                fill(f\"{entry.namespace}/{entry.name}/{entry.version}\"),\n                fill(entry.category, 20),\n                fill(entry.description, 50),\n                fill(\", \".join(entry.tags), 20),\n            ]\n        )\n\n    if more_rows:\n        table.append([\"...\", \"...\", \"...\", \"...\"])\n\n    print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n\n    if category == \"model\" and len(entries) &lt; total and namespace == \"\" and tags == \"\" and star == \"\":\n        unregistered_common_provider_models = ProviderModels(\n            CONFIG.get_client_config()\n        ).get_unregistered_common_provider_models(registry.dict_models())\n        if len(unregistered_common_provider_models):\n            print(\n                f\"There are unregistered common provider models: {unregistered_common_provider_models}. Run 'nearai registry upload-unregistered-common-provider-models' to update registry.\"  # noqa: E501\n            )\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.metadata_template","title":"metadata_template","text":"<pre><code>metadata_template(local_path: str = '.', category: str = '', description: str = '')\n</code></pre> <p>Create a metadata template.</p> Source code in <code>nearai/cli.py</code> <pre><code>def metadata_template(self, local_path: str = \".\", category: str = \"\", description: str = \"\"):\n    \"\"\"Create a metadata template.\"\"\"\n    path = Path(local_path)\n\n    metadata_path = path / \"metadata.json\"\n\n    version = path.name\n    pattern = r\"^(0|[1-9]\\d*)\\.(0|[1-9]\\d*)\\.(0|[1-9]\\d*)(?:-((?:0|[1-9]\\d*|\\d*[a-zA-Z-][0-9a-zA-Z-]*)(?:\\.(?:0|[1-9]\\d*|\\d*[a-zA-Z-][0-9a-zA-Z-]*))*))?(?:\\+([0-9a-zA-Z-]+(?:\\.[0-9a-zA-Z-]+)*))?$\"  # noqa: E501\n    assert re.match(pattern, version), f\"Invalid semantic version format: {version}\"\n    name = path.parent.name\n    assert not re.match(pattern, name), f\"Invalid agent name: {name}\"\n\n    with open(metadata_path, \"w\") as f:\n        metadata: Dict[str, Any] = {\n            \"name\": name,\n            \"version\": version,\n            \"description\": description,\n            \"category\": category,\n            \"tags\": [],\n            \"details\": {},\n            \"show_entry\": True,\n        }\n\n        if category == \"agent\":\n            metadata[\"details\"][\"agent\"] = {}\n            metadata[\"details\"][\"agent\"][\"welcome\"] = {\n                \"title\": name,\n                \"description\": description,\n            }\n            metadata[\"details\"][\"agent\"][\"defaults\"] = {\n                \"model\": DEFAULT_MODEL,\n                \"model_provider\": DEFAULT_PROVIDER,\n                \"model_temperature\": DEFAULT_MODEL_TEMPERATURE,\n                \"model_max_tokens\": DEFAULT_MODEL_MAX_TOKENS,\n                \"max_iterations\": 1,\n            }\n            metadata[\"details\"][\"agent\"][\"framework\"] = \"base\"\n\n        json.dump(metadata, f, indent=2)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.update","title":"update","text":"<pre><code>update(local_path: str = '.') -&gt; None\n</code></pre> <p>Update metadata of a registry item.</p> Source code in <code>nearai/cli.py</code> <pre><code>def update(self, local_path: str = \".\") -&gt; None:\n    \"\"\"Update metadata of a registry item.\"\"\"\n    path = Path(local_path)\n\n    if CONFIG.auth is None:\n        print(\"Please login with `nearai login`\")\n        exit(1)\n\n    metadata_path = path / \"metadata.json\"\n    check_metadata(metadata_path)\n\n    with open(metadata_path) as f:\n        metadata: Dict[str, Any] = json.load(f)\n\n    namespace = CONFIG.auth.namespace\n\n    entry_location = EntryLocation.model_validate(\n        dict(\n            namespace=namespace,\n            name=metadata.pop(\"name\"),\n            version=metadata.pop(\"version\"),\n        )\n    )\n\n    entry_metadata = EntryMetadataInput.model_validate(metadata)\n    result = registry.update(entry_location, entry_metadata)\n    print(json.dumps(result, indent=2))\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.upload","title":"upload","text":"<pre><code>upload(local_path: str = '.') -&gt; EntryLocation\n</code></pre> <p>Upload item to the registry.</p> Source code in <code>nearai/cli.py</code> <pre><code>def upload(self, local_path: str = \".\") -&gt; EntryLocation:\n    \"\"\"Upload item to the registry.\"\"\"\n    return registry.upload(Path(local_path), show_progress=True)\n</code></pre>"},{"location":"api/#nearai.cli.RegistryCli.upload_unregistered_common_provider_models","title":"upload_unregistered_common_provider_models","text":"<pre><code>upload_unregistered_common_provider_models(dry_run: bool = True) -&gt; None\n</code></pre> <p>Creates new registry items for unregistered common provider models.</p> Source code in <code>nearai/cli.py</code> <pre><code>def upload_unregistered_common_provider_models(self, dry_run: bool = True) -&gt; None:\n    \"\"\"Creates new registry items for unregistered common provider models.\"\"\"\n    provider_matches_list = ProviderModels(CONFIG.get_client_config()).get_unregistered_common_provider_models(\n        registry.dict_models()\n    )\n    if len(provider_matches_list) == 0:\n        print(\"No new models to upload.\")\n        return\n\n    print(\"Going to create new registry items:\")\n    header = [\"entry\", \"description\"]\n    table = []\n    paths = []\n    for provider_matches in provider_matches_list:\n        provider_model = provider_matches.get(DEFAULT_PROVIDER) or next(iter(provider_matches.values()))\n        _, model = get_provider_namespaced_model(provider_model)\n        assert model.namespace == \"\"\n        model.name = create_registry_name(model.name)\n        model.namespace = DEFAULT_NAMESPACE\n        version = \"1.0.0\"\n        description = \" &amp; \".join(provider_matches.values())\n        table.append(\n            [\n                fill(f\"{model.namespace}/{model.name}/{version}\"),\n                fill(description, 50),\n            ]\n        )\n\n        path = get_registry_folder() / model.namespace / model.name / version\n        path.mkdir(parents=True, exist_ok=True)\n        paths.append(path)\n        metadata_path = path / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            metadata: Dict[str, Any] = {\n                \"name\": model.name,\n                \"version\": version,\n                \"description\": description,\n                \"category\": \"model\",\n                \"tags\": [],\n                \"details\": {},\n                \"show_entry\": True,\n            }\n            json.dump(metadata, f, indent=2)\n\n    print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n    if dry_run:\n        print(\"Please verify, then repeat the command with --dry_run=False\")\n    else:\n        for path in paths:\n            self.upload(str(path))\n</code></pre>"},{"location":"api/#nearai.cli.assert_user_auth","title":"assert_user_auth","text":"<pre><code>assert_user_auth() -&gt; None\n</code></pre> <p>Ensure the user is authenticated.</p> Source code in <code>nearai/cli.py</code> <pre><code>def assert_user_auth() -&gt; None:\n    \"\"\"Ensure the user is authenticated.\"\"\"\n    if CONFIG.auth is None:\n        print(\"Please login with `nearai login` first\")\n        exit(1)\n</code></pre>"},{"location":"api/#nearai.cli.check_update","title":"check_update","text":"<pre><code>check_update()\n</code></pre> <p>Check if there is a new version of nearai CLI available.</p> Source code in <code>nearai/cli.py</code> <pre><code>def check_update():\n    \"\"\"Check if there is a new version of nearai CLI available.\"\"\"\n    try:\n        api = DefaultApi()\n        latest = api.version_v1_version_get()\n        current = importlib.metadata.version(\"nearai\")\n\n        if latest != current:\n            print(f\"New version of nearai CLI available: {latest}. Current version: {current}\")\n            print(\"Run `pip install --upgrade nearai` to update.\")\n\n    except Exception as _:\n        pass\n</code></pre>"},{"location":"api/#nearai.cli_helpers","title":"cli_helpers","text":""},{"location":"api/#nearai.cli_helpers.display_agents_in_columns","title":"display_agents_in_columns","text":"<pre><code>display_agents_in_columns(agents: list[Path]) -&gt; None\n</code></pre> <p>Display agents in a rich table format.</p> <pre><code>agents: List of Path objects pointing to agent locations (pre-sorted)\n</code></pre> Source code in <code>nearai/cli_helpers.py</code> <pre><code>def display_agents_in_columns(agents: list[Path]) -&gt; None:\n    \"\"\"Display agents in a rich table format.\n\n    Args:\n    ----\n        agents: List of Path objects pointing to agent locations (pre-sorted)\n\n    \"\"\"\n    # Create table\n    table = Table(title=\"Available Agents\", show_header=True, header_style=\"bold\", show_lines=True, expand=True)\n\n    # Add columns\n    table.add_column(\"#\", style=\"bold\", width=4)\n    table.add_column(\"Namespace\", style=\"blue\")\n    table.add_column(\"Agent Name\", style=\"cyan\")\n    table.add_column(\"Version\", style=\"green\")\n    table.add_column(\"Description\", style=\"white\")\n    table.add_column(\"Tags\", style=\"yellow\")\n\n    # Add rows\n    for idx, agent_path in enumerate(agents, 1):\n        try:\n            # Read metadata for additional info\n            with open(agent_path / \"metadata.json\") as f:\n                metadata = json.load(f)\n                description = metadata.get(\"description\", \"No description\")\n                tags = metadata.get(\"tags\", [])\n        except (FileNotFoundError, json.JSONDecodeError):\n            description = \"Unable to load metadata\"\n            tags = []\n\n        # Add row to table with separated path components\n        table.add_row(\n            str(idx),\n            agent_path.parts[-3],  # namespace\n            agent_path.parts[-2],  # agent name\n            agent_path.parts[-1],  # version\n            description,\n            \", \".join(tags) if tags else \"\u2014\",\n        )\n\n    # Display table\n    console = Console()\n    console.print(\"\\n\")\n    console.print(table)\n    console.print(\"\\n\")\n</code></pre>"},{"location":"api/#nearai.config","title":"config","text":""},{"location":"api/#nearai.config.Config","title":"Config","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>nearai/config.py</code> <pre><code>class Config(BaseModel):\n    origin: Optional[str] = None\n    api_url: Optional[str] = \"https://api.near.ai\"\n    inference_url: str = \"http://localhost:5000/v1/\"\n    inference_api_key: str = \"n/a\"\n    scheduler_account_id: str = \"nearaischeduler.near\"\n    nearai_hub: NearAiHubConfig = NearAiHubConfig()\n    confirm_commands: bool = True\n    auth: Optional[AuthData] = None\n    num_inference_retries: int = 1\n\n    def update_with(self, extra_config: Dict[str, Any], map_key: Callable[[str], str] = lambda x: x) -&gt; \"Config\":\n        \"\"\"Update the config with the given dictionary.\"\"\"\n        dict_repr = self.model_dump()\n        keys = list(map(map_key, dict_repr.keys()))\n\n        for key in keys:\n            value = extra_config.get(key, None)\n\n            if value:\n                # This will skip empty values, even if they are set in the `extra_config`\n                dict_repr[key] = value\n\n        return Config.model_validate(dict_repr)\n\n    def get(self, key: str, default: Optional[Any] = None) -&gt; Optional[Any]:\n        \"\"\"Get the value of a key in the config if it exists.\"\"\"\n        return getattr(self, key, default)\n\n    def get_client_config(self) -&gt; ClientConfig:  # noqa: D102\n        return ClientConfig(\n            base_url=self.nearai_hub.base_url,\n            auth=self.auth,\n            custom_llm_provider=self.nearai_hub.custom_llm_provider,\n            default_provider=self.nearai_hub.default_provider,\n            num_inference_retries=self.num_inference_retries,\n        )\n</code></pre>"},{"location":"api/#nearai.config.Config.get","title":"get","text":"<pre><code>get(key: str, default: Optional[Any] = None) -&gt; Optional[Any]\n</code></pre> <p>Get the value of a key in the config if it exists.</p> Source code in <code>nearai/config.py</code> <pre><code>def get(self, key: str, default: Optional[Any] = None) -&gt; Optional[Any]:\n    \"\"\"Get the value of a key in the config if it exists.\"\"\"\n    return getattr(self, key, default)\n</code></pre>"},{"location":"api/#nearai.config.Config.update_with","title":"update_with","text":"<pre><code>update_with(extra_config: Dict[str, Any], map_key: Callable[[str], str] = lambda x: x) -&gt; Config\n</code></pre> <p>Update the config with the given dictionary.</p> Source code in <code>nearai/config.py</code> <pre><code>def update_with(self, extra_config: Dict[str, Any], map_key: Callable[[str], str] = lambda x: x) -&gt; \"Config\":\n    \"\"\"Update the config with the given dictionary.\"\"\"\n    dict_repr = self.model_dump()\n    keys = list(map(map_key, dict_repr.keys()))\n\n    for key in keys:\n        value = extra_config.get(key, None)\n\n        if value:\n            # This will skip empty values, even if they are set in the `extra_config`\n            dict_repr[key] = value\n\n    return Config.model_validate(dict_repr)\n</code></pre>"},{"location":"api/#nearai.config.NearAiHubConfig","title":"NearAiHubConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>NearAiHub Config.</p> <p>login_with_near (Optional[bool]): Indicates whether to attempt login using Near Auth.</p> <p>api_key (Optional[str]): The API key to use if Near Auth is not being utilized</p> <p>base_url (Optional[str]): NEAR AI Hub url</p> <p>default_provider (Optional[str]): Default provider name</p> <p>default_model (Optional[str]): Default model name</p> <p>custom_llm_provider (Optional[str]): provider to be used by litellm proxy</p> Source code in <code>nearai/config.py</code> <pre><code>class NearAiHubConfig(BaseModel):\n    \"\"\"NearAiHub Config.\n\n    login_with_near (Optional[bool]): Indicates whether to attempt login using Near Auth.\n\n    api_key (Optional[str]): The API key to use if Near Auth is not being utilized\n\n    base_url (Optional[str]): NEAR AI Hub url\n\n    default_provider (Optional[str]): Default provider name\n\n    default_model (Optional[str]): Default model name\n\n    custom_llm_provider (Optional[str]): provider to be used by litellm proxy\n    \"\"\"\n\n    base_url: str = \"https://api.near.ai/v1\"\n    default_provider: str = DEFAULT_PROVIDER\n    default_model: str = DEFAULT_PROVIDER_MODEL\n    custom_llm_provider: str = \"openai\"\n    login_with_near: Optional[bool] = True\n    api_key: Optional[str] = \"\"\n</code></pre>"},{"location":"api/#nearai.dataset","title":"dataset","text":""},{"location":"api/#nearai.dataset.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(name: str, verbose: bool = True) -&gt; Path\n</code></pre> <p>Download the dataset from the registry and download it locally if it hasn't been downloaded yet.</p> <p>:param name: The name of the entry to download the dataset. The format should be namespace/name/version. :return: The path to the downloaded dataset</p> Source code in <code>nearai/dataset.py</code> <pre><code>def get_dataset(name: str, verbose: bool = True) -&gt; Path:\n    \"\"\"Download the dataset from the registry and download it locally if it hasn't been downloaded yet.\n\n    :param name: The name of the entry to download the dataset. The format should be namespace/name/version.\n    :return: The path to the downloaded dataset\n    \"\"\"\n    return registry.download(name, verbose=verbose)\n</code></pre>"},{"location":"api/#nearai.dataset.load_dataset","title":"load_dataset","text":"<pre><code>load_dataset(alias_or_name: str, verbose: bool = True) -&gt; Union[Dataset, DatasetDict]\n</code></pre> <p>Load a dataset from the registry.</p> Source code in <code>nearai/dataset.py</code> <pre><code>def load_dataset(alias_or_name: str, verbose: bool = True) -&gt; Union[Dataset, DatasetDict]:\n    \"\"\"Load a dataset from the registry.\"\"\"\n    path = get_dataset(alias_or_name, verbose=verbose)\n    return load_from_disk(path.as_posix())\n</code></pre>"},{"location":"api/#nearai.delegation","title":"delegation","text":""},{"location":"api/#nearai.delegation.OnBehalfOf","title":"OnBehalfOf","text":"<p>Create a context manager that allows you to delegate actions to another account.</p> <pre><code>with OnBehalfOf(\"scheduler.ai\"):\n    # Upload is done on behalf of scheduler.ai\n    # If delegation permission is not granted, this will raise an exception\n    registry.upload()\n</code></pre> Source code in <code>nearai/delegation.py</code> <pre><code>class OnBehalfOf:\n    \"\"\"Create a context manager that allows you to delegate actions to another account.\n\n    ```python\n    with OnBehalfOf(\"scheduler.ai\"):\n        # Upload is done on behalf of scheduler.ai\n        # If delegation permission is not granted, this will raise an exception\n        registry.upload()\n    ```\n    \"\"\"\n\n    def __init__(self, on_behalf_of: str):\n        \"\"\"Context manager that creates a scope where all actions are done on behalf of another account.\"\"\"\n        self.target_on_behalf_of = on_behalf_of\n        self.original_access_token = None\n\n    def __enter__(self):\n        \"\"\"Set the default client to the account we are acting on behalf of.\"\"\"\n        default_client = ApiClient.get_default()\n        self.original_access_token = default_client.configuration.access_token\n\n        if not isinstance(self.original_access_token, str):\n            return\n\n        assert self.original_access_token.startswith(\"Bearer \")\n        auth = self.original_access_token[len(\"Bearer \") :]\n        auth_data = AuthData.model_validate_json(auth)\n        auth_data.on_behalf_of = self.target_on_behalf_of\n        new_access_token = f\"Bearer {auth_data.generate_bearer_token()}\"\n        default_client.configuration.access_token = new_access_token\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Reset the default client to the original account.\"\"\"\n        default_client = ApiClient.get_default()\n        default_client.configuration.access_token = self.original_access_token\n        self.original_access_token = None\n</code></pre>"},{"location":"api/#nearai.delegation.OnBehalfOf.__enter__","title":"__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Set the default client to the account we are acting on behalf of.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def __enter__(self):\n    \"\"\"Set the default client to the account we are acting on behalf of.\"\"\"\n    default_client = ApiClient.get_default()\n    self.original_access_token = default_client.configuration.access_token\n\n    if not isinstance(self.original_access_token, str):\n        return\n\n    assert self.original_access_token.startswith(\"Bearer \")\n    auth = self.original_access_token[len(\"Bearer \") :]\n    auth_data = AuthData.model_validate_json(auth)\n    auth_data.on_behalf_of = self.target_on_behalf_of\n    new_access_token = f\"Bearer {auth_data.generate_bearer_token()}\"\n    default_client.configuration.access_token = new_access_token\n</code></pre>"},{"location":"api/#nearai.delegation.OnBehalfOf.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Reset the default client to the original account.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Reset the default client to the original account.\"\"\"\n    default_client = ApiClient.get_default()\n    default_client.configuration.access_token = self.original_access_token\n    self.original_access_token = None\n</code></pre>"},{"location":"api/#nearai.delegation.OnBehalfOf.__init__","title":"__init__","text":"<pre><code>__init__(on_behalf_of: str)\n</code></pre> <p>Context manager that creates a scope where all actions are done on behalf of another account.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def __init__(self, on_behalf_of: str):\n    \"\"\"Context manager that creates a scope where all actions are done on behalf of another account.\"\"\"\n    self.target_on_behalf_of = on_behalf_of\n    self.original_access_token = None\n</code></pre>"},{"location":"api/#nearai.delegation.check_on_behalf_of","title":"check_on_behalf_of","text":"<pre><code>check_on_behalf_of()\n</code></pre> <p>Check if the request is being made on behalf of another account.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def check_on_behalf_of():\n    \"\"\"Check if the request is being made on behalf of another account.\"\"\"\n    api = DelegationApi()\n    return api.api_client.configuration.access_token\n</code></pre>"},{"location":"api/#nearai.delegation.revoke_delegation","title":"revoke_delegation","text":"<pre><code>revoke_delegation(delegate_account_id: str)\n</code></pre> <p>Revoke delegation to a specific account.</p> Source code in <code>nearai/delegation.py</code> <pre><code>def revoke_delegation(delegate_account_id: str):\n    \"\"\"Revoke delegation to a specific account.\"\"\"\n    DelegationApi().revoke_delegation_v1_delegation_revoke_delegation_post(delegate_account_id)\n</code></pre>"},{"location":"api/#nearai.evaluation","title":"evaluation","text":""},{"location":"api/#nearai.evaluation._print_metrics_tables","title":"_print_metrics_tables","text":"<pre><code>_print_metrics_tables(rows: List[Dict[str, str]], metric_names: List[str], num_columns: int, all_key_columns: bool, metric_name_max_length: int)\n</code></pre> <p>Builds table(s) and prints them.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def _print_metrics_tables(\n    rows: List[Dict[str, str]],\n    metric_names: List[str],\n    num_columns: int,\n    all_key_columns: bool,\n    metric_name_max_length: int,\n):\n    \"\"\"Builds table(s) and prints them.\"\"\"\n    # Shorten metric names\n    short_metric_names = [_shorten_metric_name(name, metric_name_max_length) for name in metric_names]\n\n    # Prepare the base header and rows\n    base_header = [\"model\", \"agent\"]\n    if all_key_columns:\n        base_header.extend([\"namespace\", \"version\", \"provider\"])\n\n    base_rows = []\n    for row in rows:\n        base_row = [fill(row.pop(\"model\", \"\")), fill(row.pop(\"agent\", \"\"))]\n        namespace = row.pop(\"namespace\", \"\")\n        version = row.pop(\"version\", \"\")\n        provider = row.pop(\"provider\", \"\")\n        if all_key_columns:\n            base_row.extend([fill(namespace), fill(version), fill(provider)])\n        base_rows.append((base_row, row))\n\n    n_metrics_per_table = max(1, num_columns - len(base_header))\n    # Split metrics into groups\n    metric_groups = list(\n        zip(\n            [\n                short_metric_names[i : i + n_metrics_per_table]\n                for i in range(0, len(short_metric_names), n_metrics_per_table)\n            ],\n            [metric_names[i : i + n_metrics_per_table] for i in range(0, len(metric_names), n_metrics_per_table)],\n        )\n    )\n\n    # Print tables\n    for short_group, full_group in metric_groups:\n        header = base_header + short_group\n        table = []\n        for base_row, row_metrics in base_rows:\n            table_row = base_row + [fill(str(row_metrics.get(metric, \"\"))) for metric in full_group]\n            table.append(table_row)\n        print(tabulate(table, headers=header, tablefmt=\"simple_grid\"))\n</code></pre>"},{"location":"api/#nearai.evaluation._shorten_metric_name","title":"_shorten_metric_name","text":"<pre><code>_shorten_metric_name(name: str, max_length: int) -&gt; str\n</code></pre> <p>Shortens metric name if needed.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def _shorten_metric_name(name: str, max_length: int) -&gt; str:\n    \"\"\"Shortens metric name if needed.\"\"\"\n    if len(name) &lt;= max_length:\n        return name\n    keep = max_length - 2  # 2 dots\n    beginning = keep // 3\n    ending = keep - beginning\n    return name[:beginning] + \"..\" + name[-ending:]\n</code></pre>"},{"location":"api/#nearai.evaluation.load_benchmark_entry_info","title":"load_benchmark_entry_info","text":"<pre><code>load_benchmark_entry_info(info: str) -&gt; Any\n</code></pre> <p>Deserializes benchmark info entry from db data.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def load_benchmark_entry_info(info: str) -&gt; Any:\n    \"\"\"Deserializes benchmark info entry from db data.\"\"\"\n    first_decode = json.loads(info)\n    try:\n        second_decode = json.loads(first_decode)\n        return second_decode\n    except json.JSONDecodeError as e:\n        if \"Unterminated string\" in str(e):\n            last_brace = first_decode.rfind(\"}\")\n            if last_brace != -1:\n                try:\n                    return json.loads(first_decode[: last_brace + 1])\n                except json.JSONDecodeError as e:\n                    pass\n    return first_decode\n</code></pre>"},{"location":"api/#nearai.evaluation.print_evaluation_table","title":"print_evaluation_table","text":"<pre><code>print_evaluation_table(rows: List[Dict[str, str]], columns: List[str], important_columns: List[str], all_key_columns: bool, all_metrics: bool, num_columns: int, metric_name_max_length: int) -&gt; None\n</code></pre> <p>Prints table of evaluations.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def print_evaluation_table(\n    rows: List[Dict[str, str]],\n    columns: List[str],\n    important_columns: List[str],\n    all_key_columns: bool,\n    all_metrics: bool,\n    num_columns: int,\n    metric_name_max_length: int,\n) -&gt; None:\n    \"\"\"Prints table of evaluations.\"\"\"\n    metric_names = columns[5:] if all_metrics else important_columns[2:]\n    _print_metrics_tables(rows, metric_names, num_columns, all_key_columns, metric_name_max_length)\n</code></pre>"},{"location":"api/#nearai.evaluation.record_evaluation_metrics","title":"record_evaluation_metrics","text":"<pre><code>record_evaluation_metrics(solver_strategy: SolverStrategy, benchmark_id: int, data_tasks: Union[Dataset, List[dict]], metrics: Dict[str, Any], prepend_evaluation_name: bool = True) -&gt; None\n</code></pre> <p>Uploads evaluation metrics into registry.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def record_evaluation_metrics(\n    solver_strategy: SolverStrategy,\n    benchmark_id: int,\n    data_tasks: Union[Dataset, List[dict]],\n    metrics: Dict[str, Any],\n    prepend_evaluation_name: bool = True,\n) -&gt; None:\n    \"\"\"Uploads evaluation metrics into registry.\"\"\"\n    evaluation_name = solver_strategy.evaluation_name()\n    model = \"\"\n    agent = \"\"\n    version = \"\"\n    model = solver_strategy.model_name\n    agent = solver_strategy.agent_name()\n    version = solver_strategy.agent_version()\n\n    upload_evaluation(\n        evaluation_name,\n        benchmark_id,\n        data_tasks,\n        metrics if not prepend_evaluation_name else _prepend_name_to_metrics(evaluation_name, metrics),\n        model,\n        agent,\n        solver_strategy.evaluated_entry_namespace(),\n        version,\n        solver_strategy.model_provider(),\n    )\n</code></pre>"},{"location":"api/#nearai.evaluation.record_single_score_evaluation","title":"record_single_score_evaluation","text":"<pre><code>record_single_score_evaluation(solver_strategy: SolverStrategy, benchmark_id: int, data_tasks: Union[Dataset, List[dict]], score: float) -&gt; None\n</code></pre> <p>Uploads single score evaluation into registry.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def record_single_score_evaluation(\n    solver_strategy: SolverStrategy, benchmark_id: int, data_tasks: Union[Dataset, List[dict]], score: float\n) -&gt; None:\n    \"\"\"Uploads single score evaluation into registry.\"\"\"\n    evaluation_name = solver_strategy.evaluation_name()\n    record_evaluation_metrics(solver_strategy, benchmark_id, data_tasks, {evaluation_name: score}, False)\n</code></pre>"},{"location":"api/#nearai.evaluation.upload_evaluation","title":"upload_evaluation","text":"<pre><code>upload_evaluation(evaluation_name: str, benchmark_id: int, data_tasks: Union[Dataset, List[dict]], metrics: Dict[str, Any], model: str = '', agent: str = '', namespace: str = '', version: str = '', provider: str = '') -&gt; None\n</code></pre> <p>Uploads evaluation into registry.</p> <p><code>evaluation_name</code>: a unique name for (benchmark, solver) tuple, e.g. \"mbpp\" or \"live_bench\" or \"mmlu-5-shot\". <code>metrics</code>: metrics from evaluation. <code>model</code>: model that was used. <code>agent</code>: agent that was evaluated, in any. <code>namespace</code>: namespace of evaluated agent or evaluated model. <code>version</code>: version of evaluated agent or evaluated model. <code>provider</code>: provider of model used; pass <code>local</code> if running locally.</p> Source code in <code>nearai/evaluation.py</code> <pre><code>def upload_evaluation(\n    evaluation_name: str,\n    benchmark_id: int,\n    data_tasks: Union[Dataset, List[dict]],\n    metrics: Dict[str, Any],\n    model: str = \"\",\n    agent: str = \"\",\n    namespace: str = \"\",\n    version: str = \"\",\n    provider: str = \"\",\n) -&gt; None:\n    \"\"\"Uploads evaluation into registry.\n\n    `evaluation_name`: a unique name for (benchmark, solver) tuple, e.g. \"mbpp\" or \"live_bench\" or \"mmlu-5-shot\".\n    `metrics`: metrics from evaluation.\n    `model`: model that was used.\n    `agent`: agent that was evaluated, in any.\n    `namespace`: namespace of evaluated agent or evaluated model.\n    `version`: version of evaluated agent or evaluated model.\n    `provider`: provider of model used; pass `local` if running locally.\n    \"\"\"\n    key = f\"evaluation_{evaluation_name}\"\n    metrics[EVALUATED_ENTRY_METADATA] = {}\n    if agent != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"agent\"] = agent\n        key += f\"_agent_{agent}\"\n    if model != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"model\"] = model\n        key += f\"_model_{model}\"\n    if namespace != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"namespace\"] = namespace\n        key += f\"_namespace_{namespace}\"\n    if version != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"version\"] = version\n        key += f\"_version_{version}\"\n    if provider != \"\":\n        metrics[EVALUATED_ENTRY_METADATA][\"provider\"] = provider\n        key += f\"_provider_{provider}\"\n\n    entry_path = get_registry_folder() / key\n    # Create folder entry_path if not present\n    entry_path.mkdir(parents=True, exist_ok=True)\n    # Write file metrics.json inside\n    metrics_file = entry_path / \"metrics.json\"\n    with metrics_file.open(\"w\") as f:\n        json.dump(metrics, f, indent=2)\n\n    # Get solutions from cache in benchmark.py\n    cache = BenchmarkApi().get_benchmark_result_v1_benchmark_get_result_get(benchmark_id)\n    solutions = []\n    for result in cache:\n        try:\n            solution = {\n                \"datum\": data_tasks[result.index],\n                \"status\": result.solved,\n                \"info\": load_benchmark_entry_info(result.info) if result.info else {},\n            }\n            solutions.append(solution)\n        except (AttributeError, json.JSONDecodeError, TypeError) as e:\n            print(f\"Exception while creating solutions data: {str(e)}.\")\n            # Skip entries that can't be properly formatted\n            continue\n\n    # Write solutions file\n    solutions_file = entry_path / \"solutions.json\"\n    with solutions_file.open(\"w\") as f:\n        json.dump(solutions, f, indent=2)\n\n    metadata_path = entry_path / \"metadata.json\"\n    # TODO(#273): Currently that will not update existing evaluation.\n    with open(metadata_path, \"w\") as f:\n        json.dump(\n            {\n                \"name\": key,\n                \"version\": \"0.1.0\",\n                \"description\": \"\",\n                \"category\": \"evaluation\",\n                \"tags\": [],\n                \"details\": {},\n                \"show_entry\": True,\n            },\n            f,\n            indent=2,\n        )\n\n    registry.upload(Path(entry_path), show_progress=True)\n</code></pre>"},{"location":"api/#nearai.finetune","title":"finetune","text":""},{"location":"api/#nearai.finetune.FinetuneCli","title":"FinetuneCli","text":"Source code in <code>nearai/finetune/__init__.py</code> <pre><code>class FinetuneCli:\n    def start(\n        self,\n        model: str,\n        tokenizer: str,\n        dataset: str,\n        num_procs: int,\n        format: str,\n        upload_checkpoint: bool = True,\n        num_nodes: int = 1,\n        job_id: Optional[str] = None,\n        checkpoint: Optional[str] = None,\n        **dataset_kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Start a finetuning job on the current node.\n\n        Args:\n        ----\n            model: Name of a model in the registry. Base model to finetune.\n            tokenizer: Name of a tokenizer in the registry. Using tokenizer.model format.\n            dataset: Name of a dataset in the registry.\n            num_procs: Number of GPUs to use for training\n            format: Name of the configuration file to use. For example llama3-70b, llama3-8b. Valid options are in etc/finetune.\n            upload_checkpoint: Whether to upload the checkpoint to the registry. Default is True.\n            num_nodes: Number of nodes to use for training. Default is 1.\n            job_id: Unique identifier for the job. Default is None.\n            checkpoint: Name of the model checkpoint to start from. Default is None.\n            dataset_kwargs: Additional keyword arguments to pass to the dataset constructor.\n\n        \"\"\"  # noqa: E501\n        from nearai.dataset import get_dataset\n\n        assert num_nodes &gt;= 1\n\n        # Prepare job id folder\n        if job_id is None:\n            job_id = \"job\"\n        job_id = f\"{job_id}-{timestamp()}-{randint(10**8, 10**9 - 1)}\"\n        job_folder = DATA_FOLDER / \"finetune\" / job_id\n        job_folder.mkdir(parents=True, exist_ok=True)\n\n        # Either use the provided config file template or load one predefined one\n        if Path(format).exists():\n            config_template_path = Path(format)\n        else:\n            configs = ETC_FOLDER / \"finetune\"\n            config_template_path = configs / f\"{format}.yml\"\n\n        if not config_template_path.exists():\n            raise FileNotFoundError(f\"Config file not found: {config_template_path}\")\n\n        CONFIG_TEMPLATE = config_template_path.read_text()  # noqa: N806\n\n        # Download model\n        model_path = get_model(model)\n\n        # Download tokenizer\n        tokenizer_path = registry.download(tokenizer) / \"tokenizer.model\"\n        assert tokenizer_path.exists(), f\"tokenizer.model not found in {tokenizer_path}\"\n\n        # Download checkpoint if any\n        checkpoint_path = get_model(checkpoint) if checkpoint else \"null\"\n        resume_checkpoint = checkpoint_path != \"null\"\n\n        # Download dataset\n        dataset_path = get_dataset(dataset)\n\n        # Set up output directories\n        checkpoint_output_dir = job_folder / \"checkpoint_output\"\n        logging_output_dir = job_folder / \"logs\"\n        logging_output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Prepare config file\n        dataset_args_dict = deepcopy(dataset_kwargs)\n\n        dataset_args_dict[\"_component_\"] = dataset_args_dict.pop(\"method\")\n        dataset_args_dict[\"source\"] = str(dataset_path.absolute())\n        dataset_args = \"\\n\".join(f\"  {key}: {value}\" for key, value in dataset_args_dict.items())\n\n        config = job_folder / \"config.yaml\"\n        with open(config, \"w\") as f:\n            f.write(\n                CONFIG_TEMPLATE.format(\n                    TOKENIZER=str(tokenizer_path),\n                    MODEL=str(model_path),\n                    RECIPE_CHECKPOINT=checkpoint_path,\n                    RESUME_FROM_CHECKPOINT=resume_checkpoint,\n                    CHECKPOINT_OUTPUT_DIR=str(checkpoint_output_dir),\n                    DATASET_ARGS=dataset_args,\n                    LOGGING_OUTPUT_DIR=str(logging_output_dir),\n                )\n            )\n\n        # Spawn background thread to read logs and push to database\n        threading.Thread(target=find_new_logs_background, args=(logging_output_dir, job_id)).start()\n\n        print(\"Starting job at\", job_folder)\n        if num_nodes == 1:\n            run(\n                [\n                    \"tune\",\n                    \"run\",\n                    \"--nproc_per_node\",\n                    str(num_procs),\n                    \"lora_finetune_distributed\",\n                    \"--config\",\n                    str(config),\n                ]\n            )\n        else:\n            # Fetch rank and master addr from environment variables\n            raise NotImplementedError()\n\n        global BACKGROUND_PROCESS\n        BACKGROUND_PROCESS = False\n\n        if upload_checkpoint:\n            registry.upload(\n                job_folder,\n                EntryMetadata.from_dict(\n                    {\n                        \"name\": f\"finetune-{job_id}\",\n                        \"version\": \"0.0.1\",\n                        \"description\": f\"Finetuned checkpoint from base mode {model} using dataset {dataset}\",\n                        \"category\": \"finetune\",\n                        \"tags\": [\"finetune\", f\"base-model-{model}\", f\"base-dataset-{dataset}\"],\n                        \"details\": dict(\n                            model=model,\n                            tokenizer=tokenizer,\n                            dataset=dataset,\n                            num_procs=num_procs,\n                            format=format,\n                            num_nodes=num_nodes,\n                            checkpoint=checkpoint,\n                            **dataset_kwargs,\n                        ),\n                        \"show_entry\": True,\n                    }\n                ),\n                show_progress=True,\n            )\n\n    def inspect(self, job_id: str) -&gt; None:  # noqa: D102\n        raise NotImplementedError()\n</code></pre>"},{"location":"api/#nearai.finetune.FinetuneCli.start","title":"start","text":"<pre><code>start(model: str, tokenizer: str, dataset: str, num_procs: int, format: str, upload_checkpoint: bool = True, num_nodes: int = 1, job_id: Optional[str] = None, checkpoint: Optional[str] = None, **dataset_kwargs: Any) -&gt; None\n</code></pre> <p>Start a finetuning job on the current node.</p> <pre><code>model: Name of a model in the registry. Base model to finetune.\ntokenizer: Name of a tokenizer in the registry. Using tokenizer.model format.\ndataset: Name of a dataset in the registry.\nnum_procs: Number of GPUs to use for training\nformat: Name of the configuration file to use. For example llama3-70b, llama3-8b. Valid options are in etc/finetune.\nupload_checkpoint: Whether to upload the checkpoint to the registry. Default is True.\nnum_nodes: Number of nodes to use for training. Default is 1.\njob_id: Unique identifier for the job. Default is None.\ncheckpoint: Name of the model checkpoint to start from. Default is None.\ndataset_kwargs: Additional keyword arguments to pass to the dataset constructor.\n</code></pre> Source code in <code>nearai/finetune/__init__.py</code> <pre><code>def start(\n    self,\n    model: str,\n    tokenizer: str,\n    dataset: str,\n    num_procs: int,\n    format: str,\n    upload_checkpoint: bool = True,\n    num_nodes: int = 1,\n    job_id: Optional[str] = None,\n    checkpoint: Optional[str] = None,\n    **dataset_kwargs: Any,\n) -&gt; None:\n    \"\"\"Start a finetuning job on the current node.\n\n    Args:\n    ----\n        model: Name of a model in the registry. Base model to finetune.\n        tokenizer: Name of a tokenizer in the registry. Using tokenizer.model format.\n        dataset: Name of a dataset in the registry.\n        num_procs: Number of GPUs to use for training\n        format: Name of the configuration file to use. For example llama3-70b, llama3-8b. Valid options are in etc/finetune.\n        upload_checkpoint: Whether to upload the checkpoint to the registry. Default is True.\n        num_nodes: Number of nodes to use for training. Default is 1.\n        job_id: Unique identifier for the job. Default is None.\n        checkpoint: Name of the model checkpoint to start from. Default is None.\n        dataset_kwargs: Additional keyword arguments to pass to the dataset constructor.\n\n    \"\"\"  # noqa: E501\n    from nearai.dataset import get_dataset\n\n    assert num_nodes &gt;= 1\n\n    # Prepare job id folder\n    if job_id is None:\n        job_id = \"job\"\n    job_id = f\"{job_id}-{timestamp()}-{randint(10**8, 10**9 - 1)}\"\n    job_folder = DATA_FOLDER / \"finetune\" / job_id\n    job_folder.mkdir(parents=True, exist_ok=True)\n\n    # Either use the provided config file template or load one predefined one\n    if Path(format).exists():\n        config_template_path = Path(format)\n    else:\n        configs = ETC_FOLDER / \"finetune\"\n        config_template_path = configs / f\"{format}.yml\"\n\n    if not config_template_path.exists():\n        raise FileNotFoundError(f\"Config file not found: {config_template_path}\")\n\n    CONFIG_TEMPLATE = config_template_path.read_text()  # noqa: N806\n\n    # Download model\n    model_path = get_model(model)\n\n    # Download tokenizer\n    tokenizer_path = registry.download(tokenizer) / \"tokenizer.model\"\n    assert tokenizer_path.exists(), f\"tokenizer.model not found in {tokenizer_path}\"\n\n    # Download checkpoint if any\n    checkpoint_path = get_model(checkpoint) if checkpoint else \"null\"\n    resume_checkpoint = checkpoint_path != \"null\"\n\n    # Download dataset\n    dataset_path = get_dataset(dataset)\n\n    # Set up output directories\n    checkpoint_output_dir = job_folder / \"checkpoint_output\"\n    logging_output_dir = job_folder / \"logs\"\n    logging_output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Prepare config file\n    dataset_args_dict = deepcopy(dataset_kwargs)\n\n    dataset_args_dict[\"_component_\"] = dataset_args_dict.pop(\"method\")\n    dataset_args_dict[\"source\"] = str(dataset_path.absolute())\n    dataset_args = \"\\n\".join(f\"  {key}: {value}\" for key, value in dataset_args_dict.items())\n\n    config = job_folder / \"config.yaml\"\n    with open(config, \"w\") as f:\n        f.write(\n            CONFIG_TEMPLATE.format(\n                TOKENIZER=str(tokenizer_path),\n                MODEL=str(model_path),\n                RECIPE_CHECKPOINT=checkpoint_path,\n                RESUME_FROM_CHECKPOINT=resume_checkpoint,\n                CHECKPOINT_OUTPUT_DIR=str(checkpoint_output_dir),\n                DATASET_ARGS=dataset_args,\n                LOGGING_OUTPUT_DIR=str(logging_output_dir),\n            )\n        )\n\n    # Spawn background thread to read logs and push to database\n    threading.Thread(target=find_new_logs_background, args=(logging_output_dir, job_id)).start()\n\n    print(\"Starting job at\", job_folder)\n    if num_nodes == 1:\n        run(\n            [\n                \"tune\",\n                \"run\",\n                \"--nproc_per_node\",\n                str(num_procs),\n                \"lora_finetune_distributed\",\n                \"--config\",\n                str(config),\n            ]\n        )\n    else:\n        # Fetch rank and master addr from environment variables\n        raise NotImplementedError()\n\n    global BACKGROUND_PROCESS\n    BACKGROUND_PROCESS = False\n\n    if upload_checkpoint:\n        registry.upload(\n            job_folder,\n            EntryMetadata.from_dict(\n                {\n                    \"name\": f\"finetune-{job_id}\",\n                    \"version\": \"0.0.1\",\n                    \"description\": f\"Finetuned checkpoint from base mode {model} using dataset {dataset}\",\n                    \"category\": \"finetune\",\n                    \"tags\": [\"finetune\", f\"base-model-{model}\", f\"base-dataset-{dataset}\"],\n                    \"details\": dict(\n                        model=model,\n                        tokenizer=tokenizer,\n                        dataset=dataset,\n                        num_procs=num_procs,\n                        format=format,\n                        num_nodes=num_nodes,\n                        checkpoint=checkpoint,\n                        **dataset_kwargs,\n                    ),\n                    \"show_entry\": True,\n                }\n            ),\n            show_progress=True,\n        )\n</code></pre>"},{"location":"api/#nearai.finetune.parse_line","title":"parse_line","text":"<pre><code>parse_line(line: str) -&gt; Tuple[int, dict[str, float]]\n</code></pre> <p>Example of line to be parsed.</p> <p>Step 33 | loss:1.5400923490524292 lr:9.9e-05 tokens_per_second_per_gpu:101.22285588141214</p> Source code in <code>nearai/finetune/__init__.py</code> <pre><code>def parse_line(line: str) -&gt; Tuple[int, dict[str, float]]:\n    \"\"\"Example of line to be parsed.\n\n    Step 33 | loss:1.5400923490524292 lr:9.9e-05 tokens_per_second_per_gpu:101.22285588141214\n    \"\"\"\n    step_raw, metrics_raw = map(str.strip, line.strip(\" \\n\").split(\"|\"))\n    step = int(step_raw.split(\" \")[-1])\n    metrics = {metric[0]: float(metric[1]) for metric in map(lambda metric: metric.split(\":\"), metrics_raw.split(\" \"))}\n    return step, metrics\n</code></pre>"},{"location":"api/#nearai.finetune.text_completion","title":"text_completion","text":""},{"location":"api/#nearai.finetune.text_completion.TextCompletionDataset","title":"TextCompletionDataset","text":"<p>               Bases: <code>Dataset</code></p> <p>Freeform dataset for any unstructured text corpus. Quickly load any dataset from Hugging Face or local disk and tokenize it for your model.</p> <pre><code>tokenizer (BaseTokenizer): Tokenizer used to encode data. Tokenize must implement an ``encode`` and ``decode`` method.\nsource (str): path string of dataset, anything supported by Hugging Face's ``load_dataset``\n    (https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path)\ncolumn (str): name of column in the sample that contains the text data. This is typically required\n    for Hugging Face datasets or tabular data. For local datasets with a single column, use the default \"text\",\n    which is what is assigned by Hugging Face datasets when loaded into memory. Default is \"text\".\nmax_seq_len (Optional[int]): Maximum number of tokens in the returned input and label token id lists.\n    Default is None, disabling truncation. We recommend setting this to the highest you can fit in memory\n    and is supported by the model. For example, llama2-7B supports up to 4096 for sequence length.\n**load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``.\n</code></pre> Source code in <code>nearai/finetune/text_completion.py</code> <pre><code>class TextCompletionDataset(Dataset):\n    \"\"\"Freeform dataset for any unstructured text corpus. Quickly load any dataset from Hugging Face or local disk and tokenize it for your model.\n\n    Args:\n    ----\n        tokenizer (BaseTokenizer): Tokenizer used to encode data. Tokenize must implement an ``encode`` and ``decode`` method.\n        source (str): path string of dataset, anything supported by Hugging Face's ``load_dataset``\n            (https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset.path)\n        column (str): name of column in the sample that contains the text data. This is typically required\n            for Hugging Face datasets or tabular data. For local datasets with a single column, use the default \"text\",\n            which is what is assigned by Hugging Face datasets when loaded into memory. Default is \"text\".\n        max_seq_len (Optional[int]): Maximum number of tokens in the returned input and label token id lists.\n            Default is None, disabling truncation. We recommend setting this to the highest you can fit in memory\n            and is supported by the model. For example, llama2-7B supports up to 4096 for sequence length.\n        **load_dataset_kwargs (Dict[str, Any]): additional keyword arguments to pass to ``load_dataset``.\n\n    \"\"\"  # noqa: E501\n\n    def __init__(  # noqa: D107\n        self,\n        tokenizer: BaseTokenizer,\n        source: str,\n        column: str = \"text\",\n        split: Optional[str] = None,\n        max_seq_len: Optional[int] = None,\n        **load_dataset_kwargs: Dict[str, Any],\n    ) -&gt; None:\n        self._tokenizer = tokenizer\n        self._data = load_from_disk(source, **load_dataset_kwargs)\n        if split is not None:\n            self._data = self._data[split]\n        self.max_seq_len = max_seq_len\n        self._column = column\n\n    def __len__(self) -&gt; int:  # noqa: D105\n        return len(self._data)\n\n    def __getitem__(self, index: int) -&gt; Dict[str, List[int]]:  # noqa: D105\n        sample = self._data[index]\n        return self._prepare_sample(sample)\n\n    def _prepare_sample(self, sample: Mapping[str, Any]) -&gt; Dict[str, List[int]]:\n        prompt = sample[self._column]\n        tokens = self._tokenizer.encode(text=prompt, add_bos=True, add_eos=True)\n\n        # Truncate if needed, but don't coerce EOS id\n        if self.max_seq_len is not None:\n            tokens = truncate(tokens, self.max_seq_len - 1)\n\n        # No need to offset labels by 1 - happens in the recipe\n        labels = tokens.copy()\n\n        return {\"tokens\": tokens, \"labels\": labels}\n</code></pre>"},{"location":"api/#nearai.finetune.text_completion.truncate","title":"truncate","text":"<pre><code>truncate(tokens: List[Any], max_seq_len: int, eos_id: Optional[Any] = None) -&gt; List[Any]\n</code></pre> <p>Truncate a list of tokens to a maximum length. If eos_id is provided, the last token will be replaced with eos_id.</p> <pre><code>tokens (List[Any]): list of tokens to truncate\nmax_seq_len (int): maximum length of the list\neos_id (Optional[Any]): token to replace the last token with. If None, the\n    last token will not be replaced. Default is None.\n</code></pre> <pre><code>List[Any]: truncated list of tokens\n</code></pre> Source code in <code>nearai/finetune/text_completion.py</code> <pre><code>def truncate(\n    tokens: List[Any],\n    max_seq_len: int,\n    eos_id: Optional[Any] = None,\n) -&gt; List[Any]:\n    \"\"\"Truncate a list of tokens to a maximum length. If eos_id is provided, the last token will be replaced with eos_id.\n\n    Args:\n    ----\n        tokens (List[Any]): list of tokens to truncate\n        max_seq_len (int): maximum length of the list\n        eos_id (Optional[Any]): token to replace the last token with. If None, the\n            last token will not be replaced. Default is None.\n\n    Returns:\n    -------\n        List[Any]: truncated list of tokens\n\n    \"\"\"  # noqa: E501\n    tokens_truncated = tokens[:max_seq_len]\n    if eos_id is not None and tokens_truncated[-1] != eos_id:\n        tokens_truncated[-1] = eos_id\n    return tokens_truncated\n</code></pre>"},{"location":"api/#nearai.hub","title":"hub","text":""},{"location":"api/#nearai.hub.Hub","title":"Hub","text":"<p>               Bases: <code>object</code></p> Source code in <code>nearai/hub.py</code> <pre><code>class Hub(object):\n    def __init__(self, config: Config) -&gt; None:\n        \"\"\"Initializes the Hub class with the given configuration.\"\"\"\n        self.info = None\n        self.provider = None\n        self.model = None\n        self.endpoint = None\n        self.query = None\n        self._config = config\n\n    def parse_hub_chat_params(self, kwargs):\n        \"\"\"Parses and sets instance attributes from the given keyword arguments, using default values if needed.\"\"\"\n        if self._config.nearai_hub is None:\n            self._config.nearai_hub = NearAiHubConfig()\n\n        self.query = kwargs.get(\"query\")\n        self.endpoint = kwargs.get(\"endpoint\", f\"{self._config.nearai_hub.base_url}/chat/completions\")\n        self.model = kwargs.get(\"model\", self._config.nearai_hub.default_model)\n        self.provider = kwargs.get(\"provider\", self._config.nearai_hub.default_provider)\n        self.info = kwargs.get(\"info\", False)\n\n    def chat(self, kwargs):\n        \"\"\"Processes a chat request by sending parameters to the NEAR AI Hub and printing the response.\"\"\"\n        try:\n            self.parse_hub_chat_params(kwargs)\n\n            if not self.query:\n                return print(\"Error: 'query' is required for the `hub chat` command.\")\n\n            if self._config.nearai_hub is None:\n                self._config.nearai_hub = NearAiHubConfig()\n\n            data = {\n                \"max_tokens\": 256,\n                \"temperature\": 1,\n                \"frequency_penalty\": 0,\n                \"n\": 1,\n                \"messages\": [{\"role\": \"user\", \"content\": str(self.query)}],\n                \"model\": self.model,\n            }\n\n            auth = self._config.auth\n\n            if self._config.nearai_hub.login_with_near:\n                bearer_token = auth.generate_bearer_token()\n                headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {bearer_token}\"}\n\n                data[\"provider\"] = self.provider\n            elif self._config.nearai_hub.api_key:\n                headers = {\n                    \"Content-Type\": \"application/json\",\n                    \"Authorization\": \"Bearer {}\".format(self._config.nearai_hub.api_key),\n                }\n            else:\n                return print(\"Illegal NEAR AI Hub Config\")\n\n            if self.info:\n                print(f\"Requesting hub using NEAR Account {auth.account_id}\")\n\n            response = requests.post(self.endpoint, headers=headers, data=json.dumps(data))\n\n            completion = response.json()\n\n            print(completion[\"choices\"][0][\"message\"][\"content\"])\n\n        except Exception as e:\n            print(f\"Request failed: {e}\")\n</code></pre>"},{"location":"api/#nearai.hub.Hub.__init__","title":"__init__","text":"<pre><code>__init__(config: Config) -&gt; None\n</code></pre> <p>Initializes the Hub class with the given configuration.</p> Source code in <code>nearai/hub.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    \"\"\"Initializes the Hub class with the given configuration.\"\"\"\n    self.info = None\n    self.provider = None\n    self.model = None\n    self.endpoint = None\n    self.query = None\n    self._config = config\n</code></pre>"},{"location":"api/#nearai.hub.Hub.chat","title":"chat","text":"<pre><code>chat(kwargs)\n</code></pre> <p>Processes a chat request by sending parameters to the NEAR AI Hub and printing the response.</p> Source code in <code>nearai/hub.py</code> <pre><code>def chat(self, kwargs):\n    \"\"\"Processes a chat request by sending parameters to the NEAR AI Hub and printing the response.\"\"\"\n    try:\n        self.parse_hub_chat_params(kwargs)\n\n        if not self.query:\n            return print(\"Error: 'query' is required for the `hub chat` command.\")\n\n        if self._config.nearai_hub is None:\n            self._config.nearai_hub = NearAiHubConfig()\n\n        data = {\n            \"max_tokens\": 256,\n            \"temperature\": 1,\n            \"frequency_penalty\": 0,\n            \"n\": 1,\n            \"messages\": [{\"role\": \"user\", \"content\": str(self.query)}],\n            \"model\": self.model,\n        }\n\n        auth = self._config.auth\n\n        if self._config.nearai_hub.login_with_near:\n            bearer_token = auth.generate_bearer_token()\n            headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {bearer_token}\"}\n\n            data[\"provider\"] = self.provider\n        elif self._config.nearai_hub.api_key:\n            headers = {\n                \"Content-Type\": \"application/json\",\n                \"Authorization\": \"Bearer {}\".format(self._config.nearai_hub.api_key),\n            }\n        else:\n            return print(\"Illegal NEAR AI Hub Config\")\n\n        if self.info:\n            print(f\"Requesting hub using NEAR Account {auth.account_id}\")\n\n        response = requests.post(self.endpoint, headers=headers, data=json.dumps(data))\n\n        completion = response.json()\n\n        print(completion[\"choices\"][0][\"message\"][\"content\"])\n\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n</code></pre>"},{"location":"api/#nearai.hub.Hub.parse_hub_chat_params","title":"parse_hub_chat_params","text":"<pre><code>parse_hub_chat_params(kwargs)\n</code></pre> <p>Parses and sets instance attributes from the given keyword arguments, using default values if needed.</p> Source code in <code>nearai/hub.py</code> <pre><code>def parse_hub_chat_params(self, kwargs):\n    \"\"\"Parses and sets instance attributes from the given keyword arguments, using default values if needed.\"\"\"\n    if self._config.nearai_hub is None:\n        self._config.nearai_hub = NearAiHubConfig()\n\n    self.query = kwargs.get(\"query\")\n    self.endpoint = kwargs.get(\"endpoint\", f\"{self._config.nearai_hub.base_url}/chat/completions\")\n    self.model = kwargs.get(\"model\", self._config.nearai_hub.default_model)\n    self.provider = kwargs.get(\"provider\", self._config.nearai_hub.default_provider)\n    self.info = kwargs.get(\"info\", False)\n</code></pre>"},{"location":"api/#nearai.lib","title":"lib","text":""},{"location":"api/#nearai.lib.parse_location","title":"parse_location","text":"<pre><code>parse_location(entry_location: str) -&gt; EntryLocation\n</code></pre> <p>Create a EntryLocation from a string in the format namespace/name/version.</p> Source code in <code>nearai/lib.py</code> <pre><code>def parse_location(entry_location: str) -&gt; EntryLocation:\n    \"\"\"Create a EntryLocation from a string in the format namespace/name/version.\"\"\"\n    match = entry_location_pattern.match(entry_location)\n\n    if match is None:\n        raise ValueError(f\"Invalid entry format: {entry_location}. Should have the format &lt;namespace&gt;/&lt;name&gt;/&lt;version&gt;\")\n\n    return EntryLocation(\n        namespace=match.group(\"namespace\"),\n        name=match.group(\"name\"),\n        version=match.group(\"version\"),\n    )\n</code></pre>"},{"location":"api/#nearai.login","title":"login","text":""},{"location":"api/#nearai.login.AuthHandler","title":"AuthHandler","text":"<p>               Bases: <code>SimpleHTTPRequestHandler</code></p> Source code in <code>nearai/login.py</code> <pre><code>class AuthHandler(http.server.SimpleHTTPRequestHandler):\n    def log_message(self, format, *args):\n        \"\"\"Webserver logging method.\"\"\"\n        pass  # Override to suppress logging\n\n    def do_GET(self):  # noqa: N802\n        \"\"\"Webserver GET method.\"\"\"\n        global NONCE, PORT\n\n        script_path = Path(__file__).resolve()\n        assets_folder = script_path.parent / \"assets\"\n\n        if self.path.startswith(\"/capture\"):\n            with open(os.path.join(assets_folder, \"auth_capture.html\"), \"r\", encoding=\"utf-8\") as file:\n                content = file.read()\n            self.send_response(200)\n            self.send_header(\"Content-type\", \"text/html\")\n            self.end_headers()\n            self.wfile.write(content.encode(\"utf-8\"))\n\n        if self.path.startswith(\"/auth\"):\n            parsed_url = urlparse.urlparse(self.path)\n            fragment = parsed_url.query\n            params = urlparse.parse_qs(fragment)\n\n            required_params = [\"accountId\", \"signature\", \"publicKey\"]\n\n            if all(param in params for param in required_params):\n                update_auth_config(\n                    params[\"accountId\"][0],\n                    params[\"signature\"][0],\n                    params[\"publicKey\"][0],\n                    callback_url=generate_callback_url(PORT),\n                    nonce=NONCE,\n                )\n            else:\n                print(\"Required parameters not found\")\n\n            with open(os.path.join(assets_folder, \"auth_complete.html\"), \"r\", encoding=\"utf-8\") as file:\n                content = file.read()\n            self.send_response(200)\n            self.send_header(\"Content-type\", \"text/html\")\n            self.end_headers()\n            self.wfile.write(content.encode(\"utf-8\"))\n\n            # Give the server some time to read the response before shutting it down\n            def shutdown_server():\n                global httpd\n                time.sleep(2)  # Wait 2 seconds before shutting down\n                if httpd:\n                    httpd.shutdown()\n\n            threading.Thread(target=shutdown_server).start()\n</code></pre>"},{"location":"api/#nearai.login.AuthHandler.do_GET","title":"do_GET","text":"<pre><code>do_GET()\n</code></pre> <p>Webserver GET method.</p> Source code in <code>nearai/login.py</code> <pre><code>def do_GET(self):  # noqa: N802\n    \"\"\"Webserver GET method.\"\"\"\n    global NONCE, PORT\n\n    script_path = Path(__file__).resolve()\n    assets_folder = script_path.parent / \"assets\"\n\n    if self.path.startswith(\"/capture\"):\n        with open(os.path.join(assets_folder, \"auth_capture.html\"), \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n        self.send_response(200)\n        self.send_header(\"Content-type\", \"text/html\")\n        self.end_headers()\n        self.wfile.write(content.encode(\"utf-8\"))\n\n    if self.path.startswith(\"/auth\"):\n        parsed_url = urlparse.urlparse(self.path)\n        fragment = parsed_url.query\n        params = urlparse.parse_qs(fragment)\n\n        required_params = [\"accountId\", \"signature\", \"publicKey\"]\n\n        if all(param in params for param in required_params):\n            update_auth_config(\n                params[\"accountId\"][0],\n                params[\"signature\"][0],\n                params[\"publicKey\"][0],\n                callback_url=generate_callback_url(PORT),\n                nonce=NONCE,\n            )\n        else:\n            print(\"Required parameters not found\")\n\n        with open(os.path.join(assets_folder, \"auth_complete.html\"), \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n        self.send_response(200)\n        self.send_header(\"Content-type\", \"text/html\")\n        self.end_headers()\n        self.wfile.write(content.encode(\"utf-8\"))\n\n        # Give the server some time to read the response before shutting it down\n        def shutdown_server():\n            global httpd\n            time.sleep(2)  # Wait 2 seconds before shutting down\n            if httpd:\n                httpd.shutdown()\n\n        threading.Thread(target=shutdown_server).start()\n</code></pre>"},{"location":"api/#nearai.login.AuthHandler.log_message","title":"log_message","text":"<pre><code>log_message(format, *args)\n</code></pre> <p>Webserver logging method.</p> Source code in <code>nearai/login.py</code> <pre><code>def log_message(self, format, *args):\n    \"\"\"Webserver logging method.\"\"\"\n    pass  # Override to suppress logging\n</code></pre>"},{"location":"api/#nearai.login.find_open_port","title":"find_open_port","text":"<pre><code>find_open_port() -&gt; int\n</code></pre> <p>Finds and returns an open port number by binding to a free port on the local machine.</p> Source code in <code>nearai/login.py</code> <pre><code>def find_open_port() -&gt; int:\n    \"\"\"Finds and returns an open port number by binding to a free port on the local machine.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((\"\", 0))\n        return s.getsockname()[1]\n</code></pre>"},{"location":"api/#nearai.login.generate_and_save_signature","title":"generate_and_save_signature","text":"<pre><code>generate_and_save_signature(account_id, private_key)\n</code></pre> <p>Generates a signature for the given account ID and private key, then updates the auth configuration.</p> Source code in <code>nearai/login.py</code> <pre><code>def generate_and_save_signature(account_id, private_key):\n    \"\"\"Generates a signature for the given account ID and private key, then updates the auth configuration.\"\"\"\n    nonce = generate_nonce()\n    payload = near.Payload(MESSAGE, nonce, RECIPIENT, \"\")\n\n    signature, public_key = near.create_signature(private_key, payload)\n\n    if update_auth_config(account_id, signature, public_key, \"\", nonce):\n        print_login_status()\n</code></pre>"},{"location":"api/#nearai.login.generate_callback_url","title":"generate_callback_url","text":"<pre><code>generate_callback_url(port)\n</code></pre> <p>Generates a callback URL using the specified port number.</p> Source code in <code>nearai/login.py</code> <pre><code>def generate_callback_url(port):\n    \"\"\"Generates a callback URL using the specified port number.\"\"\"\n    return f\"http://localhost:{port}/capture\"\n</code></pre>"},{"location":"api/#nearai.login.generate_nonce","title":"generate_nonce","text":"<pre><code>generate_nonce()\n</code></pre> <p>Generates a nonce based on the current time in milliseconds.</p> Source code in <code>nearai/login.py</code> <pre><code>def generate_nonce():\n    \"\"\"Generates a nonce based on the current time in milliseconds.\"\"\"\n    return str(int(time.time() * 1000))\n</code></pre>"},{"location":"api/#nearai.login.login_with_file_credentials","title":"login_with_file_credentials","text":"<pre><code>login_with_file_credentials(account_id)\n</code></pre> <p>Logs in using credentials from a file for the specified account ID, generating and saving a signature.</p> Source code in <code>nearai/login.py</code> <pre><code>def login_with_file_credentials(account_id):\n    \"\"\"Logs in using credentials from a file for the specified account ID, generating and saving a signature.\"\"\"\n    file_path = os.path.expanduser(os.path.join(\"~/.near-credentials/\", \"mainnet\", f\"{account_id}.json\"))\n\n    if os.path.exists(file_path):\n        with open(file_path, \"r\") as file:\n            content = file.read()\n            account_data = json.loads(content)\n            private_key = account_data.get(\"private_key\", None)\n            if not private_key:\n                return print(f\"Private key is missing for {account_id} on mainnet\")\n            generate_and_save_signature(account_id, account_data[\"private_key\"])\n\n    else:\n        return print(f\"Account data is missing for {account_id}\")\n</code></pre>"},{"location":"api/#nearai.login.login_with_near_auth","title":"login_with_near_auth","text":"<pre><code>login_with_near_auth(remote, auth_url)\n</code></pre> <p>Initiates the login process using NEAR authentication, either starting a local server to handle the callback or providing a URL for remote authentication.</p> Source code in <code>nearai/login.py</code> <pre><code>def login_with_near_auth(remote, auth_url):\n    \"\"\"Initiates the login process using NEAR authentication, either starting a local server to handle the callback or providing a URL for remote authentication.\"\"\"  # noqa: E501\n    global NONCE, PORT\n    NONCE = generate_nonce()\n\n    params = {\n        \"message\": MESSAGE,\n        \"nonce\": NONCE,\n        \"recipient\": RECIPIENT,\n    }\n\n    if not remote:\n        PORT = find_open_port()\n\n        global httpd\n        with socketserver.TCPServer((\"\", PORT), AuthHandler) as httpd:\n            params[\"callbackUrl\"] = f\"http://localhost:{PORT}/capture\"\n\n            encoded_params = urlparse.urlencode(params)\n\n            print_url_message(f\"{auth_url}?{encoded_params}\")\n\n            httpd.serve_forever()\n\n    else:\n        encoded_params = urlparse.urlencode(params)\n\n        print_url_message(f\"{auth_url}?{encoded_params}\")\n        print(\"After visiting the URL, follow the instructions to save your auth signature\")\n</code></pre>"},{"location":"api/#nearai.login.print_login_status","title":"print_login_status","text":"<pre><code>print_login_status()\n</code></pre> <p>Prints the current authentication status if available in the config file.</p> Source code in <code>nearai/login.py</code> <pre><code>def print_login_status():\n    \"\"\"Prints the current authentication status if available in the config file.\"\"\"\n    config = load_config_file()\n    if config.get(\"auth\") and config[\"auth\"].get(\"account_id\"):\n        print(f'Auth data for: {config[\"auth\"][\"account_id\"]}')\n        print(f'signature: {config[\"auth\"][\"signature\"]}')\n        print(f'public_key: {config[\"auth\"][\"public_key\"]}')\n        print(f'nonce: {config[\"auth\"][\"nonce\"]}')\n        print(f'message: {config[\"auth\"][\"message\"]}')\n        print(f'recipient: {config[\"auth\"][\"recipient\"]}')\n    else:\n        print(\"Near auth details not found\")\n</code></pre>"},{"location":"api/#nearai.login.print_url_message","title":"print_url_message","text":"<pre><code>print_url_message(url)\n</code></pre> <p>Prints a message instructing the user to visit the given URL to complete the login process.</p> Source code in <code>nearai/login.py</code> <pre><code>def print_url_message(url):\n    \"\"\"Prints a message instructing the user to visit the given URL to complete the login process.\"\"\"\n    print(f\"Please visit the following URL to complete the login process: {url}\")\n</code></pre>"},{"location":"api/#nearai.login.update_auth_config","title":"update_auth_config","text":"<pre><code>update_auth_config(account_id, signature, public_key, callback_url, nonce)\n</code></pre> <p>Update authentication configuration if the provided signature is valid.</p> Source code in <code>nearai/login.py</code> <pre><code>def update_auth_config(account_id, signature, public_key, callback_url, nonce):\n    \"\"\"Update authentication configuration if the provided signature is valid.\"\"\"\n    if near.verify_signed_message(\n        account_id,\n        public_key,\n        signature,\n        MESSAGE,\n        nonce,\n        RECIPIENT,\n        callback_url,\n    ):\n        config = load_config_file()\n\n        auth = AuthData.model_validate(\n            {\n                \"account_id\": account_id,\n                \"signature\": signature,\n                \"public_key\": public_key,\n                \"callback_url\": callback_url,\n                \"nonce\": nonce,\n                \"recipient\": RECIPIENT,\n                \"message\": MESSAGE,\n            }\n        )\n\n        config[\"auth\"] = auth.model_dump()\n        save_config_file(config)\n\n        print(f\"Auth data has been successfully saved! You are now logged in with account ID: {account_id}\")\n        return True\n    else:\n        print(\"Signature verification failed. Abort\")\n        return False\n</code></pre>"},{"location":"api/#nearai.model","title":"model","text":""},{"location":"api/#nearai.model.get_model","title":"get_model","text":"<pre><code>get_model(name: str) -&gt; Path\n</code></pre> <p>Download the model from the registry and download it locally if it hasn't been downloaded yet.</p> <p>:param name: The name of the entry to download the model. The format should be namespace/name/version. :return: The path to the downloaded model</p> Source code in <code>nearai/model.py</code> <pre><code>def get_model(name: str) -&gt; Path:\n    \"\"\"Download the model from the registry and download it locally if it hasn't been downloaded yet.\n\n    :param name: The name of the entry to download the model. The format should be namespace/name/version.\n    :return: The path to the downloaded model\n    \"\"\"\n    return registry.download(name)\n</code></pre>"},{"location":"api/#nearai.registry","title":"registry","text":""},{"location":"api/#nearai.registry.Registry","title":"Registry","text":"Source code in <code>nearai/registry.py</code> <pre><code>class Registry:\n    def __init__(self):\n        \"\"\"Create Registry object to interact with the registry programmatically.\"\"\"\n        self.download_folder = DATA_FOLDER / \"registry\"\n        self.api = RegistryApi()\n\n        if not self.download_folder.exists():\n            self.download_folder.mkdir(parents=True, exist_ok=True)\n\n    def update(self, entry_location: EntryLocation, metadata: EntryMetadataInput) -&gt; Dict[str, Any]:\n        \"\"\"Update metadata of a entry in the registry.\"\"\"\n        result = self.api.upload_metadata_v1_registry_upload_metadata_post(\n            BodyUploadMetadataV1RegistryUploadMetadataPost(metadata=metadata, entry_location=entry_location)\n        )\n        return result\n\n    def info(self, entry_location: EntryLocation) -&gt; Optional[EntryMetadata]:\n        \"\"\"Get metadata of a entry in the registry.\"\"\"\n        try:\n            return self.api.download_metadata_v1_registry_download_metadata_post(\n                BodyDownloadMetadataV1RegistryDownloadMetadataPost.from_dict(dict(entry_location=entry_location))\n            )\n        except NotFoundException:\n            return None\n\n    def upload_file(self, entry_location: EntryLocation, local_path: Path, path: Path) -&gt; bool:\n        \"\"\"Upload a file to the registry.\"\"\"\n        with open(local_path, \"rb\") as file:\n            data = file.read()\n\n            try:\n                self.api.upload_file_v1_registry_upload_file_post(\n                    path=str(path),\n                    file=data,\n                    namespace=entry_location.namespace,\n                    name=entry_location.name,\n                    version=entry_location.version,\n                )\n                return True\n            except BadRequestException as e:\n                if isinstance(e.body, str) and \"already exists\" in e.body:\n                    return False\n\n                raise e\n\n    def download_file(self, entry_location: EntryLocation, path: Path, local_path: Path):\n        \"\"\"Download a file from the registry.\"\"\"\n        result = self.api.download_file_v1_registry_download_file_post_without_preload_content(\n            BodyDownloadFileV1RegistryDownloadFilePost.from_dict(\n                dict(\n                    entry_location=entry_location,\n                    path=str(path),\n                )\n            )\n        )\n\n        local_path.parent.mkdir(parents=True, exist_ok=True)\n\n        with open(local_path, \"wb\") as f:\n            copyfileobj(result, f)\n\n    def download(\n        self,\n        entry_location: Union[str, EntryLocation],\n        force: bool = False,\n        show_progress: bool = False,\n        verbose: bool = True,\n    ) -&gt; Path:\n        \"\"\"Download entry from the registry locally.\"\"\"\n        if isinstance(entry_location, str):\n            entry_location = parse_location(entry_location)\n\n        download_path = get_registry_folder() / entry_location.namespace / entry_location.name / entry_location.version\n\n        if download_path.exists():\n            if not force:\n                if verbose:\n                    print(\n                        f\"Entry {entry_location} already exists at {download_path}. Use --force to overwrite the entry.\"\n                    )\n                return download_path\n\n        files = registry.list_files(entry_location)\n\n        download_path.mkdir(parents=True, exist_ok=True)\n\n        metadata = registry.info(entry_location)\n\n        if metadata is None:\n            raise ValueError(f\"Entry {entry_location} not found.\")\n\n        metadata_path = download_path / \"metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            f.write(metadata.model_dump_json(indent=2))\n\n        for file in (pbar := tqdm(files, disable=not show_progress)):\n            pbar.set_description(file)\n            registry.download_file(entry_location, file, download_path / file)\n\n        return download_path\n\n    def upload(\n        self,\n        local_path: Path,\n        metadata: Optional[EntryMetadata] = None,\n        show_progress: bool = False,\n    ) -&gt; EntryLocation:\n        \"\"\"Upload entry to the registry.\n\n        If metadata is provided it will overwrite the metadata in the directory,\n        otherwise it will use the metadata.json found on the root of the directory.\n        \"\"\"\n        path = Path(local_path).absolute()\n\n        if not path.exists():\n            # try path in local registry if original path not exists\n            path = get_registry_folder() / local_path\n\n        if CONFIG.auth is None:\n            print(\"Please login with `nearai login`\")\n            exit(1)\n\n        metadata_path = path / \"metadata.json\"\n\n        if metadata is not None:\n            with open(metadata_path, \"w\") as f:\n                f.write(metadata.model_dump_json(indent=2))\n\n        check_metadata(metadata_path)\n\n        with open(metadata_path) as f:\n            plain_metadata: Dict[str, Any] = json.load(f)\n\n        namespace = get_namespace(local_path)\n        name = plain_metadata.pop(\"name\")\n\n        entry_location = EntryLocation.model_validate(\n            dict(\n                namespace=namespace,\n                name=name,\n                version=plain_metadata.pop(\"version\"),\n            )\n        )\n\n        entry_metadata = EntryMetadataInput.model_validate(plain_metadata)\n        source = entry_metadata.details.get(\"_source\", None)\n\n        if source is not None:\n            print(f\"Only default source is allowed, found: {source}. Remove details._source from metadata.\")\n            exit(1)\n\n        if self.info(entry_location) is None:\n            # New entry location. Check for similar names in registry.\n            entries = self.list_all_visible()\n            canonical_namespace = get_canonical_name(namespace)\n            canonical_name = get_canonical_name(name)\n\n            for entry in entries:\n                if entry.name == name and entry.namespace == namespace:\n                    break\n                if (\n                    get_canonical_name(entry.name) == canonical_name\n                    and get_canonical_name(entry.namespace) == canonical_namespace\n                ):\n                    print(f\"A registry item with a similar name already exists: {entry.namespace}/{entry.name}\")\n                    exit(1)\n\n        registry.update(entry_location, entry_metadata)\n\n        all_files = []\n        total_size = 0\n\n        # Traverse all files in the directory `path`\n        for file in path.rglob(\"*\"):\n            if not file.is_file():\n                continue\n\n            relative = file.relative_to(path)\n\n            # Don't upload metadata file.\n            if file == metadata_path:\n                continue\n\n            # Don't upload backup files.\n            if file.name.endswith(\"~\"):\n                continue\n\n            # Don't upload configuration files.\n            if relative.parts[0] == \".nearai\":\n                continue\n\n            size = file.stat().st_size\n            total_size += size\n\n            all_files.append((file, relative, size))\n\n        pbar = tqdm(total=total_size, unit=\"B\", unit_scale=True, disable=not show_progress)\n        for file, relative, size in all_files:\n            if \"__pycache__\" in relative.parts:\n                continue\n            registry.upload_file(entry_location, file, relative)\n            pbar.update(size)\n\n        return entry_location\n\n    def list_files(self, entry_location: EntryLocation) -&gt; List[str]:\n        \"\"\"List files in from an entry in the registry.\n\n        Return the relative paths to all files with respect to the root of the entry.\n        \"\"\"\n        result = self.api.list_files_v1_registry_list_files_post(\n            BodyListFilesV1RegistryListFilesPost.from_dict(dict(entry_location=entry_location))\n        )\n        return [file.filename for file in result]\n\n    def list(\n        self,\n        namespace: str,\n        category: str,\n        tags: str,\n        total: int,\n        offset: int,\n        show_all: bool,\n        show_latest_version: bool,\n        starred_by: str = \"\",\n    ) -&gt; List[EntryInformation]:\n        \"\"\"List and filter entries in the registry.\"\"\"\n        return self.api.list_entries_v1_registry_list_entries_post(\n            namespace=namespace,\n            category=category,\n            tags=tags,\n            total=total,\n            offset=offset,\n            show_hidden=show_all,\n            show_latest_version=show_latest_version,\n            starred_by=starred_by,\n        )\n\n    def list_all_visible(self, category: str = \"\") -&gt; List[EntryInformation]:\n        \"\"\"List all visible entries.\"\"\"\n        total = 10000\n        entries = self.list(\n            namespace=\"\",\n            category=category,\n            tags=\"\",\n            total=total,\n            offset=0,\n            show_all=False,\n            show_latest_version=True,\n        )\n        assert len(entries) &lt; total\n        return entries\n\n    def dict_models(self) -&gt; Dict[NamespacedName, NamespacedName]:\n        \"\"\"Returns a mapping canonical-&gt;name.\"\"\"\n        entries = self.list_all_visible(category=\"model\")\n        result: Dict[NamespacedName, NamespacedName] = {}\n        for entry in entries:\n            namespaced_name = NamespacedName(name=entry.name, namespace=entry.namespace)\n            canonical_namespaced_name = namespaced_name.canonical()\n            if canonical_namespaced_name in result:\n                raise ValueError(\n                    f\"Duplicate registry entry for model {namespaced_name}, canonical {canonical_namespaced_name}\"\n                )\n            result[canonical_namespaced_name] = namespaced_name\n        return result\n</code></pre>"},{"location":"api/#nearai.registry.Registry.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Create Registry object to interact with the registry programmatically.</p> Source code in <code>nearai/registry.py</code> <pre><code>def __init__(self):\n    \"\"\"Create Registry object to interact with the registry programmatically.\"\"\"\n    self.download_folder = DATA_FOLDER / \"registry\"\n    self.api = RegistryApi()\n\n    if not self.download_folder.exists():\n        self.download_folder.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/#nearai.registry.Registry.dict_models","title":"dict_models","text":"<pre><code>dict_models() -&gt; Dict[NamespacedName, NamespacedName]\n</code></pre> <p>Returns a mapping canonical-&gt;name.</p> Source code in <code>nearai/registry.py</code> <pre><code>def dict_models(self) -&gt; Dict[NamespacedName, NamespacedName]:\n    \"\"\"Returns a mapping canonical-&gt;name.\"\"\"\n    entries = self.list_all_visible(category=\"model\")\n    result: Dict[NamespacedName, NamespacedName] = {}\n    for entry in entries:\n        namespaced_name = NamespacedName(name=entry.name, namespace=entry.namespace)\n        canonical_namespaced_name = namespaced_name.canonical()\n        if canonical_namespaced_name in result:\n            raise ValueError(\n                f\"Duplicate registry entry for model {namespaced_name}, canonical {canonical_namespaced_name}\"\n            )\n        result[canonical_namespaced_name] = namespaced_name\n    return result\n</code></pre>"},{"location":"api/#nearai.registry.Registry.download","title":"download","text":"<pre><code>download(entry_location: Union[str, EntryLocation], force: bool = False, show_progress: bool = False, verbose: bool = True) -&gt; Path\n</code></pre> <p>Download entry from the registry locally.</p> Source code in <code>nearai/registry.py</code> <pre><code>def download(\n    self,\n    entry_location: Union[str, EntryLocation],\n    force: bool = False,\n    show_progress: bool = False,\n    verbose: bool = True,\n) -&gt; Path:\n    \"\"\"Download entry from the registry locally.\"\"\"\n    if isinstance(entry_location, str):\n        entry_location = parse_location(entry_location)\n\n    download_path = get_registry_folder() / entry_location.namespace / entry_location.name / entry_location.version\n\n    if download_path.exists():\n        if not force:\n            if verbose:\n                print(\n                    f\"Entry {entry_location} already exists at {download_path}. Use --force to overwrite the entry.\"\n                )\n            return download_path\n\n    files = registry.list_files(entry_location)\n\n    download_path.mkdir(parents=True, exist_ok=True)\n\n    metadata = registry.info(entry_location)\n\n    if metadata is None:\n        raise ValueError(f\"Entry {entry_location} not found.\")\n\n    metadata_path = download_path / \"metadata.json\"\n    with open(metadata_path, \"w\") as f:\n        f.write(metadata.model_dump_json(indent=2))\n\n    for file in (pbar := tqdm(files, disable=not show_progress)):\n        pbar.set_description(file)\n        registry.download_file(entry_location, file, download_path / file)\n\n    return download_path\n</code></pre>"},{"location":"api/#nearai.registry.Registry.download_file","title":"download_file","text":"<pre><code>download_file(entry_location: EntryLocation, path: Path, local_path: Path)\n</code></pre> <p>Download a file from the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def download_file(self, entry_location: EntryLocation, path: Path, local_path: Path):\n    \"\"\"Download a file from the registry.\"\"\"\n    result = self.api.download_file_v1_registry_download_file_post_without_preload_content(\n        BodyDownloadFileV1RegistryDownloadFilePost.from_dict(\n            dict(\n                entry_location=entry_location,\n                path=str(path),\n            )\n        )\n    )\n\n    local_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(local_path, \"wb\") as f:\n        copyfileobj(result, f)\n</code></pre>"},{"location":"api/#nearai.registry.Registry.info","title":"info","text":"<pre><code>info(entry_location: EntryLocation) -&gt; Optional[EntryMetadata]\n</code></pre> <p>Get metadata of a entry in the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def info(self, entry_location: EntryLocation) -&gt; Optional[EntryMetadata]:\n    \"\"\"Get metadata of a entry in the registry.\"\"\"\n    try:\n        return self.api.download_metadata_v1_registry_download_metadata_post(\n            BodyDownloadMetadataV1RegistryDownloadMetadataPost.from_dict(dict(entry_location=entry_location))\n        )\n    except NotFoundException:\n        return None\n</code></pre>"},{"location":"api/#nearai.registry.Registry.list","title":"list","text":"<pre><code>list(namespace: str, category: str, tags: str, total: int, offset: int, show_all: bool, show_latest_version: bool, starred_by: str = '') -&gt; List[EntryInformation]\n</code></pre> <p>List and filter entries in the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def list(\n    self,\n    namespace: str,\n    category: str,\n    tags: str,\n    total: int,\n    offset: int,\n    show_all: bool,\n    show_latest_version: bool,\n    starred_by: str = \"\",\n) -&gt; List[EntryInformation]:\n    \"\"\"List and filter entries in the registry.\"\"\"\n    return self.api.list_entries_v1_registry_list_entries_post(\n        namespace=namespace,\n        category=category,\n        tags=tags,\n        total=total,\n        offset=offset,\n        show_hidden=show_all,\n        show_latest_version=show_latest_version,\n        starred_by=starred_by,\n    )\n</code></pre>"},{"location":"api/#nearai.registry.Registry.list_all_visible","title":"list_all_visible","text":"<pre><code>list_all_visible(category: str = '') -&gt; List[EntryInformation]\n</code></pre> <p>List all visible entries.</p> Source code in <code>nearai/registry.py</code> <pre><code>def list_all_visible(self, category: str = \"\") -&gt; List[EntryInformation]:\n    \"\"\"List all visible entries.\"\"\"\n    total = 10000\n    entries = self.list(\n        namespace=\"\",\n        category=category,\n        tags=\"\",\n        total=total,\n        offset=0,\n        show_all=False,\n        show_latest_version=True,\n    )\n    assert len(entries) &lt; total\n    return entries\n</code></pre>"},{"location":"api/#nearai.registry.Registry.list_files","title":"list_files","text":"<pre><code>list_files(entry_location: EntryLocation) -&gt; List[str]\n</code></pre> <p>List files in from an entry in the registry.</p> <p>Return the relative paths to all files with respect to the root of the entry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def list_files(self, entry_location: EntryLocation) -&gt; List[str]:\n    \"\"\"List files in from an entry in the registry.\n\n    Return the relative paths to all files with respect to the root of the entry.\n    \"\"\"\n    result = self.api.list_files_v1_registry_list_files_post(\n        BodyListFilesV1RegistryListFilesPost.from_dict(dict(entry_location=entry_location))\n    )\n    return [file.filename for file in result]\n</code></pre>"},{"location":"api/#nearai.registry.Registry.update","title":"update","text":"<pre><code>update(entry_location: EntryLocation, metadata: EntryMetadataInput) -&gt; Dict[str, Any]\n</code></pre> <p>Update metadata of a entry in the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def update(self, entry_location: EntryLocation, metadata: EntryMetadataInput) -&gt; Dict[str, Any]:\n    \"\"\"Update metadata of a entry in the registry.\"\"\"\n    result = self.api.upload_metadata_v1_registry_upload_metadata_post(\n        BodyUploadMetadataV1RegistryUploadMetadataPost(metadata=metadata, entry_location=entry_location)\n    )\n    return result\n</code></pre>"},{"location":"api/#nearai.registry.Registry.upload","title":"upload","text":"<pre><code>upload(local_path: Path, metadata: Optional[EntryMetadata] = None, show_progress: bool = False) -&gt; EntryLocation\n</code></pre> <p>Upload entry to the registry.</p> <p>If metadata is provided it will overwrite the metadata in the directory, otherwise it will use the metadata.json found on the root of the directory.</p> Source code in <code>nearai/registry.py</code> <pre><code>def upload(\n    self,\n    local_path: Path,\n    metadata: Optional[EntryMetadata] = None,\n    show_progress: bool = False,\n) -&gt; EntryLocation:\n    \"\"\"Upload entry to the registry.\n\n    If metadata is provided it will overwrite the metadata in the directory,\n    otherwise it will use the metadata.json found on the root of the directory.\n    \"\"\"\n    path = Path(local_path).absolute()\n\n    if not path.exists():\n        # try path in local registry if original path not exists\n        path = get_registry_folder() / local_path\n\n    if CONFIG.auth is None:\n        print(\"Please login with `nearai login`\")\n        exit(1)\n\n    metadata_path = path / \"metadata.json\"\n\n    if metadata is not None:\n        with open(metadata_path, \"w\") as f:\n            f.write(metadata.model_dump_json(indent=2))\n\n    check_metadata(metadata_path)\n\n    with open(metadata_path) as f:\n        plain_metadata: Dict[str, Any] = json.load(f)\n\n    namespace = get_namespace(local_path)\n    name = plain_metadata.pop(\"name\")\n\n    entry_location = EntryLocation.model_validate(\n        dict(\n            namespace=namespace,\n            name=name,\n            version=plain_metadata.pop(\"version\"),\n        )\n    )\n\n    entry_metadata = EntryMetadataInput.model_validate(plain_metadata)\n    source = entry_metadata.details.get(\"_source\", None)\n\n    if source is not None:\n        print(f\"Only default source is allowed, found: {source}. Remove details._source from metadata.\")\n        exit(1)\n\n    if self.info(entry_location) is None:\n        # New entry location. Check for similar names in registry.\n        entries = self.list_all_visible()\n        canonical_namespace = get_canonical_name(namespace)\n        canonical_name = get_canonical_name(name)\n\n        for entry in entries:\n            if entry.name == name and entry.namespace == namespace:\n                break\n            if (\n                get_canonical_name(entry.name) == canonical_name\n                and get_canonical_name(entry.namespace) == canonical_namespace\n            ):\n                print(f\"A registry item with a similar name already exists: {entry.namespace}/{entry.name}\")\n                exit(1)\n\n    registry.update(entry_location, entry_metadata)\n\n    all_files = []\n    total_size = 0\n\n    # Traverse all files in the directory `path`\n    for file in path.rglob(\"*\"):\n        if not file.is_file():\n            continue\n\n        relative = file.relative_to(path)\n\n        # Don't upload metadata file.\n        if file == metadata_path:\n            continue\n\n        # Don't upload backup files.\n        if file.name.endswith(\"~\"):\n            continue\n\n        # Don't upload configuration files.\n        if relative.parts[0] == \".nearai\":\n            continue\n\n        size = file.stat().st_size\n        total_size += size\n\n        all_files.append((file, relative, size))\n\n    pbar = tqdm(total=total_size, unit=\"B\", unit_scale=True, disable=not show_progress)\n    for file, relative, size in all_files:\n        if \"__pycache__\" in relative.parts:\n            continue\n        registry.upload_file(entry_location, file, relative)\n        pbar.update(size)\n\n    return entry_location\n</code></pre>"},{"location":"api/#nearai.registry.Registry.upload_file","title":"upload_file","text":"<pre><code>upload_file(entry_location: EntryLocation, local_path: Path, path: Path) -&gt; bool\n</code></pre> <p>Upload a file to the registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def upload_file(self, entry_location: EntryLocation, local_path: Path, path: Path) -&gt; bool:\n    \"\"\"Upload a file to the registry.\"\"\"\n    with open(local_path, \"rb\") as file:\n        data = file.read()\n\n        try:\n            self.api.upload_file_v1_registry_upload_file_post(\n                path=str(path),\n                file=data,\n                namespace=entry_location.namespace,\n                name=entry_location.name,\n                version=entry_location.version,\n            )\n            return True\n        except BadRequestException as e:\n            if isinstance(e.body, str) and \"already exists\" in e.body:\n                return False\n\n            raise e\n</code></pre>"},{"location":"api/#nearai.registry.get_namespace","title":"get_namespace","text":"<pre><code>get_namespace(local_path: Path) -&gt; str\n</code></pre> <p>Returns namespace of an item or user namespace.</p> Source code in <code>nearai/registry.py</code> <pre><code>def get_namespace(local_path: Path) -&gt; str:\n    \"\"\"Returns namespace of an item or user namespace.\"\"\"\n    registry_folder = get_registry_folder()\n\n    try:\n        # Check if the path matches the expected structure\n        relative_path = local_path.relative_to(registry_folder)\n        parts = relative_path.parts\n\n        # If the path has 3 parts (namespace, item_name, version),\n        # return the first part as the namespace\n        if len(parts) == 3:\n            return str(parts[0])\n    except ValueError:\n        # relative_to() raises ValueError if local_path is not relative to registry_folder\n        pass\n\n    # If we couldn't extract a namespace from the path, return the default\n    if CONFIG.auth is None:\n        raise ValueError(\"AuthData is None\")\n    return CONFIG.auth.namespace\n</code></pre>"},{"location":"api/#nearai.registry.get_registry_folder","title":"get_registry_folder","text":"<pre><code>get_registry_folder() -&gt; Path\n</code></pre> <p>Path to local registry.</p> Source code in <code>nearai/registry.py</code> <pre><code>def get_registry_folder() -&gt; Path:\n    \"\"\"Path to local registry.\"\"\"\n    return DATA_FOLDER / REGISTRY_FOLDER\n</code></pre>"},{"location":"api/#nearai.shared","title":"shared","text":""},{"location":"api/#nearai.shared.auth_data","title":"auth_data","text":""},{"location":"api/#nearai.shared.auth_data.AuthData","title":"AuthData","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>nearai/shared/auth_data.py</code> <pre><code>class AuthData(BaseModel):\n    account_id: str\n    signature: str\n    public_key: str\n    callback_url: str\n    nonce: str\n    recipient: str\n    message: str\n    on_behalf_of: Optional[str] = None\n\n    def generate_bearer_token(self):\n        \"\"\"Generates a JSON-encoded bearer token containing authentication data.\"\"\"\n        required_keys = {\"account_id\", \"public_key\", \"signature\", \"callback_url\", \"message\", \"nonce\", \"recipient\"}\n\n        for key in required_keys:\n            if getattr(self, key) is None:\n                raise ValueError(f\"Missing required auth data: {key}\")\n\n        if self.on_behalf_of is not None:\n            required_keys.add(\"on_behalf_of\")\n\n        bearer_data = {key: getattr(self, key) for key in required_keys}\n\n        return json.dumps(bearer_data)\n\n    @property\n    def namespace(self):\n        \"\"\"Get the account ID for the auth data.\n\n        In case you are running a request on behalf of another account, this will return the account ID of the account.\n        \"\"\"\n        if self.on_behalf_of is not None:\n            return self.on_behalf_of\n        return self.account_id\n</code></pre>"},{"location":"api/#nearai.shared.auth_data.AuthData.namespace","title":"namespace  <code>property</code>","text":"<pre><code>namespace\n</code></pre> <p>Get the account ID for the auth data.</p> <p>In case you are running a request on behalf of another account, this will return the account ID of the account.</p>"},{"location":"api/#nearai.shared.auth_data.AuthData.generate_bearer_token","title":"generate_bearer_token","text":"<pre><code>generate_bearer_token()\n</code></pre> <p>Generates a JSON-encoded bearer token containing authentication data.</p> Source code in <code>nearai/shared/auth_data.py</code> <pre><code>def generate_bearer_token(self):\n    \"\"\"Generates a JSON-encoded bearer token containing authentication data.\"\"\"\n    required_keys = {\"account_id\", \"public_key\", \"signature\", \"callback_url\", \"message\", \"nonce\", \"recipient\"}\n\n    for key in required_keys:\n        if getattr(self, key) is None:\n            raise ValueError(f\"Missing required auth data: {key}\")\n\n    if self.on_behalf_of is not None:\n        required_keys.add(\"on_behalf_of\")\n\n    bearer_data = {key: getattr(self, key) for key in required_keys}\n\n    return json.dumps(bearer_data)\n</code></pre>"},{"location":"api/#nearai.shared.cache","title":"cache","text":""},{"location":"api/#nearai.shared.cache.mem_cache_with_timeout","title":"mem_cache_with_timeout","text":"<pre><code>mem_cache_with_timeout(timeout: int)\n</code></pre> <p>Decorator to cache function results for a specified timeout period.</p> Source code in <code>nearai/shared/cache.py</code> <pre><code>def mem_cache_with_timeout(timeout: int):\n    \"\"\"Decorator to cache function results for a specified timeout period.\"\"\"\n\n    def decorator(func):\n        cache = {}\n\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            now = time.time()\n            key = (args, frozenset(kwargs.items()))\n            if key in cache:\n                result, timestamp = cache[key]\n                if now - timestamp &lt; timeout:\n                    return result\n            result = func(*args, **kwargs)\n            cache[key] = (result, now)\n            return result\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/#nearai.shared.client_config","title":"client_config","text":""},{"location":"api/#nearai.shared.client_config.ClientConfig","title":"ClientConfig","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>nearai/shared/client_config.py</code> <pre><code>class ClientConfig(BaseModel):\n    base_url: str = \"https://api.near.ai/v1\"\n    custom_llm_provider: str = \"openai\"\n    auth: Optional[AuthData] = None\n    default_provider: Optional[str] = None  # future: remove in favor of api decision\n    num_inference_retries: int = 1\n\n    def get_hub_client(self):\n        \"\"\"Get the hub client.\"\"\"\n        signature = f\"Bearer {self.auth.model_dump_json()}\"\n        base_url = self.base_url\n        return openai.OpenAI(\n            base_url=base_url, api_key=signature, timeout=DEFAULT_TIMEOUT, max_retries=DEFAULT_MAX_RETRIES\n        )\n</code></pre>"},{"location":"api/#nearai.shared.client_config.ClientConfig.get_hub_client","title":"get_hub_client","text":"<pre><code>get_hub_client()\n</code></pre> <p>Get the hub client.</p> Source code in <code>nearai/shared/client_config.py</code> <pre><code>def get_hub_client(self):\n    \"\"\"Get the hub client.\"\"\"\n    signature = f\"Bearer {self.auth.model_dump_json()}\"\n    base_url = self.base_url\n    return openai.OpenAI(\n        base_url=base_url, api_key=signature, timeout=DEFAULT_TIMEOUT, max_retries=DEFAULT_MAX_RETRIES\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client","title":"inference_client","text":""},{"location":"api/#nearai.shared.inference_client.InferenceClient","title":"InferenceClient","text":"<p>               Bases: <code>object</code></p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>class InferenceClient(object):\n    def __init__(self, config: ClientConfig, runner_api_key: str = \"\", agent_identifier: str = \"\") -&gt; None:  # noqa: D107\n        self._config = config\n        self.runner_api_key = runner_api_key\n        self.agent_identifier = agent_identifier\n        self._auth = None\n        self.generate_auth_for_current_agent(config, agent_identifier)\n        self.client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        self._provider_models: Optional[ProviderModels] = None\n\n    def generate_auth_for_current_agent(self, config, agent_identifier):\n        \"\"\"Regenerate auth for the current agent.\"\"\"\n        self.agent_identifier = agent_identifier\n        if config.auth is not None:\n            auth_bearer_token = config.auth.generate_bearer_token()\n            new_token = json.loads(auth_bearer_token)\n            new_token[\"runner_data\"] = json.dumps({\"agent\": agent_identifier, \"runner_api_key\": self.runner_api_key})\n            auth_bearer_token = json.dumps(new_token)\n            self._auth = auth_bearer_token\n        else:\n            self._auth = None\n\n    # This makes sense in the CLI where we don't mind doing this request and caching it.\n    # In the aws_runner this is an extra request every time we run.\n    # TODO(#233): add a choice of a provider model in aws_runner, and then this step can be skipped.\n    @cached_property\n    def provider_models(self) -&gt; ProviderModels:  # noqa: D102\n        if self._provider_models is None:\n            self._provider_models = ProviderModels(self._config)\n        return self._provider_models\n\n    def set_provider_models(self, provider_models: Optional[ProviderModels]):\n        \"\"\"Set provider models. Used by external caching.\"\"\"\n        if provider_models is None:\n            self._provider_models = ProviderModels(self._config)\n        else:\n            self._provider_models = provider_models\n\n    def get_agent_public_key(self, agent_name: str) -&gt; str:\n        \"\"\"Request agent public key.\"\"\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n        }\n\n        data = {\"agent_name\": agent_name}\n\n        endpoint = f\"{self._config.base_url}/get_agent_public_key\"\n\n        try:\n            response = requests.post(endpoint, headers=headers, params=data)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            raise ValueError(f\"Failed to get agent public key: {e}\") from None\n\n    def completions(\n        self,\n        model: str,\n        messages: Iterable[ChatCompletionMessageParam],\n        stream: bool = False,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        **kwargs: Any,\n    ) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"Takes a `model` and `messages` and returns completions.\n\n        `model` can be:\n        1. full path `provider::model_full_path`.\n        2. `model_short_name`. Default provider will be used.\n        \"\"\"\n        provider, model = self.provider_models.match_provider_model(model)\n\n        if temperature is None:\n            temperature = DEFAULT_MODEL_TEMPERATURE\n\n        if max_tokens is None:\n            max_tokens = DEFAULT_MODEL_MAX_TOKENS\n\n        # NOTE(#246): this is to disable \"Provider List\" messages.\n        litellm.suppress_debug_info = True\n\n        # lite_llm uses the openai.request_timeout to set the timeout for the request of \"openai\" custom provider\n        openai.timeout = DEFAULT_TIMEOUT\n        openai.max_retries = DEFAULT_MAX_RETRIES\n\n        for i in range(0, self._config.num_inference_retries):\n            try:\n                result: Union[ModelResponse, CustomStreamWrapper] = litellm_completion(\n                    model,\n                    messages,\n                    stream=stream,\n                    custom_llm_provider=self._config.custom_llm_provider,\n                    input_cost_per_token=0,\n                    output_cost_per_token=0,\n                    temperature=temperature,\n                    max_tokens=max_tokens,\n                    base_url=self._config.base_url,\n                    provider=provider,\n                    api_key=self._auth,\n                    timeout=DEFAULT_TIMEOUT,\n                    request_timeout=DEFAULT_TIMEOUT,\n                    num_retries=1,\n                    **kwargs,\n                )\n                break\n            except Exception as e:\n                print(\"Completions exception:\", e)\n                if i == self._config.num_inference_retries - 1:\n                    raise ValueError(f\"Bad request: {e}\") from None\n                else:\n                    print(\"Retrying...\")\n\n        return result\n\n    def query_vector_store(\n        self, vector_store_id: str, query: str, full_files: bool = False\n    ) -&gt; Union[List[SimilaritySearch], List[SimilaritySearchFile]]:\n        \"\"\"Query a vector store.\"\"\"\n        if self._config is None:\n            raise ValueError(\"Missing NEAR AI Hub config\")\n\n        auth_bearer_token = self._auth\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {auth_bearer_token}\",\n        }\n\n        data = {\"query\": query, \"full_files\": full_files}\n\n        endpoint = f\"{self._config.base_url}/vector_stores/{vector_store_id}/search\"\n\n        try:\n            response = requests.post(endpoint, headers=headers, json=data)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            raise ValueError(f\"Error querying vector store: {e}\") from None\n\n    def upload_file(\n        self,\n        file_content: str,\n        purpose: Literal[\"assistants\", \"batch\", \"fine-tune\", \"vision\"],\n        encoding: Optional[str] = \"utf-8\",\n        file_name: Optional[str] = \"file.txt\",\n        file_type: Optional[str] = \"text/plain\",\n    ) -&gt; Optional[FileObject]:\n        \"\"\"Uploads a file.\"\"\"\n        client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        if file_content:\n            file_data = io.BytesIO(file_content.encode(encoding or \"utf-8\"))\n            return client.files.create(file=(file_name, file_data, file_type), purpose=purpose)\n        else:\n            return None\n\n    def remove_file(self, file_id: str):\n        \"\"\"Removes a file.\"\"\"\n        client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        return client.files.delete(file_id=file_id)\n\n    def add_file_to_vector_store(self, vector_store_id: str, file_id: str) -&gt; VectorStoreFile:\n        \"\"\"Adds a file to vector store.\"\"\"\n        client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        return client.beta.vector_stores.files.create(vector_store_id=vector_store_id, file_id=file_id)\n\n    def get_vector_store_files(self, vector_store_id: str) -&gt; Optional[List[VectorStoreFile]]:\n        \"\"\"Adds a file to vector store.\"\"\"\n        client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        return client.beta.vector_stores.files.list(vector_store_id=vector_store_id).data\n\n    def create_vector_store_from_source(\n        self,\n        name: str,\n        source: Union[GitHubSource, GitLabSource],\n        source_auth: Optional[str] = None,\n        chunking_strategy: Optional[ChunkingStrategy] = None,\n        expires_after: Optional[ExpiresAfter] = None,\n        metadata: Optional[Dict[str, str]] = None,\n    ) -&gt; VectorStore:\n        \"\"\"Creates a vector store from the given source.\n\n        Args:\n        ----\n            name (str): The name of the vector store.\n            source (Union[GitHubSource, GitLabSource]): The source from which to create the vector store.\n            source_auth (Optional[str]): The source authentication token.\n            chunking_strategy (Optional[ChunkingStrategy]): The chunking strategy to use.\n            expires_after (Optional[ExpiresAfter]): The expiration policy.\n            metadata (Optional[Dict[str, str]]): Additional metadata.\n\n        Returns:\n        -------\n            VectorStore: The created vector store.\n\n        \"\"\"\n        print(f\"Creating vector store from source: {source}\")\n        headers = {\n            \"Authorization\": f\"Bearer {self._auth}\",\n            \"Content-Type\": \"application/json\",\n        }\n        data = {\n            \"name\": name,\n            \"source\": source,\n            \"source_auth\": source_auth,\n            \"chunking_strategy\": chunking_strategy,\n            \"expires_after\": expires_after,\n            \"metadata\": metadata,\n        }\n        endpoint = f\"{self._config.base_url}/vector_stores/from_source\"\n\n        try:\n            response = requests.post(endpoint, headers=headers, json=data)\n            print(response.json())\n            response.raise_for_status()\n            return VectorStore(**response.json())\n        except requests.RequestException as e:\n            raise ValueError(f\"Failed to create vector store: {e}\") from None\n\n    def create_vector_store(\n        self,\n        name: str,\n        file_ids: List[str],\n        expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN,\n        chunking_strategy: Union[\n            AutoFileChunkingStrategyParam, StaticFileChunkingStrategyObjectParam, NotGiven\n        ] = NOT_GIVEN,\n        metadata: Optional[Dict[str, str]] = None,\n    ) -&gt; VectorStore:\n        \"\"\"Creates Vector Store.\n\n        :param name: Vector store name.\n        :param file_ids: Files to be added to the vector store.\n        :param expires_after: Expiration policy.\n        :param chunking_strategy: Chunking strategy.\n        :param metadata: Additional metadata.\n        :return: Returns the created vector store or error.\n        \"\"\"\n        client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n        return client.beta.vector_stores.create(\n            file_ids=file_ids,\n            name=name,\n            expires_after=expires_after,\n            chunking_strategy=chunking_strategy,\n            metadata=metadata,\n        )\n\n    def get_vector_store(self, vector_store_id: str) -&gt; VectorStore:\n        \"\"\"Gets a vector store by id.\"\"\"\n        endpoint = f\"{self._config.base_url}/vector_stores/{vector_store_id}\"\n        auth_bearer_token = self._auth\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {auth_bearer_token}\",\n        }\n\n        response = requests.get(endpoint, headers=headers)\n        response.raise_for_status()\n        return VectorStore(**response.json())\n\n    def create_thread(self, messages):\n        \"\"\"Create a thread.\"\"\"\n        return self.client.beta.threads.create(messages=messages)\n\n    def get_thread(self, thread_id: str):\n        \"\"\"Get a thread.\"\"\"\n        return self.client.beta.threads.retrieve(thread_id=thread_id)\n\n    def threads_messages_create(self, thread_id: str, content: str, role: Literal[\"user\", \"assistant\"]):\n        \"\"\"Create a message in a thread.\"\"\"\n        return self.client.beta.threads.messages.create(thread_id=thread_id, content=content, role=role)\n\n    def threads_create_and_run_poll(self, assistant_id: str, model: str, messages: List[ChatCompletionMessageParam]):\n        \"\"\"Create a thread and run the assistant.\"\"\"\n        thread = self.create_thread(messages)\n        return self.client.beta.threads.create_and_run_poll(thread=thread, assistant_id=assistant_id, model=model)\n\n    def threads_list_messages(self, thread_id: str, order: Literal[\"asc\", \"desc\"] = \"asc\"):\n        \"\"\"List messages in a thread.\"\"\"\n        return self.client.beta.threads.messages.list(thread_id=thread_id, order=order)\n\n    def threads_fork(self, thread_id: str):\n        \"\"\"Fork a thread.\"\"\"\n        forked_thread = self.client.post(path=f\"{self._config.base_url}/threads/{thread_id}/fork\", cast_to=Thread)\n        return forked_thread\n\n    def create_subthread(\n        self,\n        thread_id: str,\n        messages_to_copy: Optional[List[str]] = None,\n        new_messages: Optional[List[ChatCompletionMessageParam]] = None,\n    ):\n        \"\"\"Create a subthread.\"\"\"\n        return self.client.post(\n            path=f\"{self._config.base_url}/threads/{thread_id}/subthread\",\n            body={messages_to_copy: messages_to_copy, new_messages: new_messages},\n            cast_to=Thread,\n        )\n\n    def threads_runs_create(self, thread_id: str, assistant_id: str, model: str):\n        \"\"\"Create a run in a thread.\"\"\"\n        return self.client.beta.threads.runs.create(thread_id=thread_id, assistant_id=assistant_id, model=model)\n\n    def run_agent(\n        self, run_on_thread_id: str, assistant_id: str, parent_run_id: str = \"\", run_mode: RunMode = RunMode.SIMPLE\n    ):\n        \"\"\"Starts a child agent run from a parent agent run.\"\"\"\n        extra_body = {}\n        if parent_run_id:\n            extra_body[\"parent_run_id\"] = parent_run_id\n        extra_body[\"run_mode\"] = run_mode.value  # type: ignore\n        return self.client.beta.threads.runs.create(\n            thread_id=run_on_thread_id,\n            assistant_id=assistant_id,\n            extra_body=extra_body,\n        )\n\n    def schedule_run(\n        self,\n        agent: str,\n        input_message: str,\n        thread_id: Optional[str],\n        run_params: Optional[Dict[str, str]],\n        run_at: datetime,\n    ):\n        \"\"\"Query a vector store.\"\"\"\n        if self._config is None:\n            raise ValueError(\"Missing NearAI Hub config\")\n\n        auth_bearer_token = self._auth\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {auth_bearer_token}\",\n        }\n\n        if run_params is None:\n            run_params = {}\n\n        data = {\n            \"agent\": agent,\n            \"input_message\": input_message,\n            \"thread_id\": thread_id,\n            \"run_params\": run_params,\n            \"run_at\": run_at,\n        }\n\n        endpoint = f\"{self._config.base_url}/schedule_run\"\n\n        try:\n            response = requests.post(endpoint, headers=headers, json=data)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            raise ValueError(f\"Error querying schedule_run: {e}\") from None\n\n    def query_user_memory(self, query: str):\n        \"\"\"Query the user memory.\"\"\"\n        return self.client.post(\n            path=f\"{self._config.base_url}/vector_stores/memory/query\",\n            body={\"query\": query},\n            cast_to=str,\n        )\n\n    def add_user_memory(self, memory: str):\n        \"\"\"Add user memory.\"\"\"\n        return self.client.post(\n            path=f\"{self._config.base_url}/vector_stores/memory\",\n            body={\"memory\": memory},\n            cast_to=str,\n        )\n\n    def generate_image(self, prompt: str):\n        \"\"\"Generate an image.\"\"\"\n        return self.client.images.generate(prompt=prompt)\n\n    def save_agent_data(self, key: str, agent_data: Dict[str, Any]):\n        \"\"\"Save agent data for the agent this client was initialized with.\"\"\"\n        return self.client.post(\n            path=f\"{self._config.base_url}/agent_data\",\n            body={\n                \"key\": key,\n                \"value\": agent_data,\n            },\n            cast_to=Dict[str, Any],\n        )\n\n    def get_agent_data(self):\n        \"\"\"Get agent data for the agent this client was initialized with.\"\"\"\n        return self.client.get(\n            path=f\"{self._config.base_url}/agent_data\",\n            cast_to=Dict[str, str],\n        )\n\n    def get_agent_data_by_key(self, key: str):\n        \"\"\"Get agent data by key for the agent this client was initialized with.\"\"\"\n        return self.client.get(\n            path=f\"{self._config.base_url}/agent_data/{key}\",\n            cast_to=Dict[str, str],\n        )\n\n    def find_agents(self, owner_id: Optional[str] = None, with_capabilities: Optional[bool] = False):\n        \"\"\"Filter agents.\"\"\"\n        return self.client.post(\n            path=f\"{self._config.base_url}/find_agents\",\n            body={\"owner_id\": owner_id, \"with_capabilities\": with_capabilities},\n            cast_to=List[Any],\n        )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.add_file_to_vector_store","title":"add_file_to_vector_store","text":"<pre><code>add_file_to_vector_store(vector_store_id: str, file_id: str) -&gt; VectorStoreFile\n</code></pre> <p>Adds a file to vector store.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def add_file_to_vector_store(self, vector_store_id: str, file_id: str) -&gt; VectorStoreFile:\n    \"\"\"Adds a file to vector store.\"\"\"\n    client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n    return client.beta.vector_stores.files.create(vector_store_id=vector_store_id, file_id=file_id)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.add_user_memory","title":"add_user_memory","text":"<pre><code>add_user_memory(memory: str)\n</code></pre> <p>Add user memory.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def add_user_memory(self, memory: str):\n    \"\"\"Add user memory.\"\"\"\n    return self.client.post(\n        path=f\"{self._config.base_url}/vector_stores/memory\",\n        body={\"memory\": memory},\n        cast_to=str,\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.completions","title":"completions","text":"<pre><code>completions(model: str, messages: Iterable[ChatCompletionMessageParam], stream: bool = False, temperature: Optional[float] = None, max_tokens: Optional[int] = None, **kwargs: Any) -&gt; Union[ModelResponse, CustomStreamWrapper]\n</code></pre> <p>Takes a <code>model</code> and <code>messages</code> and returns completions.</p> <p><code>model</code> can be: 1. full path <code>provider::model_full_path</code>. 2. <code>model_short_name</code>. Default provider will be used.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def completions(\n    self,\n    model: str,\n    messages: Iterable[ChatCompletionMessageParam],\n    stream: bool = False,\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; Union[ModelResponse, CustomStreamWrapper]:\n    \"\"\"Takes a `model` and `messages` and returns completions.\n\n    `model` can be:\n    1. full path `provider::model_full_path`.\n    2. `model_short_name`. Default provider will be used.\n    \"\"\"\n    provider, model = self.provider_models.match_provider_model(model)\n\n    if temperature is None:\n        temperature = DEFAULT_MODEL_TEMPERATURE\n\n    if max_tokens is None:\n        max_tokens = DEFAULT_MODEL_MAX_TOKENS\n\n    # NOTE(#246): this is to disable \"Provider List\" messages.\n    litellm.suppress_debug_info = True\n\n    # lite_llm uses the openai.request_timeout to set the timeout for the request of \"openai\" custom provider\n    openai.timeout = DEFAULT_TIMEOUT\n    openai.max_retries = DEFAULT_MAX_RETRIES\n\n    for i in range(0, self._config.num_inference_retries):\n        try:\n            result: Union[ModelResponse, CustomStreamWrapper] = litellm_completion(\n                model,\n                messages,\n                stream=stream,\n                custom_llm_provider=self._config.custom_llm_provider,\n                input_cost_per_token=0,\n                output_cost_per_token=0,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                base_url=self._config.base_url,\n                provider=provider,\n                api_key=self._auth,\n                timeout=DEFAULT_TIMEOUT,\n                request_timeout=DEFAULT_TIMEOUT,\n                num_retries=1,\n                **kwargs,\n            )\n            break\n        except Exception as e:\n            print(\"Completions exception:\", e)\n            if i == self._config.num_inference_retries - 1:\n                raise ValueError(f\"Bad request: {e}\") from None\n            else:\n                print(\"Retrying...\")\n\n    return result\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.create_subthread","title":"create_subthread","text":"<pre><code>create_subthread(thread_id: str, messages_to_copy: Optional[List[str]] = None, new_messages: Optional[List[ChatCompletionMessageParam]] = None)\n</code></pre> <p>Create a subthread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def create_subthread(\n    self,\n    thread_id: str,\n    messages_to_copy: Optional[List[str]] = None,\n    new_messages: Optional[List[ChatCompletionMessageParam]] = None,\n):\n    \"\"\"Create a subthread.\"\"\"\n    return self.client.post(\n        path=f\"{self._config.base_url}/threads/{thread_id}/subthread\",\n        body={messages_to_copy: messages_to_copy, new_messages: new_messages},\n        cast_to=Thread,\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.create_thread","title":"create_thread","text":"<pre><code>create_thread(messages)\n</code></pre> <p>Create a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def create_thread(self, messages):\n    \"\"\"Create a thread.\"\"\"\n    return self.client.beta.threads.create(messages=messages)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.create_vector_store","title":"create_vector_store","text":"<pre><code>create_vector_store(name: str, file_ids: List[str], expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN, chunking_strategy: Union[AutoFileChunkingStrategyParam, StaticFileChunkingStrategyObjectParam, NotGiven] = NOT_GIVEN, metadata: Optional[Dict[str, str]] = None) -&gt; VectorStore\n</code></pre> <p>Creates Vector Store.</p> <p>:param name: Vector store name. :param file_ids: Files to be added to the vector store. :param expires_after: Expiration policy. :param chunking_strategy: Chunking strategy. :param metadata: Additional metadata. :return: Returns the created vector store or error.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def create_vector_store(\n    self,\n    name: str,\n    file_ids: List[str],\n    expires_after: Union[ExpiresAfter, NotGiven] = NOT_GIVEN,\n    chunking_strategy: Union[\n        AutoFileChunkingStrategyParam, StaticFileChunkingStrategyObjectParam, NotGiven\n    ] = NOT_GIVEN,\n    metadata: Optional[Dict[str, str]] = None,\n) -&gt; VectorStore:\n    \"\"\"Creates Vector Store.\n\n    :param name: Vector store name.\n    :param file_ids: Files to be added to the vector store.\n    :param expires_after: Expiration policy.\n    :param chunking_strategy: Chunking strategy.\n    :param metadata: Additional metadata.\n    :return: Returns the created vector store or error.\n    \"\"\"\n    client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n    return client.beta.vector_stores.create(\n        file_ids=file_ids,\n        name=name,\n        expires_after=expires_after,\n        chunking_strategy=chunking_strategy,\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.create_vector_store_from_source","title":"create_vector_store_from_source","text":"<pre><code>create_vector_store_from_source(name: str, source: Union[GitHubSource, GitLabSource], source_auth: Optional[str] = None, chunking_strategy: Optional[ChunkingStrategy] = None, expires_after: Optional[ExpiresAfter] = None, metadata: Optional[Dict[str, str]] = None) -&gt; VectorStore\n</code></pre> <p>Creates a vector store from the given source.</p> <pre><code>name (str): The name of the vector store.\nsource (Union[GitHubSource, GitLabSource]): The source from which to create the vector store.\nsource_auth (Optional[str]): The source authentication token.\nchunking_strategy (Optional[ChunkingStrategy]): The chunking strategy to use.\nexpires_after (Optional[ExpiresAfter]): The expiration policy.\nmetadata (Optional[Dict[str, str]]): Additional metadata.\n</code></pre> <pre><code>VectorStore: The created vector store.\n</code></pre> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def create_vector_store_from_source(\n    self,\n    name: str,\n    source: Union[GitHubSource, GitLabSource],\n    source_auth: Optional[str] = None,\n    chunking_strategy: Optional[ChunkingStrategy] = None,\n    expires_after: Optional[ExpiresAfter] = None,\n    metadata: Optional[Dict[str, str]] = None,\n) -&gt; VectorStore:\n    \"\"\"Creates a vector store from the given source.\n\n    Args:\n    ----\n        name (str): The name of the vector store.\n        source (Union[GitHubSource, GitLabSource]): The source from which to create the vector store.\n        source_auth (Optional[str]): The source authentication token.\n        chunking_strategy (Optional[ChunkingStrategy]): The chunking strategy to use.\n        expires_after (Optional[ExpiresAfter]): The expiration policy.\n        metadata (Optional[Dict[str, str]]): Additional metadata.\n\n    Returns:\n    -------\n        VectorStore: The created vector store.\n\n    \"\"\"\n    print(f\"Creating vector store from source: {source}\")\n    headers = {\n        \"Authorization\": f\"Bearer {self._auth}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"name\": name,\n        \"source\": source,\n        \"source_auth\": source_auth,\n        \"chunking_strategy\": chunking_strategy,\n        \"expires_after\": expires_after,\n        \"metadata\": metadata,\n    }\n    endpoint = f\"{self._config.base_url}/vector_stores/from_source\"\n\n    try:\n        response = requests.post(endpoint, headers=headers, json=data)\n        print(response.json())\n        response.raise_for_status()\n        return VectorStore(**response.json())\n    except requests.RequestException as e:\n        raise ValueError(f\"Failed to create vector store: {e}\") from None\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.find_agents","title":"find_agents","text":"<pre><code>find_agents(owner_id: Optional[str] = None, with_capabilities: Optional[bool] = False)\n</code></pre> <p>Filter agents.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def find_agents(self, owner_id: Optional[str] = None, with_capabilities: Optional[bool] = False):\n    \"\"\"Filter agents.\"\"\"\n    return self.client.post(\n        path=f\"{self._config.base_url}/find_agents\",\n        body={\"owner_id\": owner_id, \"with_capabilities\": with_capabilities},\n        cast_to=List[Any],\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.generate_auth_for_current_agent","title":"generate_auth_for_current_agent","text":"<pre><code>generate_auth_for_current_agent(config, agent_identifier)\n</code></pre> <p>Regenerate auth for the current agent.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def generate_auth_for_current_agent(self, config, agent_identifier):\n    \"\"\"Regenerate auth for the current agent.\"\"\"\n    self.agent_identifier = agent_identifier\n    if config.auth is not None:\n        auth_bearer_token = config.auth.generate_bearer_token()\n        new_token = json.loads(auth_bearer_token)\n        new_token[\"runner_data\"] = json.dumps({\"agent\": agent_identifier, \"runner_api_key\": self.runner_api_key})\n        auth_bearer_token = json.dumps(new_token)\n        self._auth = auth_bearer_token\n    else:\n        self._auth = None\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.generate_image","title":"generate_image","text":"<pre><code>generate_image(prompt: str)\n</code></pre> <p>Generate an image.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def generate_image(self, prompt: str):\n    \"\"\"Generate an image.\"\"\"\n    return self.client.images.generate(prompt=prompt)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_agent_data","title":"get_agent_data","text":"<pre><code>get_agent_data()\n</code></pre> <p>Get agent data for the agent this client was initialized with.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_agent_data(self):\n    \"\"\"Get agent data for the agent this client was initialized with.\"\"\"\n    return self.client.get(\n        path=f\"{self._config.base_url}/agent_data\",\n        cast_to=Dict[str, str],\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_agent_data_by_key","title":"get_agent_data_by_key","text":"<pre><code>get_agent_data_by_key(key: str)\n</code></pre> <p>Get agent data by key for the agent this client was initialized with.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_agent_data_by_key(self, key: str):\n    \"\"\"Get agent data by key for the agent this client was initialized with.\"\"\"\n    return self.client.get(\n        path=f\"{self._config.base_url}/agent_data/{key}\",\n        cast_to=Dict[str, str],\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_agent_public_key","title":"get_agent_public_key","text":"<pre><code>get_agent_public_key(agent_name: str) -&gt; str\n</code></pre> <p>Request agent public key.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_agent_public_key(self, agent_name: str) -&gt; str:\n    \"\"\"Request agent public key.\"\"\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n    }\n\n    data = {\"agent_name\": agent_name}\n\n    endpoint = f\"{self._config.base_url}/get_agent_public_key\"\n\n    try:\n        response = requests.post(endpoint, headers=headers, params=data)\n        response.raise_for_status()\n        return response.json()\n    except requests.RequestException as e:\n        raise ValueError(f\"Failed to get agent public key: {e}\") from None\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_thread","title":"get_thread","text":"<pre><code>get_thread(thread_id: str)\n</code></pre> <p>Get a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_thread(self, thread_id: str):\n    \"\"\"Get a thread.\"\"\"\n    return self.client.beta.threads.retrieve(thread_id=thread_id)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_vector_store","title":"get_vector_store","text":"<pre><code>get_vector_store(vector_store_id: str) -&gt; VectorStore\n</code></pre> <p>Gets a vector store by id.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_vector_store(self, vector_store_id: str) -&gt; VectorStore:\n    \"\"\"Gets a vector store by id.\"\"\"\n    endpoint = f\"{self._config.base_url}/vector_stores/{vector_store_id}\"\n    auth_bearer_token = self._auth\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {auth_bearer_token}\",\n    }\n\n    response = requests.get(endpoint, headers=headers)\n    response.raise_for_status()\n    return VectorStore(**response.json())\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.get_vector_store_files","title":"get_vector_store_files","text":"<pre><code>get_vector_store_files(vector_store_id: str) -&gt; Optional[List[VectorStoreFile]]\n</code></pre> <p>Adds a file to vector store.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def get_vector_store_files(self, vector_store_id: str) -&gt; Optional[List[VectorStoreFile]]:\n    \"\"\"Adds a file to vector store.\"\"\"\n    client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n    return client.beta.vector_stores.files.list(vector_store_id=vector_store_id).data\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.query_user_memory","title":"query_user_memory","text":"<pre><code>query_user_memory(query: str)\n</code></pre> <p>Query the user memory.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def query_user_memory(self, query: str):\n    \"\"\"Query the user memory.\"\"\"\n    return self.client.post(\n        path=f\"{self._config.base_url}/vector_stores/memory/query\",\n        body={\"query\": query},\n        cast_to=str,\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.query_vector_store","title":"query_vector_store","text":"<pre><code>query_vector_store(vector_store_id: str, query: str, full_files: bool = False) -&gt; Union[List[SimilaritySearch], List[SimilaritySearchFile]]\n</code></pre> <p>Query a vector store.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def query_vector_store(\n    self, vector_store_id: str, query: str, full_files: bool = False\n) -&gt; Union[List[SimilaritySearch], List[SimilaritySearchFile]]:\n    \"\"\"Query a vector store.\"\"\"\n    if self._config is None:\n        raise ValueError(\"Missing NEAR AI Hub config\")\n\n    auth_bearer_token = self._auth\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {auth_bearer_token}\",\n    }\n\n    data = {\"query\": query, \"full_files\": full_files}\n\n    endpoint = f\"{self._config.base_url}/vector_stores/{vector_store_id}/search\"\n\n    try:\n        response = requests.post(endpoint, headers=headers, json=data)\n        response.raise_for_status()\n        return response.json()\n    except requests.RequestException as e:\n        raise ValueError(f\"Error querying vector store: {e}\") from None\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.remove_file","title":"remove_file","text":"<pre><code>remove_file(file_id: str)\n</code></pre> <p>Removes a file.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def remove_file(self, file_id: str):\n    \"\"\"Removes a file.\"\"\"\n    client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n    return client.files.delete(file_id=file_id)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.run_agent","title":"run_agent","text":"<pre><code>run_agent(run_on_thread_id: str, assistant_id: str, parent_run_id: str = '', run_mode: RunMode = SIMPLE)\n</code></pre> <p>Starts a child agent run from a parent agent run.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def run_agent(\n    self, run_on_thread_id: str, assistant_id: str, parent_run_id: str = \"\", run_mode: RunMode = RunMode.SIMPLE\n):\n    \"\"\"Starts a child agent run from a parent agent run.\"\"\"\n    extra_body = {}\n    if parent_run_id:\n        extra_body[\"parent_run_id\"] = parent_run_id\n    extra_body[\"run_mode\"] = run_mode.value  # type: ignore\n    return self.client.beta.threads.runs.create(\n        thread_id=run_on_thread_id,\n        assistant_id=assistant_id,\n        extra_body=extra_body,\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.save_agent_data","title":"save_agent_data","text":"<pre><code>save_agent_data(key: str, agent_data: Dict[str, Any])\n</code></pre> <p>Save agent data for the agent this client was initialized with.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def save_agent_data(self, key: str, agent_data: Dict[str, Any]):\n    \"\"\"Save agent data for the agent this client was initialized with.\"\"\"\n    return self.client.post(\n        path=f\"{self._config.base_url}/agent_data\",\n        body={\n            \"key\": key,\n            \"value\": agent_data,\n        },\n        cast_to=Dict[str, Any],\n    )\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.schedule_run","title":"schedule_run","text":"<pre><code>schedule_run(agent: str, input_message: str, thread_id: Optional[str], run_params: Optional[Dict[str, str]], run_at: datetime)\n</code></pre> <p>Query a vector store.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def schedule_run(\n    self,\n    agent: str,\n    input_message: str,\n    thread_id: Optional[str],\n    run_params: Optional[Dict[str, str]],\n    run_at: datetime,\n):\n    \"\"\"Query a vector store.\"\"\"\n    if self._config is None:\n        raise ValueError(\"Missing NearAI Hub config\")\n\n    auth_bearer_token = self._auth\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {auth_bearer_token}\",\n    }\n\n    if run_params is None:\n        run_params = {}\n\n    data = {\n        \"agent\": agent,\n        \"input_message\": input_message,\n        \"thread_id\": thread_id,\n        \"run_params\": run_params,\n        \"run_at\": run_at,\n    }\n\n    endpoint = f\"{self._config.base_url}/schedule_run\"\n\n    try:\n        response = requests.post(endpoint, headers=headers, json=data)\n        response.raise_for_status()\n        return response.json()\n    except requests.RequestException as e:\n        raise ValueError(f\"Error querying schedule_run: {e}\") from None\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.set_provider_models","title":"set_provider_models","text":"<pre><code>set_provider_models(provider_models: Optional[ProviderModels])\n</code></pre> <p>Set provider models. Used by external caching.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def set_provider_models(self, provider_models: Optional[ProviderModels]):\n    \"\"\"Set provider models. Used by external caching.\"\"\"\n    if provider_models is None:\n        self._provider_models = ProviderModels(self._config)\n    else:\n        self._provider_models = provider_models\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_create_and_run_poll","title":"threads_create_and_run_poll","text":"<pre><code>threads_create_and_run_poll(assistant_id: str, model: str, messages: List[ChatCompletionMessageParam])\n</code></pre> <p>Create a thread and run the assistant.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_create_and_run_poll(self, assistant_id: str, model: str, messages: List[ChatCompletionMessageParam]):\n    \"\"\"Create a thread and run the assistant.\"\"\"\n    thread = self.create_thread(messages)\n    return self.client.beta.threads.create_and_run_poll(thread=thread, assistant_id=assistant_id, model=model)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_fork","title":"threads_fork","text":"<pre><code>threads_fork(thread_id: str)\n</code></pre> <p>Fork a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_fork(self, thread_id: str):\n    \"\"\"Fork a thread.\"\"\"\n    forked_thread = self.client.post(path=f\"{self._config.base_url}/threads/{thread_id}/fork\", cast_to=Thread)\n    return forked_thread\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_list_messages","title":"threads_list_messages","text":"<pre><code>threads_list_messages(thread_id: str, order: Literal['asc', 'desc'] = 'asc')\n</code></pre> <p>List messages in a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_list_messages(self, thread_id: str, order: Literal[\"asc\", \"desc\"] = \"asc\"):\n    \"\"\"List messages in a thread.\"\"\"\n    return self.client.beta.threads.messages.list(thread_id=thread_id, order=order)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_messages_create","title":"threads_messages_create","text":"<pre><code>threads_messages_create(thread_id: str, content: str, role: Literal['user', 'assistant'])\n</code></pre> <p>Create a message in a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_messages_create(self, thread_id: str, content: str, role: Literal[\"user\", \"assistant\"]):\n    \"\"\"Create a message in a thread.\"\"\"\n    return self.client.beta.threads.messages.create(thread_id=thread_id, content=content, role=role)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.threads_runs_create","title":"threads_runs_create","text":"<pre><code>threads_runs_create(thread_id: str, assistant_id: str, model: str)\n</code></pre> <p>Create a run in a thread.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def threads_runs_create(self, thread_id: str, assistant_id: str, model: str):\n    \"\"\"Create a run in a thread.\"\"\"\n    return self.client.beta.threads.runs.create(thread_id=thread_id, assistant_id=assistant_id, model=model)\n</code></pre>"},{"location":"api/#nearai.shared.inference_client.InferenceClient.upload_file","title":"upload_file","text":"<pre><code>upload_file(file_content: str, purpose: Literal['assistants', 'batch', 'fine-tune', 'vision'], encoding: Optional[str] = 'utf-8', file_name: Optional[str] = 'file.txt', file_type: Optional[str] = 'text/plain') -&gt; Optional[FileObject]\n</code></pre> <p>Uploads a file.</p> Source code in <code>nearai/shared/inference_client.py</code> <pre><code>def upload_file(\n    self,\n    file_content: str,\n    purpose: Literal[\"assistants\", \"batch\", \"fine-tune\", \"vision\"],\n    encoding: Optional[str] = \"utf-8\",\n    file_name: Optional[str] = \"file.txt\",\n    file_type: Optional[str] = \"text/plain\",\n) -&gt; Optional[FileObject]:\n    \"\"\"Uploads a file.\"\"\"\n    client = openai.OpenAI(base_url=self._config.base_url, api_key=self._auth)\n    if file_content:\n        file_data = io.BytesIO(file_content.encode(encoding or \"utf-8\"))\n        return client.files.create(file=(file_name, file_data, file_type), purpose=purpose)\n    else:\n        return None\n</code></pre>"},{"location":"api/#nearai.shared.models","title":"models","text":""},{"location":"api/#nearai.shared.models.AutoFileChunkingStrategyParam","title":"AutoFileChunkingStrategyParam","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>nearai/shared/models.py</code> <pre><code>class AutoFileChunkingStrategyParam(TypedDict, total=False):\n    type: Required[Literal[\"auto\"]]\n    \"\"\"Always `auto`.\"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.AutoFileChunkingStrategyParam.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['auto']]\n</code></pre> <p>Always <code>auto</code>.</p>"},{"location":"api/#nearai.shared.models.ChunkingStrategy","title":"ChunkingStrategy","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the chunking strategy for vector stores.</p> Source code in <code>nearai/shared/models.py</code> <pre><code>class ChunkingStrategy(BaseModel):\n    \"\"\"Defines the chunking strategy for vector stores.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest","title":"CreateVectorStoreRequest","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request model for creating a new vector store.</p> Source code in <code>nearai/shared/models.py</code> <pre><code>class CreateVectorStoreRequest(BaseModel):\n    \"\"\"Request model for creating a new vector store.\"\"\"\n\n    chunking_strategy: Union[AutoFileChunkingStrategyParam, StaticFileChunkingStrategyParam, None] = None\n    \"\"\"The chunking strategy to use for the vector store.\"\"\"\n    expires_after: Optional[ExpiresAfter] = None\n    \"\"\"The expiration time for the vector store.\"\"\"\n    file_ids: Optional[List[str]] = None\n    \"\"\"The file IDs to attach to the vector store.\"\"\"\n    metadata: Optional[Dict[str, str]] = None\n    \"\"\"The metadata to attach to the vector store.\"\"\"\n    name: str\n    \"\"\"The name of the vector store.\"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.chunking_strategy","title":"chunking_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>chunking_strategy: Union[AutoFileChunkingStrategyParam, StaticFileChunkingStrategyParam, None] = None\n</code></pre> <p>The chunking strategy to use for the vector store.</p>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.expires_after","title":"expires_after  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>expires_after: Optional[ExpiresAfter] = None\n</code></pre> <p>The expiration time for the vector store.</p>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.file_ids","title":"file_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>file_ids: Optional[List[str]] = None\n</code></pre> <p>The file IDs to attach to the vector store.</p>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[Dict[str, str]] = None\n</code></pre> <p>The metadata to attach to the vector store.</p>"},{"location":"api/#nearai.shared.models.CreateVectorStoreRequest.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the vector store.</p>"},{"location":"api/#nearai.shared.models.ExpiresAfter","title":"ExpiresAfter","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>nearai/shared/models.py</code> <pre><code>class ExpiresAfter(TypedDict, total=False):\n    anchor: Required[Literal[\"last_active_at\"]]\n    \"\"\"Anchor timestamp after which the expiration policy applies.\n\n    Supported anchors: `last_active_at`.\n    \"\"\"\n\n    days: Required[int]\n    \"\"\"The number of days after the anchor time that the vector store will expire.\"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.ExpiresAfter.anchor","title":"anchor  <code>instance-attribute</code>","text":"<pre><code>anchor: Required[Literal['last_active_at']]\n</code></pre> <p>Anchor timestamp after which the expiration policy applies.</p> <p>Supported anchors: <code>last_active_at</code>.</p>"},{"location":"api/#nearai.shared.models.ExpiresAfter.days","title":"days  <code>instance-attribute</code>","text":"<pre><code>days: Required[int]\n</code></pre> <p>The number of days after the anchor time that the vector store will expire.</p>"},{"location":"api/#nearai.shared.models.StaticFileChunkingStrategyParam","title":"StaticFileChunkingStrategyParam","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>nearai/shared/models.py</code> <pre><code>class StaticFileChunkingStrategyParam(TypedDict, total=False):\n    chunk_overlap_tokens: Required[int]\n    \"\"\"The number of tokens that overlap between chunks. The default value is `400`.\n\n    Note that the overlap must not exceed half of `max_chunk_size_tokens`.\n    \"\"\"\n\n    max_chunk_size_tokens: Required[int]\n    \"\"\"The maximum number of tokens in each chunk.\n\n    The default value is `800`. The minimum value is `100` and the maximum value is\n    `4096`.\n    \"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.StaticFileChunkingStrategyParam.chunk_overlap_tokens","title":"chunk_overlap_tokens  <code>instance-attribute</code>","text":"<pre><code>chunk_overlap_tokens: Required[int]\n</code></pre> <p>The number of tokens that overlap between chunks. The default value is <code>400</code>.</p> <p>Note that the overlap must not exceed half of <code>max_chunk_size_tokens</code>.</p>"},{"location":"api/#nearai.shared.models.StaticFileChunkingStrategyParam.max_chunk_size_tokens","title":"max_chunk_size_tokens  <code>instance-attribute</code>","text":"<pre><code>max_chunk_size_tokens: Required[int]\n</code></pre> <p>The maximum number of tokens in each chunk.</p> <p>The default value is <code>800</code>. The minimum value is <code>100</code> and the maximum value is <code>4096</code>.</p>"},{"location":"api/#nearai.shared.models.VectorStoreFileCreate","title":"VectorStoreFileCreate","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request model for creating a vector store file.</p> Source code in <code>nearai/shared/models.py</code> <pre><code>class VectorStoreFileCreate(BaseModel):\n    \"\"\"Request model for creating a vector store file.\"\"\"\n\n    file_id: str\n    \"\"\"File ID returned from upload file endpoint.\"\"\"\n</code></pre>"},{"location":"api/#nearai.shared.models.VectorStoreFileCreate.file_id","title":"file_id  <code>instance-attribute</code>","text":"<pre><code>file_id: str\n</code></pre> <p>File ID returned from upload file endpoint.</p>"},{"location":"api/#nearai.shared.naming","title":"naming","text":""},{"location":"api/#nearai.shared.naming.NamespacedName","title":"NamespacedName","text":"Source code in <code>nearai/shared/naming.py</code> <pre><code>class NamespacedName:\n    def __init__(self, name: str, namespace: str = \"\"):  # noqa: D107\n        self.name = name\n        self.namespace = namespace\n\n    def __eq__(self, other):  # noqa: D105\n        if not isinstance(other, NamespacedName):\n            return NotImplemented\n        return self.name == other.name and self.namespace == other.namespace\n\n    def __hash__(self):  # noqa: D105\n        return hash((self.name, self.namespace))\n\n    def __str__(self):  # noqa: D105\n        if self.namespace:\n            return f\"{self.namespace}/{self.name}\"\n        return self.name\n\n    def __repr__(self):  # noqa: D105\n        return f\"NamespacedName(name='{self.name}', namespace='{self.namespace}')\"\n\n    def canonical(self) -&gt; \"NamespacedName\":  # noqa: D105\n        \"\"\"Returns canonical NamespacedName.\"\"\"\n        return NamespacedName(\n            name=get_canonical_name(self.name),\n            namespace=get_canonical_name(self.namespace) if self.namespace != DEFAULT_NAMESPACE else \"\",\n        )\n</code></pre>"},{"location":"api/#nearai.shared.naming.NamespacedName.canonical","title":"canonical","text":"<pre><code>canonical() -&gt; NamespacedName\n</code></pre> <p>Returns canonical NamespacedName.</p> Source code in <code>nearai/shared/naming.py</code> <pre><code>def canonical(self) -&gt; \"NamespacedName\":  # noqa: D105\n    \"\"\"Returns canonical NamespacedName.\"\"\"\n    return NamespacedName(\n        name=get_canonical_name(self.name),\n        namespace=get_canonical_name(self.namespace) if self.namespace != DEFAULT_NAMESPACE else \"\",\n    )\n</code></pre>"},{"location":"api/#nearai.shared.naming.create_registry_name","title":"create_registry_name","text":"<pre><code>create_registry_name(name: str) -&gt; str\n</code></pre> <p>Formats <code>name</code> for a suitable registry name.</p> Source code in <code>nearai/shared/naming.py</code> <pre><code>def create_registry_name(name: str) -&gt; str:\n    \"\"\"Formats `name` for a suitable registry name.\"\"\"\n    # Convert to lowercase\n    name = name.lower()\n    # Convert '.' between digits to 'p'\n    name = re.sub(r\"(\\d)\\.(\\d)\", r\"\\1p\\2\", name)\n    # Convert '&lt;digit&gt;v&lt;digit&gt;' -&gt; '&lt;digit&gt;-&lt;digit&gt;'\n    name = re.sub(r\"(\\d)v(\\d)\", r\"\\1-\\2\", name)\n    # Convert '&lt;not letter&gt;v&lt;digit&gt;' -&gt; '&lt;not letter&gt;&lt;digit&gt;'\n    name = re.sub(r\"(^|[^a-z])v(\\d)\", r\"\\1\\2\", name)\n    # Replace non-alphanumeric characters between digits with '-'\n    name = re.sub(r\"(\\d)[^a-z0-9]+(\\d)\", r\"\\1-\\2\", name)\n    # Remove remaining non-alphanumeric characters, except '-'\n    name = re.sub(r\"[^a-z0-9-]\", \"\", name)\n    # Convert 'metallama' or 'meta-llama' to 'llama'\n    name = name.replace(\"metallama\", \"llama\")\n    name = name.replace(\"meta-llama\", \"llama\")\n    # Convert 'qwenq' or 'qwen-q' to 'q'\n    name = name.replace(\"qwenq\", \"q\")\n    name = name.replace(\"qwen-q\", \"q\")\n    return name\n</code></pre>"},{"location":"api/#nearai.shared.naming.get_canonical_name","title":"get_canonical_name","text":"<pre><code>get_canonical_name(name: str) -&gt; str\n</code></pre> <p>Returns a name that can be used for matching entities.</p> <p>Applies such transformations: 1. All letters lowercase. 2. Convert '.' between digits to 'p'. 3. Convert 'v' -&gt; '' 4. Remove all non-alphanumeric characters except between digits.     Use '_' between digits. 5. Convert 'metallama' -&gt; 'llama'. <p>e.g. \"llama-3.1-70b-instruct\" -&gt; \"llama3p1_70binstruct\"</p> Source code in <code>nearai/shared/naming.py</code> <pre><code>def get_canonical_name(name: str) -&gt; str:\n    \"\"\"Returns a name that can be used for matching entities.\n\n    Applies such transformations:\n    1. All letters lowercase.\n    2. Convert '.' between digits to 'p'.\n    3. Convert '&lt;not letter&gt;v&lt;digit&gt;' -&gt; '&lt;not letter&gt;&lt;digit&gt;'\n    4. Remove all non-alphanumeric characters except between digits.\n        Use '_' between digits.\n    5. Convert 'metallama' -&gt; 'llama'.\n\n    e.g. \"llama-3.1-70b-instruct\" -&gt; \"llama3p1_70binstruct\"\n    \"\"\"\n    # Convert to lowercase\n    name = name.lower()\n    # Convert '.' between digits to 'p'\n    name = re.sub(r\"(\\d)\\.(\\d)\", r\"\\1p\\2\", name)\n    # Convert '&lt;digit&gt;v&lt;digit&gt;' -&gt; '&lt;digit&gt;_&lt;digit&gt;'\n    name = re.sub(r\"(\\d)v(\\d)\", r\"\\1_\\2\", name)\n    # Convert '&lt;not letter&gt;v&lt;digit&gt;' -&gt; '&lt;not letter&gt;&lt;digit&gt;'\n    name = re.sub(r\"(^|[^a-z])v(\\d)\", r\"\\1\\2\", name)\n    # Replace non-alphanumeric characters between digits with '_'\n    name = re.sub(r\"(\\d)[^a-z0-9]+(\\d)\", r\"\\1_\\2\", name)\n    # Remove remaining non-alphanumeric characters, except '_'\n    name = re.sub(r\"[^a-z0-9_]\", \"\", name)\n    # Remove any remaining underscores that are not between digits\n    name = re.sub(r\"(?&lt;!\\d)_|_(?!\\d)\", \"\", name)\n    # Convert 'metallama' to 'llama'\n    name = name.replace(\"metallama\", \"llama\")\n    # Convert 'qwenq' to 'q'\n    name = name.replace(\"qwenq\", \"q\")\n    return name\n</code></pre>"},{"location":"api/#nearai.shared.near","title":"near","text":""},{"location":"api/#nearai.shared.near.sign","title":"sign","text":""},{"location":"api/#nearai.shared.near.sign.SignatureVerificationResult","title":"SignatureVerificationResult","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>class SignatureVerificationResult(Enum):\n    TRUE = True\n    FALSE = False\n    VERIFY_ACCESS_KEY_OWNER_SERVICE_NOT_AVAILABLE = \"verify_access_key_owner_not_available\"\n\n    @classmethod\n    def from_bool(cls, value: bool):\n        \"\"\"Gets VerificationResult based on a boolean value.\"\"\"\n        return cls.TRUE if value else cls.FALSE\n\n    def __bool__(self):\n        \"\"\"Overrides the behavior when checking for truthiness.\"\"\"\n        return self == SignatureVerificationResult.TRUE\n</code></pre> __bool__ \u00b6 <pre><code>__bool__()\n</code></pre> <p>Overrides the behavior when checking for truthiness.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def __bool__(self):\n    \"\"\"Overrides the behavior when checking for truthiness.\"\"\"\n    return self == SignatureVerificationResult.TRUE\n</code></pre> from_bool <code>classmethod</code> \u00b6 <pre><code>from_bool(value: bool)\n</code></pre> <p>Gets VerificationResult based on a boolean value.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>@classmethod\ndef from_bool(cls, value: bool):\n    \"\"\"Gets VerificationResult based on a boolean value.\"\"\"\n    return cls.TRUE if value else cls.FALSE\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.convert_nonce","title":"convert_nonce","text":"<pre><code>convert_nonce(value: Union[str, bytes, list[int]])\n</code></pre> <p>Converts a given value to a 32-byte nonce.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def convert_nonce(value: Union[str, bytes, list[int]]):\n    \"\"\"Converts a given value to a 32-byte nonce.\"\"\"\n    if isinstance(value, bytes):\n        if len(value) &gt; 32:\n            raise ValueError(\"Invalid nonce length\")\n        if len(value) &lt; 32:\n            value = value.rjust(32, b\"0\")\n        return value\n    elif isinstance(value, str):\n        nonce_bytes = value.encode(\"utf-8\")\n        if len(nonce_bytes) &gt; 32:\n            raise ValueError(\"Invalid nonce length\")\n        if len(nonce_bytes) &lt; 32:\n            nonce_bytes = nonce_bytes.rjust(32, b\"0\")\n        return nonce_bytes\n    elif isinstance(value, list):\n        if len(value) != 32:\n            raise ValueError(\"Invalid nonce length\")\n        return bytes(value)\n    else:\n        raise ValueError(\"Invalid nonce format\")\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.create_inference_signature","title":"create_inference_signature","text":"<pre><code>create_inference_signature(private_key: str, payload: CompletionSignaturePayload) -&gt; tuple[str, str]\n</code></pre> <p>Creates a cryptographic signature for a given extended inference payload using a specified private key.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def create_inference_signature(private_key: str, payload: CompletionSignaturePayload) -&gt; tuple[str, str]:\n    \"\"\"Creates a cryptographic signature for a given extended inference payload using a specified private key.\"\"\"\n    borsh_payload = BinarySerializer(dict(COMPLETION_PAYLOAD_SCHEMA)).serialize(payload)\n\n    to_sign = hashlib.sha256(borsh_payload).digest()\n\n    private_key_base58 = private_key[len(ED_PREFIX) :]\n    private_key_bytes = base58.b58decode(private_key_base58)\n\n    if len(private_key_bytes) != 64:\n        raise ValueError(\"The private key must be exactly 64 bytes long\")\n\n    private_key_seed = private_key_bytes[:32]\n\n    signing_key = nacl.signing.SigningKey(private_key_seed)\n    public_key = signing_key.verify_key\n\n    signed = signing_key.sign(to_sign)\n    signature = base64.b64encode(signed.signature).decode(\"utf-8\")\n\n    public_key_base58 = base58.b58encode(public_key.encode()).decode(\"utf-8\")\n    full_public_key = ED_PREFIX + public_key_base58\n\n    return signature, full_public_key\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.create_signature","title":"create_signature","text":"<pre><code>create_signature(private_key: str, payload: Payload) -&gt; tuple[str, str]\n</code></pre> <p>Creates a cryptographic signature for a given payload using a specified private key.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def create_signature(private_key: str, payload: Payload) -&gt; tuple[str, str]:\n    \"\"\"Creates a cryptographic signature for a given payload using a specified private key.\"\"\"\n    borsh_payload = BinarySerializer(dict(PAYLOAD_SCHEMA)).serialize(payload)\n\n    to_sign = hashlib.sha256(borsh_payload).digest()\n\n    # Extract and decode the private key\n    private_key_base58 = private_key[len(ED_PREFIX) :]\n    private_key_bytes = base58.b58decode(private_key_base58)\n\n    if len(private_key_bytes) != 64:\n        raise ValueError(\"The private key must be exactly 64 bytes long\")\n\n    # Use only the first 32 bytes as the seed\n    private_key_seed = private_key_bytes[:32]\n\n    signing_key = nacl.signing.SigningKey(private_key_seed)\n    public_key = signing_key.verify_key\n\n    signed = signing_key.sign(to_sign)\n    signature = base64.b64encode(signed.signature).decode(\"utf-8\")\n\n    public_key_base58 = base58.b58encode(public_key.encode()).decode(\"utf-8\")\n    full_public_key = ED_PREFIX + public_key_base58\n\n    return signature, full_public_key\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.validate_completion_signature","title":"validate_completion_signature","text":"<pre><code>validate_completion_signature(public_key: str, signature: str, payload: CompletionSignaturePayload)\n</code></pre> <p>Validates a cryptographic signature for a given payload using a specified public key.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def validate_completion_signature(public_key: str, signature: str, payload: CompletionSignaturePayload):\n    \"\"\"Validates a cryptographic signature for a given payload using a specified public key.\"\"\"\n    borsh_payload = BinarySerializer(dict(COMPLETION_PAYLOAD_SCHEMA)).serialize(payload)\n    to_sign = hashlib.sha256(borsh_payload).digest()\n    real_signature = base64.b64decode(signature)\n\n    verify_key: nacl.signing.VerifyKey = nacl.signing.VerifyKey(base58.b58decode(public_key[len(ED_PREFIX) :]))\n\n    try:\n        verify_key.verify(to_sign, real_signature)\n        return True\n    except nacl.exceptions.BadSignatureError:\n        return False\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.validate_nonce","title":"validate_nonce","text":"<pre><code>validate_nonce(value: Union[str, bytes, list[int]])\n</code></pre> <p>Ensures that the nonce is a valid timestamp.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def validate_nonce(value: Union[str, bytes, list[int]]):\n    \"\"\"Ensures that the nonce is a valid timestamp.\"\"\"\n    nonce = convert_nonce(value)\n    nonce_int = int(nonce.decode(\"utf-8\"))\n\n    now = int(time.time() * 1000)\n\n    if nonce_int &gt; now:\n        raise ValueError(\"Nonce is in the future\")\n    if now - nonce_int &gt; 10 * 365 * 24 * 60 * 60 * 1000:\n        \"\"\"If the timestamp is older than 10 years, it is considered invalid. Forcing apps to use unique nonces.\"\"\"\n        raise ValueError(\"Nonce is too old\")\n\n    return nonce\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.validate_signature","title":"validate_signature","text":"<pre><code>validate_signature(public_key: str, signature: str, payload: Payload)\n</code></pre> <p>Validates a cryptographic signature for a given payload using a specified public key.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def validate_signature(public_key: str, signature: str, payload: Payload):\n    \"\"\"Validates a cryptographic signature for a given payload using a specified public key.\"\"\"\n    borsh_payload = BinarySerializer(dict(PAYLOAD_SCHEMA)).serialize(payload)\n    to_sign = hashlib.sha256(borsh_payload).digest()\n    real_signature = base64.b64decode(signature)\n\n    verify_key: nacl.signing.VerifyKey = nacl.signing.VerifyKey(base58.b58decode(public_key[len(ED_PREFIX) :]))\n\n    try:\n        verify_key.verify(to_sign, real_signature)\n        # print(\"Signature is valid.\")\n        return True\n    except nacl.exceptions.BadSignatureError:\n        # print(\"Signature was forged or corrupt.\")\n        return False\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.verify_access_key_owner","title":"verify_access_key_owner","text":"<pre><code>verify_access_key_owner(public_key, account_id) -&gt; SignatureVerificationResult\n</code></pre> <p>Verifies if a given public key belongs to a specified account ID using FastNEAR API.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>@mem_cache_with_timeout(300)\ndef verify_access_key_owner(public_key, account_id) -&gt; SignatureVerificationResult:\n    \"\"\"Verifies if a given public key belongs to a specified account ID using FastNEAR API.\"\"\"\n    try:\n        logger.info(f\"Verifying access key owner for public key: {public_key}, account_id: {account_id}\")\n        url = f\"https://api.fastnear.com/v0/public_key/{public_key}\"\n        response = requests.get(url)\n        response.raise_for_status()\n        content = response.json()\n        account_ids = content.get(\"account_ids\", [])\n        key_owner_verified = account_id in account_ids\n        if not key_owner_verified:\n            logger.info(\"Key's owner verification failed. Only NEAR Mainnet accounts are supported.\")\n        return SignatureVerificationResult.from_bool(key_owner_verified)\n    except requests.exceptions.HTTPError as http_err:\n        logger.error(f\"HTTP error occurred: {http_err}\")\n    except Exception as err:\n        logger.error(f\"Other error occurred: {err}\")\n\n    return SignatureVerificationResult.VERIFY_ACCESS_KEY_OWNER_SERVICE_NOT_AVAILABLE\n</code></pre>"},{"location":"api/#nearai.shared.near.sign.verify_signed_message","title":"verify_signed_message","text":"<pre><code>verify_signed_message(account_id, public_key, signature, message, nonce, recipient, callback_url) -&gt; SignatureVerificationResult\n</code></pre> <p>Verifies a signed message and ensures the public key belongs to the specified account.</p> Source code in <code>nearai/shared/near/sign.py</code> <pre><code>def verify_signed_message(\n    account_id, public_key, signature, message, nonce, recipient, callback_url\n) -&gt; SignatureVerificationResult:\n    \"\"\"Verifies a signed message and ensures the public key belongs to the specified account.\"\"\"\n    is_valid = validate_signature(public_key, signature, Payload(message, nonce, recipient, callback_url))\n\n    if not is_valid and callback_url is not None:\n        is_valid = validate_signature(public_key, signature, Payload(message, nonce, recipient, None))\n\n    if is_valid:\n        # verify that key belongs to `account_id`\n        return verify_access_key_owner(public_key, account_id)\n\n    # TODO verifies that key is a FULL ACCESS KEY\n\n    return SignatureVerificationResult.FALSE\n</code></pre>"},{"location":"api/#nearai.shared.provider_models","title":"provider_models","text":""},{"location":"api/#nearai.shared.provider_models.ProviderModels","title":"ProviderModels","text":"Source code in <code>nearai/shared/provider_models.py</code> <pre><code>class ProviderModels:\n    def __init__(self, config: ClientConfig) -&gt; None:  # noqa: D107\n        self._config = config\n\n    @cached_property\n    def provider_models(self) -&gt; Dict[NamespacedName, Dict[str, str]]:\n        \"\"\"Returns a mapping canonical-&gt;provider-&gt;model_full_name.\"\"\"\n        client = self._config.get_hub_client()\n\n        try:\n            models = client.models.list()\n\n            assert len(models.data) &gt; 0, \"No models found\"\n            result: Dict[NamespacedName, Dict[str, str]] = {}\n            for model in models.data:\n                provider, namespaced_model = get_provider_namespaced_model(model.id)\n                namespaced_model = namespaced_model.canonical()\n                if namespaced_model not in result:\n                    result[namespaced_model] = {}\n                if provider in result[namespaced_model]:\n                    raise ValueError(f\"Duplicate entry for provider {provider} and model {namespaced_model}\")\n                result[namespaced_model][provider] = model.id\n\n            return result\n\n        except requests.RequestException as e:\n            raise RuntimeError(f\"Error fetching models: {str(e)}\") from e\n\n    def available_provider_matches(self, model: NamespacedName) -&gt; Dict[str, str]:\n        \"\"\"Returns provider matches for `model`.\"\"\"\n        return self.provider_models.get(model.canonical(), {})\n\n    def match_provider_model(self, model: str, provider: Optional[str] = None) -&gt; Tuple[str, str]:\n        \"\"\"Returns provider and model_full_path for given `model` and optional `provider`.\n\n        `model` may take different formats. Supported ones:\n        1. model_full_path, e.g. \"fireworks::accounts/yi-01-ai/models/yi-large\"\n        2. model_full_path without provider, e.g. \"accounts/yi-01-ai/models/yi-large\"\n        3. model_short_name as used by provider, e.g. \"llama-v3-70b-instruct\"\n        4. namespace/model_short_name as used by provider, e.g. \"yi-01-ai/yi-large\"\n        5. model_name as used in registry, e.g. \"llama-3-70b-instruct\"\n        6. namespace/model_name as used in registry, e.g. \"near.ai/llama-3-70b-instruct\"\n        \"\"\"\n        if provider == \"\":\n            provider = None\n        matched_provider, namespaced_model = get_provider_namespaced_model(model, provider)\n        namespaced_model = namespaced_model.canonical()\n        if namespaced_model not in self.provider_models:\n            raise ValueError(f\"Model {namespaced_model} not present in provider models {self.provider_models}\")\n        available_matches = self.provider_models[namespaced_model]\n        if matched_provider not in available_matches:\n            for match in available_matches.keys():\n                matched_provider = match\n                break\n        if provider and provider != matched_provider:\n            raise ValueError(\n                f\"Requested provider {provider} for model {model} does not match matched_provider {matched_provider}\"\n            )\n        return matched_provider, available_matches[matched_provider]\n\n    def get_unregistered_common_provider_models(\n        self, registry_models: Dict[NamespacedName, NamespacedName]\n    ) -&gt; List[Dict[str, str]]:\n        \"\"\"Returns provider matches for unregistered provider models with default namespace.\"\"\"\n        result: List[Dict[str, str]] = []\n        for namespaced_name, available_matches in self.provider_models.items():\n            if namespaced_name.namespace != \"\" or namespaced_name in registry_models:\n                continue\n            result.append(available_matches)\n        return result\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.ProviderModels.provider_models","title":"provider_models  <code>cached</code> <code>property</code>","text":"<pre><code>provider_models: Dict[NamespacedName, Dict[str, str]]\n</code></pre> <p>Returns a mapping canonical-&gt;provider-&gt;model_full_name.</p>"},{"location":"api/#nearai.shared.provider_models.ProviderModels.available_provider_matches","title":"available_provider_matches","text":"<pre><code>available_provider_matches(model: NamespacedName) -&gt; Dict[str, str]\n</code></pre> <p>Returns provider matches for <code>model</code>.</p> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def available_provider_matches(self, model: NamespacedName) -&gt; Dict[str, str]:\n    \"\"\"Returns provider matches for `model`.\"\"\"\n    return self.provider_models.get(model.canonical(), {})\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.ProviderModels.get_unregistered_common_provider_models","title":"get_unregistered_common_provider_models","text":"<pre><code>get_unregistered_common_provider_models(registry_models: Dict[NamespacedName, NamespacedName]) -&gt; List[Dict[str, str]]\n</code></pre> <p>Returns provider matches for unregistered provider models with default namespace.</p> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def get_unregistered_common_provider_models(\n    self, registry_models: Dict[NamespacedName, NamespacedName]\n) -&gt; List[Dict[str, str]]:\n    \"\"\"Returns provider matches for unregistered provider models with default namespace.\"\"\"\n    result: List[Dict[str, str]] = []\n    for namespaced_name, available_matches in self.provider_models.items():\n        if namespaced_name.namespace != \"\" or namespaced_name in registry_models:\n            continue\n        result.append(available_matches)\n    return result\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.ProviderModels.match_provider_model","title":"match_provider_model","text":"<pre><code>match_provider_model(model: str, provider: Optional[str] = None) -&gt; Tuple[str, str]\n</code></pre> <p>Returns provider and model_full_path for given <code>model</code> and optional <code>provider</code>.</p> <p><code>model</code> may take different formats. Supported ones: 1. model_full_path, e.g. \"fireworks::accounts/yi-01-ai/models/yi-large\" 2. model_full_path without provider, e.g. \"accounts/yi-01-ai/models/yi-large\" 3. model_short_name as used by provider, e.g. \"llama-v3-70b-instruct\" 4. namespace/model_short_name as used by provider, e.g. \"yi-01-ai/yi-large\" 5. model_name as used in registry, e.g. \"llama-3-70b-instruct\" 6. namespace/model_name as used in registry, e.g. \"near.ai/llama-3-70b-instruct\"</p> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def match_provider_model(self, model: str, provider: Optional[str] = None) -&gt; Tuple[str, str]:\n    \"\"\"Returns provider and model_full_path for given `model` and optional `provider`.\n\n    `model` may take different formats. Supported ones:\n    1. model_full_path, e.g. \"fireworks::accounts/yi-01-ai/models/yi-large\"\n    2. model_full_path without provider, e.g. \"accounts/yi-01-ai/models/yi-large\"\n    3. model_short_name as used by provider, e.g. \"llama-v3-70b-instruct\"\n    4. namespace/model_short_name as used by provider, e.g. \"yi-01-ai/yi-large\"\n    5. model_name as used in registry, e.g. \"llama-3-70b-instruct\"\n    6. namespace/model_name as used in registry, e.g. \"near.ai/llama-3-70b-instruct\"\n    \"\"\"\n    if provider == \"\":\n        provider = None\n    matched_provider, namespaced_model = get_provider_namespaced_model(model, provider)\n    namespaced_model = namespaced_model.canonical()\n    if namespaced_model not in self.provider_models:\n        raise ValueError(f\"Model {namespaced_model} not present in provider models {self.provider_models}\")\n    available_matches = self.provider_models[namespaced_model]\n    if matched_provider not in available_matches:\n        for match in available_matches.keys():\n            matched_provider = match\n            break\n    if provider and provider != matched_provider:\n        raise ValueError(\n            f\"Requested provider {provider} for model {model} does not match matched_provider {matched_provider}\"\n        )\n    return matched_provider, available_matches[matched_provider]\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.get_provider_model","title":"get_provider_model","text":"<pre><code>get_provider_model(provider: Optional[str], model: str) -&gt; Tuple[Optional[str], str]\n</code></pre> <p>Splits the <code>model</code> string based on a predefined separator and returns the components.</p> <pre><code>provider (Optional[str]): The default provider name. Can be `None` if the provider\n                          is included in the `model` string.\nmodel (str): The model identifier, which may include the provider name separated by\n             a specific delimiter (defined by `PROVIDER_MODEL_SEP`, e.g. `::`).\n</code></pre> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def get_provider_model(provider: Optional[str], model: str) -&gt; Tuple[Optional[str], str]:\n    \"\"\"Splits the `model` string based on a predefined separator and returns the components.\n\n    Args:\n    ----\n        provider (Optional[str]): The default provider name. Can be `None` if the provider\n                                  is included in the `model` string.\n        model (str): The model identifier, which may include the provider name separated by\n                     a specific delimiter (defined by `PROVIDER_MODEL_SEP`, e.g. `::`).\n\n    \"\"\"\n    if PROVIDER_MODEL_SEP in model:\n        parts = model.split(PROVIDER_MODEL_SEP)\n        assert len(parts) == 2\n        return parts[0], parts[1]\n    return provider, model\n</code></pre>"},{"location":"api/#nearai.shared.provider_models.get_provider_namespaced_model","title":"get_provider_namespaced_model","text":"<pre><code>get_provider_namespaced_model(provider_model: str, provider: Optional[str] = None) -&gt; Tuple[str, NamespacedName]\n</code></pre> <p>Given <code>provider_model</code> returns provider and namespaced model.</p> Source code in <code>nearai/shared/provider_models.py</code> <pre><code>def get_provider_namespaced_model(provider_model: str, provider: Optional[str] = None) -&gt; Tuple[str, NamespacedName]:\n    \"\"\"Given `provider_model` returns provider and namespaced model.\"\"\"\n    provider_model = provider_model.replace(\"accounts/\", \"\")\n    provider_model = provider_model.replace(\"fireworks/\", \"\")\n    provider_model = provider_model.replace(\"models/\", \"\")\n    provider_opt, model = get_provider_model(DEFAULT_PROVIDER if not provider else provider, provider_model)\n    provider = cast(str, provider_opt)\n    if provider == \"hyperbolic\":\n        model = re.sub(r\".*/\", \"\", model)\n        return provider, NamespacedName(model)\n    if provider == \"fireworks\":\n        parts = model.split(\"/\")\n        if len(parts) == 1:\n            return provider, NamespacedName(name=parts[0])\n        elif len(parts) == 2:\n            return provider, NamespacedName(namespace=parts[0], name=parts[1])\n        else:\n            raise ValueError(f\"Invalid model format for Fireworks: {model}\")\n    if provider == \"local\":\n        model = re.sub(r\".*/\", \"\", model)\n        return provider, NamespacedName(name=model)\n    raise ValueError(f\"Unrecognized provider: {provider}\")\n</code></pre>"},{"location":"api/#nearai.solvers","title":"solvers","text":""},{"location":"api/#nearai.solvers.DDOTSV0Solver","title":"DDOTSV0Solver","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for competitive programming problems live on DDOTS.</p> <p>This dataset will run agents in an Agent environment previously prepared.</p> <p>workspace/     .id             -- Id of the problem     PROBLEM.txt     -- Description of the problem</p> <p>The agent should call env.submit_python(code) to submit the code to the DDOTS server.</p> Source code in <code>nearai/solvers/ddot_v0_solver.py</code> <pre><code>class DDOTSV0Solver(SolverStrategy):\n    \"\"\"Solver strategy for competitive programming problems live on DDOTS.\n\n    This dataset will run agents in an Agent environment previously prepared.\n\n    workspace/\n        .id             -- Id of the problem\n        PROBLEM.txt     -- Description of the problem\n\n    The agent should call env.submit_python(code) to submit the code to the DDOTS server.\n\n    \"\"\"\n\n    def __init__(self, dataset_ref: Dataset, agents: str, max_iterations: int, save_snapshots: bool = False):  # noqa: D107\n        client_config = ClientConfig(\n            base_url=CONFIG.nearai_hub.base_url,\n            auth=CONFIG.auth,\n        )\n        self.agents = [Agent.load_agent(agent, client_config) for agent in agents.split(\",\")]\n        self.max_iterations = max_iterations\n\n        date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        rnd_id = random.randint(10**8, 10**9 - 1)\n        self._saved_trajectories = DATA_FOLDER / \"data\" / \"ddots_v0_trajectories\" / f\"{date}_{rnd_id}\"\n        self._saved_trajectories.mkdir(parents=True, exist_ok=True)\n\n        self.save_snapshots = save_snapshots\n        print(\"Saving trajectories to\", self._saved_trajectories)\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"ddots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"ddots_codeforces_small/v0\", \"datasets/ddots_codeforces_medium_A_B/v0\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        problem_id = datum[\"problem_id\"]\n        description = datum[\"description\"]\n\n        client_config = ClientConfig(\n            base_url=CONFIG.nearai_hub.base_url,\n            auth=CONFIG.auth,\n        )\n        client = InferenceClient(client_config)\n        env = DDOTSEnvironment(self.agents, problem_id, description, client)\n        env.write_file(\".solved\", str(False))\n\n        try:\n            env.run(description, max_iterations=self.max_iterations)\n            env.write_file(\".solved\", str(env.solved))\n\n        except Exception as e:\n            print(f\"Error running task: {e}\")\n\n        finally:\n            if self.save_snapshots:\n                snapshot = env.create_snapshot()\n                with open(self._saved_trajectories / f\"{problem_id}.tar.gz\", \"wb\") as f:\n                    f.write(snapshot)\n\n        return env.solved\n</code></pre>"},{"location":"api/#nearai.solvers.GSM8KSolverStrategy","title":"GSM8KSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the GSM8K dataset.</p> Source code in <code>nearai/solvers/gsm8k_solver.py</code> <pre><code>class GSM8KSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the GSM8K dataset.\"\"\"\n\n    SHOTS = 8\n\n    def __init__(self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\") -&gt; None:  # noqa: D107\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"gsm8k\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"gsm8k\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        parsed_datum: GSM8KDatum = GSM8KDatum(**datum)\n\n        problem_shots_indices = list(range(0, self.SHOTS))\n        problem_shots = list(\n            map(\n                lambda i: GSM8KDatum(**self.dataset_ref[\"train\"][i]).model_dump(),\n                problem_shots_indices,\n            )\n        )\n\n        session = self.start_inference_session(\"\")\n        session.add_system_message(\n            dedent(\n                \"\"\"\n                    You are a helpful assistant. You're goal is to answer word based math questions.\n                    \"\"\"\n                + \"\\n\\n\"\n                + \"Here are some examples of math questions and their answers:\"\n                + \"\\n\\n\".join([f\"Question: {shot['question']}\\nAnswer: {shot['answer']}\" for shot in problem_shots])\n                + \"\\n\\n\"\n                + \"Now, answer the next question provided in the user prompt. \"\n                + \"Think step by step about how to solve the problem. \"\n                + \"Then, provide the answer.\"\n            )\n        )\n        res_output = session.run_task(parsed_datum.question).strip()\n\n        ## cleanup the output\n        session = self.start_inference_session(\"\")\n        res_refined_output = session.run_task(\n            dedent(\n                f\"\"\"\n                    You are a helpful assistant. You're goal is to answer math questions.\n\n                    You have just answered a math question with the following response:\n\n                    --- BEGIN RESPONSE ---\n                    {res_output}\n                    --- END RESPONSE ---\n\n                    Please refine your answer.\n\n                    Only output the final number *without units* as your answer. Nothing else.\n                    \"\"\"\n            )\n        ).strip()\n        res_refined_output = res_refined_output.replace(\"$\", \"\").replace(\",\", \"\")\n        if \" \" in res_refined_output:\n            res_refined_output = res_refined_output.split(\" \")[0]\n        try:\n            res_refined_output = str(int(res_refined_output))\n        except Exception:\n            pass\n        try:\n            res_refined_output = str(int(float(res_refined_output)))\n        except Exception:\n            pass\n\n        refined_answer = parsed_datum.answer.replace(\"$\", \"\").replace(\",\", \"\")\n        print(res_refined_output, refined_answer)\n        return res_refined_output == refined_answer\n</code></pre>"},{"location":"api/#nearai.solvers.HellaswagSolverStrategy","title":"HellaswagSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MMLU dataset.</p> Source code in <code>nearai/solvers/hellaswag_solver.py</code> <pre><code>class HellaswagSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MMLU dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 8\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return f\"hellaswag_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"hellaswag\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = HellaswagDatum(**datum).model_dump()\n\n        choices = [\"A\", \"B\", \"C\", \"D\"]\n        example_problems_indices = list(range(0, 5 * self.shots, 5))\n        example_problems = list(\n            map(\n                lambda d: HellaswagDatum(**d).model_dump(),\n                [self.dataset_ref[\"validation\"][i] for i in example_problems_indices],\n            )\n        )\n        base_prompt = Template(\n            open(PROMPTS_FOLDER / \"hellaswag_verbose_answer.j2\").read(),\n            trim_blocks=True,\n        ).render(\n            example_problems=example_problems,\n            challenge_problem=datum,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"hellaswag_extract_answer.j2\").read(),\n            trim_blocks=True,\n        ).render(\n            challenge_problem=datum,\n            answer_text=response,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(extract_answer_prompt)\n\n        try:\n            answer = choices.index(response)\n            return bool(answer == int(datum[\"label\"]))\n        except Exception:\n            print(\"Failed to parse answer\")\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.LeanSolverStrategy","title":"LeanSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy to evaluate against Lean problems.</p> Source code in <code>nearai/solvers/lean_solver.py</code> <pre><code>class LeanSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy to evaluate against Lean problems.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\"\n    ) -&gt; None:\n        super().__init__(model, agent)\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        assert self.dataset_evaluation_name\n        return self.dataset_evaluation_name\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"lean\"]\n\n    def solve(self, datum: dict) -&gt; Tuple[bool, dict]:  # noqa: D102\n        lean_datum = LeanDatum.model_validate(datum)\n        lean_datum.url = load_repository(lean_datum.url)\n\n        info: dict = {}\n        info[\"verbose\"] = {}\n\n        lean_task = LeanTaskInfo(\n            lean_datum.url,\n            lean_datum.commit,\n            lean_datum.filename,\n            lean_datum.theorem,\n            load_theorem(lean_datum),\n        )\n        info[\"verbose\"][\"theorem_raw\"] = lean_task.theorem_raw\n\n        base_prompt = Template(open(PROMPTS_FOLDER / \"lean_answer.j2\").read(), trim_blocks=True).render(\n            url=lean_task.url,\n            commit=lean_task.commit,\n            filepath=lean_task.filename,\n            theorem_name=lean_task.theorem,\n            theorem_raw=lean_task.theorem_raw,\n            begin_marker=BEGIN_MARKER,\n            end_marker=END_MARKER,\n        )\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        json_response = extract_between_markers(response)\n        if not json_response:\n            info[\"error\"] = \"Failed to extract between markers.\"\n            info[\"verbose\"][\"response\"] = response\n            return False, info\n\n        tactics = parse_tactics(json_response)\n        if not tactics:\n            info[\"error\"] = \"Failed to parse tactics.\"\n            info[\"verbose\"][\"response\"] = json_response\n            return False, info\n\n        # Sometimes, there are timeout errors.\n        num_attempts = 3\n        info[\"tactics\"] = tactics\n        for i in range(0, num_attempts):\n            if i != 0:\n                info[\"check_solution_attempts\"] = f\"{i+1} (max: {num_attempts})\"\n            try:\n                r, m = check_solution(lean_datum, tactics)\n                if r:\n                    info[\"verbose\"][\"check_solution_message\"] = m\n                else:\n                    info[\"check_solution_message\"] = m\n                return r, info\n            except Exception as e:\n                if i == num_attempts - 1:\n                    error_message = f\"Exception while checking solution: {str(e)}.\"\n                    print(error_message)\n                    info[\"error\"] = error_message\n        return False, info\n</code></pre>"},{"location":"api/#nearai.solvers.LiveBenchSolverStrategy","title":"LiveBenchSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the live bench dataset.</p> Source code in <code>nearai/solvers/livebench_solver.py</code> <pre><code>class LiveBenchSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the live bench dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: str, model: str = \"\", agent: str = \"\", step: str = \"all\"\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.step = step\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"live_bench\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"live_bench\"]\n\n    def get_custom_tasks(self) -&gt; List[dict]:  # noqa: D102\n        return [{\"summary\": \"all\"}]\n\n    @property\n    def evaluated_entry_name(self) -&gt; str:  # noqa: D102\n        name = \"\"\n        if self.agent:\n            name = self.agent_name()\n            if self.model_name != \"\":\n                name += f\"_with_model_{self.model_name}\"\n        else:\n            name = self.model_name\n        assert \"/\" not in name\n        return name.lower()\n\n    @SolverStrategyClassProperty\n    def scoring_method(self) -&gt; SolverScoringMethod:  # noqa: D102\n        return SolverScoringMethod.Custom\n\n    def solve(self, _datum: dict) -&gt; Tuple[bool, dict]:  # noqa: D102\n        if self.step == \"gen_model_answer\":\n            self.gen_model_answer()\n            return True, {}\n        if self.step == \"gen_ground_truth_judgement\":\n            return self.gen_ground_truth_judgement(), {}\n        if self.step == \"show_livebench_results\":\n            return self.show_livebench_results()\n        if self.step == \"all\":\n            self.gen_model_answer()\n            if not self.gen_ground_truth_judgement():\n                return False, {}\n            return self.show_livebench_results()\n        return False, {}\n\n    def gen_model_answer(self) -&gt; None:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step gen_model_answer -----------\")\n        print(\"\")\n        list_of_question_files = glob.glob(f\"{self.dataset_ref}/**/question.jsonl\", recursive=True)\n        for question_file in list_of_question_files:\n            questions = load_questions_jsonl(question_file)\n            bench_name = os.path.dirname(question_file).split(str(self.dataset_ref))[-1]\n            answer_file = _get_answer_file_path(bench_name, self.evaluated_entry_name)\n            print(f\"Questions from {question_file}\")\n            print(f\"Output to {answer_file}\")\n            self.run_eval(questions, answer_file)\n\n    def run_eval(self, questions, answer_file) -&gt; None:  # noqa: D102\n        answer_file = os.path.expanduser(answer_file)\n\n        # Load existing answers\n        existing_answers = set()\n        if os.path.exists(answer_file):\n            print(\n                f\"Answer file {answer_file} exists. Will skip already answered questions. Delete this file if that is not intended.\"  # noqa: E501\n            )\n            with open(answer_file, \"r\") as fin:\n                for line in fin:\n                    answer = json.loads(line)\n                    existing_answers.add(answer[\"question_id\"])\n\n        for question in tqdm(questions):\n            if question[\"question_id\"] in existing_answers:\n                continue\n            choices = self.answer_question(question)\n\n            ans_json = {\n                \"question_id\": question[\"question_id\"],\n                \"answer_id\": shortuuid.uuid(),\n                \"model_id\": self.evaluated_entry_name,\n                \"choices\": choices,\n                \"tstamp\": time.time(),\n            }\n\n            os.makedirs(os.path.dirname(answer_file), exist_ok=True)\n            with open(answer_file, \"a\") as fout:\n                fout.write(json.dumps(ans_json) + \"\\n\")\n\n    def answer_question(self, question) -&gt; List[dict]:  # noqa: D102\n        turns = []\n        session = self.start_inference_session(question[\"question_id\"])\n        for qs in question[\"turns\"]:\n            output = session.run_task(qs)\n            turns.append(output)\n\n        return [{\"index\": 0, \"turns\": turns}]\n\n    def gen_ground_truth_judgement(self) -&gt; bool:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step gen_ground_truth_judgement -----------\")\n        print(\"\")\n        script_path = \"nearai/projects/live_bench/gen_ground_truth_judgement.sh\"\n\n        try:\n            # Run the script without capturing output\n            subprocess.run([\"/bin/bash\", script_path, self.evaluated_entry_name, self.dataset_ref], check=True)\n            return True\n\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while running the script: {e}\")\n            return False\n\n    def show_livebench_results(self) -&gt; Tuple[bool, dict]:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step show_livebench_results -----------\")\n        print(\"\")\n        script_path = \"nearai/projects/live_bench/show_livebench_results.sh\"\n\n        try:\n            # Run the script without capturing output\n            subprocess.run([\"/bin/bash\", script_path, self.evaluated_entry_name], check=True)\n\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while running the script: {e}\")\n            return False, {}\n\n        return self.create_result_dict()\n\n    def read_csv_to_dict(self, file_path) -&gt; dict:  # noqa: D102\n        file_path = os.path.expanduser(file_path)\n        with open(file_path, \"r\") as f:\n            reader = csv.DictReader(f)\n            matching_rows = [row for row in reader if row[\"model\"] == self.evaluated_entry_name]\n            return matching_rows[-1] if matching_rows else {}  # Get the last matching row\n\n    def create_result_dict(self) -&gt; Tuple[bool, dict]:  # noqa: D102\n        tasks_data = self.read_csv_to_dict(_get_all_tasks_csv_file())\n        groups_data = self.read_csv_to_dict(_get_all_groups_csv_file())\n\n        if not tasks_data or not groups_data:\n            return False, {}  # Return None if the model is not found in either file\n\n        result: dict = {\"tasks\": {}, \"groups\": {}}\n\n        for key, value in tasks_data.items():\n            if key != \"model\":\n                result[\"tasks\"][key] = float(value)\n\n        for key, value in groups_data.items():\n            if key != \"model\":\n                result[\"groups\"][key] = float(value)\n\n        return True, result\n\n    def get_evaluation_metrics(self, tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]:  # noqa: D102\n        results: Dict[str, Dict[str, Any]] = tasks_results[-1][1]\n        if len(results) == 0:\n            raise ValueError(\"Cache empty. Rerun the job with --force. Use --step arg to specify a step.\")\n        metrics: Dict[str, Any] = {\"average\": results[\"groups\"][\"average\"]}\n\n        for group, score in results[\"groups\"].items():\n            if group == \"average\":\n                continue\n            metrics[f\"group/{group}\"] = score\n\n        for task, score in results[\"tasks\"].items():\n            metrics[f\"task/{task}\"] = score\n\n        return metrics\n</code></pre>"},{"location":"api/#nearai.solvers.MBPPSolverStrategy","title":"MBPPSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MBPP dataset.</p> Source code in <code>nearai/solvers/mbpp_solver.py</code> <pre><code>class MBPPSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MBPP dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 3\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        prefix = self.dataset_evaluation_name if self.dataset_evaluation_name else \"mbpp\"\n        return f\"{prefix}_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"mbpp\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = MBPPDatum(**datum).model_dump()\n\n        ## Allow LLM to think \"out loud\" for it's answer\n        function_name = get_function_name(datum[\"code\"])\n        example_problems = list(islice(self.dataset_ref[\"prompt\"], self.shots))\n        base_prompt = Template(open(PROMPTS_FOLDER / \"mbpp_verbose_answer.j2\").read(), trim_blocks=True).render(\n            function_name=function_name,\n            example_problems=example_problems,\n            challenge_problem=datum,\n        )\n        response = self.start_inference_session(str(datum[\"task_id\"])).run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"mbpp_extract_answer.j2\").read(), trim_blocks=True\n        ).render(\n            function_name=function_name,\n            answer_text=response,\n        )\n        response = self.start_inference_session(str(datum[\"task_id\"])).run_task(extract_answer_prompt)\n\n        ## Parse the python code\n        python_code_blocks = parse_python_code_block(response) + parse_code_block(response)\n        code = \"\"\n        if len(python_code_blocks) == 0:\n            code = response\n        else:\n            code = python_code_blocks[0]\n\n        ## Evaluate the code\n        try:\n            for test in datum[\"test_list\"] + datum[\"challenge_test_list\"]:\n                test_code = code + \"\\n\" + test\n                if not run_with_timeout(test_code):\n                    return False\n            return True\n        except Exception:\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.MMLUSolverStrategy","title":"MMLUSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MMLU dataset.</p> Source code in <code>nearai/solvers/mmlu_solver.py</code> <pre><code>class MMLUSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MMLU dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 8\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        prefix = self.dataset_evaluation_name if self.dataset_evaluation_name else \"mmlu\"\n        return f\"{prefix}_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"mmlu\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = MMLUDatum(**datum).model_dump()\n\n        choices = [\"A\", \"B\", \"C\", \"D\"]\n        example_problems_indices = list(range(0, 5 * self.shots, 5))\n        example_problems = list(\n            map(\n                lambda d: MMLUDatum(**d).model_dump(),\n                [self.dataset_ref[\"dev\"][i] for i in example_problems_indices],\n            )\n        )\n        base_prompt = Template(open(PROMPTS_FOLDER / \"mmlu_verbose_answer.j2\").read(), trim_blocks=True).render(\n            example_problems=example_problems,\n            challenge_problem=datum,\n            choices=choices,\n        )\n\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"mmlu_extract_answer.j2\").read(), trim_blocks=True\n        ).render(\n            challenge_problem=datum,\n            answer_text=response,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(extract_answer_prompt)\n\n        try:\n            answer = choices.index(response)\n            return bool(answer == datum[\"answer\"])\n        except Exception:\n            print(\"Failed to parse answer\")\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy","title":"SolverStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for solver strategies.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>class SolverStrategy(ABC, metaclass=SolverStrategyMeta):\n    \"\"\"Abstract class for solver strategies.\"\"\"\n\n    def __init__(self, model: str = \"\", agent: str = \"\") -&gt; None:\n        CONFIG.confirm_commands = False\n        self.client_config = CONFIG.get_client_config()\n        self.client = InferenceClient(self.client_config)\n        assert model != \"\" or agent != \"\"\n        self.dataset_evaluation_name = \"\"\n\n        self.provider = \"\"\n        self.model_namespace = \"\"\n        self.model_full_path = \"\"\n        self.model_name = \"\"\n        if model != \"\":\n            self.provider, self.model_full_path = self.client.provider_models.match_provider_model(model)\n            self.provider, namespaced_model = get_provider_namespaced_model(self.model_full_path, self.provider)\n            self.model_namespace = namespaced_model.namespace\n            self.model_name = namespaced_model.name\n\n        self.agent = agent\n        self.agent_params = {\n            \"api_url\": CONFIG.api_url,\n            \"data_source\": \"local_files\",\n            \"temperature\": 0.0,\n            \"record_run\": False,\n            \"verbose\": False,\n            \"change_to_agent_temp_dir\": False,\n        }\n        if self.model_full_path:\n            self.agent_params[\"model\"] = self.model_full_path\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Returns the name of the solver strategy.\"\"\"\n        return type(self).__name__\n\n    @SolverStrategyClassProperty\n    def scoring_method(self) -&gt; SolverScoringMethod:\n        return SolverScoringMethod.TrueOrFalseList\n\n    @abstractmethod\n    def evaluation_name(self) -&gt; str:\n        \"\"\"Returns a unique name for (benchmark, solver) tuple, e.g. 'mbpp' or 'live_bench' or 'mmlu-5-shot'.\"\"\"\n        ...\n\n    @abstractmethod\n    def compatible_datasets(self) -&gt; List[str]:\n        \"\"\"Returns the list of datasets that the solver strategy is compatible with.\"\"\"\n        ...\n\n    def agent_name(self) -&gt; str:\n        \"\"\"Returns agent name that is evaluated.\"\"\"\n        if not self.agent:\n            return \"\"\n        path = Path(self.agent)\n        return path.parent.name\n\n    def agent_version(self) -&gt; str:\n        \"\"\"Returns agent name that is evaluated.\"\"\"\n        if not self.agent:\n            return \"\"\n        path = Path(self.agent)\n        return path.name\n\n    def evaluated_entry_namespace(self) -&gt; str:\n        \"\"\"Returns namespace of a model or agent to be evaluated.\"\"\"\n        if self.agent:\n            path = Path(self.agent)\n            return path.parent.parent.name\n        return self.model_namespace\n\n    def model_provider(self) -&gt; str:\n        \"\"\"Returns model provider.\"\"\"\n        if self.provider != \"\":\n            return self.provider\n        if self.agent != \"\":\n            agent_obj = Agent.load_agent(self.agent, self.client_config)\n            return agent_obj.model_provider\n        return \"\"\n\n    @abstractmethod\n    def solve(self, datum: dict) -&gt; Union[bool, Tuple[bool, Any]]:\n        \"\"\"Solves the task for the given datum.\"\"\"\n        ...\n\n    def get_custom_tasks(self) -&gt; List[dict]:\n        \"\"\"Custom tasks for custom benchmark.\"\"\"\n        if self.scoring_method == SolverScoringMethod.Custom:\n            raise NotImplementedError(\"get_custom_tasks must be implemented for Custom scoring method\")\n        else:\n            raise AttributeError(\"get_custom_tasks is only applicable for Custom scoring method\")\n\n    def get_evaluation_metrics(self, tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]:\n        \"\"\"Given results for all datums, returns evaluation metrics.\n\n        Not used by TrueOrFalseList scoring method.\n        Do not prepend with evaluation_name. If hierarchical, use slashes /.\n        Expected metrics is a dict of scores, e.g.: {\"average\": &lt;val&gt;, \"group/coding\": &lt;val&gt;}.\n        \"\"\"\n        raise NotImplementedError(\"get_evaluation_metrics not implemented\")\n\n    def start_inference_session(self, task_id: str) -&gt; SolverInferenceSession:\n        return SolverInferenceSession(\n            self.agent, self.agent_params, self.model_full_path, self.client, self.evaluation_name()\n        ).start_inference_session(task_id)\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the name of the solver strategy.</p>"},{"location":"api/#nearai.solvers.SolverStrategy.agent_name","title":"agent_name","text":"<pre><code>agent_name() -&gt; str\n</code></pre> <p>Returns agent name that is evaluated.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def agent_name(self) -&gt; str:\n    \"\"\"Returns agent name that is evaluated.\"\"\"\n    if not self.agent:\n        return \"\"\n    path = Path(self.agent)\n    return path.parent.name\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.agent_version","title":"agent_version","text":"<pre><code>agent_version() -&gt; str\n</code></pre> <p>Returns agent name that is evaluated.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def agent_version(self) -&gt; str:\n    \"\"\"Returns agent name that is evaluated.\"\"\"\n    if not self.agent:\n        return \"\"\n    path = Path(self.agent)\n    return path.name\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.compatible_datasets","title":"compatible_datasets  <code>abstractmethod</code>","text":"<pre><code>compatible_datasets() -&gt; List[str]\n</code></pre> <p>Returns the list of datasets that the solver strategy is compatible with.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>@abstractmethod\ndef compatible_datasets(self) -&gt; List[str]:\n    \"\"\"Returns the list of datasets that the solver strategy is compatible with.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.evaluated_entry_namespace","title":"evaluated_entry_namespace","text":"<pre><code>evaluated_entry_namespace() -&gt; str\n</code></pre> <p>Returns namespace of a model or agent to be evaluated.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def evaluated_entry_namespace(self) -&gt; str:\n    \"\"\"Returns namespace of a model or agent to be evaluated.\"\"\"\n    if self.agent:\n        path = Path(self.agent)\n        return path.parent.parent.name\n    return self.model_namespace\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.evaluation_name","title":"evaluation_name  <code>abstractmethod</code>","text":"<pre><code>evaluation_name() -&gt; str\n</code></pre> <p>Returns a unique name for (benchmark, solver) tuple, e.g. 'mbpp' or 'live_bench' or 'mmlu-5-shot'.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>@abstractmethod\ndef evaluation_name(self) -&gt; str:\n    \"\"\"Returns a unique name for (benchmark, solver) tuple, e.g. 'mbpp' or 'live_bench' or 'mmlu-5-shot'.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.get_custom_tasks","title":"get_custom_tasks","text":"<pre><code>get_custom_tasks() -&gt; List[dict]\n</code></pre> <p>Custom tasks for custom benchmark.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def get_custom_tasks(self) -&gt; List[dict]:\n    \"\"\"Custom tasks for custom benchmark.\"\"\"\n    if self.scoring_method == SolverScoringMethod.Custom:\n        raise NotImplementedError(\"get_custom_tasks must be implemented for Custom scoring method\")\n    else:\n        raise AttributeError(\"get_custom_tasks is only applicable for Custom scoring method\")\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.get_evaluation_metrics","title":"get_evaluation_metrics","text":"<pre><code>get_evaluation_metrics(tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]\n</code></pre> <p>Given results for all datums, returns evaluation metrics.</p> <p>Not used by TrueOrFalseList scoring method. Do not prepend with evaluation_name. If hierarchical, use slashes /. Expected metrics is a dict of scores, e.g.: {\"average\": , \"group/coding\": }. Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def get_evaluation_metrics(self, tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]:\n    \"\"\"Given results for all datums, returns evaluation metrics.\n\n    Not used by TrueOrFalseList scoring method.\n    Do not prepend with evaluation_name. If hierarchical, use slashes /.\n    Expected metrics is a dict of scores, e.g.: {\"average\": &lt;val&gt;, \"group/coding\": &lt;val&gt;}.\n    \"\"\"\n    raise NotImplementedError(\"get_evaluation_metrics not implemented\")\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.model_provider","title":"model_provider","text":"<pre><code>model_provider() -&gt; str\n</code></pre> <p>Returns model provider.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>def model_provider(self) -&gt; str:\n    \"\"\"Returns model provider.\"\"\"\n    if self.provider != \"\":\n        return self.provider\n    if self.agent != \"\":\n        agent_obj = Agent.load_agent(self.agent, self.client_config)\n        return agent_obj.model_provider\n    return \"\"\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategy.solve","title":"solve  <code>abstractmethod</code>","text":"<pre><code>solve(datum: dict) -&gt; Union[bool, Tuple[bool, Any]]\n</code></pre> <p>Solves the task for the given datum.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>@abstractmethod\ndef solve(self, datum: dict) -&gt; Union[bool, Tuple[bool, Any]]:\n    \"\"\"Solves the task for the given datum.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#nearai.solvers.SolverStrategyMeta","title":"SolverStrategyMeta","text":"<p>               Bases: <code>ABCMeta</code></p> <p>Metaclass that automatically registers subclasses in the SolverStrategyRegistry.</p> Source code in <code>nearai/solvers/__init__.py</code> <pre><code>class SolverStrategyMeta(ABCMeta):\n    \"\"\"Metaclass that automatically registers subclasses in the SolverStrategyRegistry.\"\"\"\n\n    def __new__(cls, name: str, bases: tuple, namespace: dict) -&gt; Any:\n        new_class = super().__new__(cls, name, bases, namespace)\n        if bases != (ABC,):  # Avoid registering the abstract base class itself\n            SolverStrategyRegistry[new_class.__name__] = new_class  # type: ignore\n        return new_class\n</code></pre>"},{"location":"api/#nearai.solvers.ddot_v0_solver","title":"ddot_v0_solver","text":""},{"location":"api/#nearai.solvers.ddot_v0_solver.DDOTSEnvironment","title":"DDOTSEnvironment","text":"<p>               Bases: <code>Environment</code></p> Source code in <code>nearai/solvers/ddot_v0_solver.py</code> <pre><code>class DDOTSEnvironment(Environment):\n    def __init__(self, agents: List[Agent], problem_id: str, description: str, client):  # noqa: D107\n        self.tdir = TemporaryDirectory()\n        self.hub_client = get_hub_client()\n        thread = self.hub_client.beta.threads.create()\n        super().__init__(\n            self.tdir.name,\n            agents,\n            client,\n            self.hub_client,\n            thread.id,\n            \"todo\",\n            approvals={\"confirm_execution\": lambda _: False},\n        )\n\n        self.problem_id = problem_id\n        self.solved = False\n\n        files = {\n            \".id\": problem_id,\n            \"PROBLEM.txt\": description,\n            \"solution.py\": \"\",\n            \"test.in\": \"\",\n            \"test.sh\": \"#!/bin/bash\\npython3 solution.py &lt; test.in\",\n        }\n        for fname, content in files.items():\n            with open(self.tdir.name + \"/\" + fname, \"w\") as f:\n                f.write(content)\n\n    async def async_submit(self, code: str) -&gt; Tuple[bool, str]:  # noqa: D102\n        submission_id = await submit_problem(self.problem_id, code, Extensions.PYTHON)\n\n        try:\n            await is_output_ready(submission_id)\n        except Exception:\n            print(\"WARNING: Submission took too long to execute on DDOTS\")\n            self.mark_done()\n            return False, \"Submission took too long to execute on the platform\"\n\n        ok = await submission_accepted(submission_id)\n\n        if ok:\n            self.solved = True\n            self.mark_done()\n            return True, \"\"\n\n        output = await get_output(submission_id)\n\n        return False, output\n\n    def submit_python(self, code: str) -&gt; Tuple[bool, str]:\n        \"\"\"Returns True if the submission was accepted, False otherwise.\n\n        The second element of the tuple is the output of the checker if the submission was rejected.\n        \"\"\"\n        return asyncio.run(self.async_submit(code))\n</code></pre>"},{"location":"api/#nearai.solvers.ddot_v0_solver.DDOTSEnvironment.submit_python","title":"submit_python","text":"<pre><code>submit_python(code: str) -&gt; Tuple[bool, str]\n</code></pre> <p>Returns True if the submission was accepted, False otherwise.</p> <p>The second element of the tuple is the output of the checker if the submission was rejected.</p> Source code in <code>nearai/solvers/ddot_v0_solver.py</code> <pre><code>def submit_python(self, code: str) -&gt; Tuple[bool, str]:\n    \"\"\"Returns True if the submission was accepted, False otherwise.\n\n    The second element of the tuple is the output of the checker if the submission was rejected.\n    \"\"\"\n    return asyncio.run(self.async_submit(code))\n</code></pre>"},{"location":"api/#nearai.solvers.ddot_v0_solver.DDOTSV0Solver","title":"DDOTSV0Solver","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for competitive programming problems live on DDOTS.</p> <p>This dataset will run agents in an Agent environment previously prepared.</p> <p>workspace/     .id             -- Id of the problem     PROBLEM.txt     -- Description of the problem</p> <p>The agent should call env.submit_python(code) to submit the code to the DDOTS server.</p> Source code in <code>nearai/solvers/ddot_v0_solver.py</code> <pre><code>class DDOTSV0Solver(SolverStrategy):\n    \"\"\"Solver strategy for competitive programming problems live on DDOTS.\n\n    This dataset will run agents in an Agent environment previously prepared.\n\n    workspace/\n        .id             -- Id of the problem\n        PROBLEM.txt     -- Description of the problem\n\n    The agent should call env.submit_python(code) to submit the code to the DDOTS server.\n\n    \"\"\"\n\n    def __init__(self, dataset_ref: Dataset, agents: str, max_iterations: int, save_snapshots: bool = False):  # noqa: D107\n        client_config = ClientConfig(\n            base_url=CONFIG.nearai_hub.base_url,\n            auth=CONFIG.auth,\n        )\n        self.agents = [Agent.load_agent(agent, client_config) for agent in agents.split(\",\")]\n        self.max_iterations = max_iterations\n\n        date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        rnd_id = random.randint(10**8, 10**9 - 1)\n        self._saved_trajectories = DATA_FOLDER / \"data\" / \"ddots_v0_trajectories\" / f\"{date}_{rnd_id}\"\n        self._saved_trajectories.mkdir(parents=True, exist_ok=True)\n\n        self.save_snapshots = save_snapshots\n        print(\"Saving trajectories to\", self._saved_trajectories)\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"ddots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"ddots_codeforces_small/v0\", \"datasets/ddots_codeforces_medium_A_B/v0\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        problem_id = datum[\"problem_id\"]\n        description = datum[\"description\"]\n\n        client_config = ClientConfig(\n            base_url=CONFIG.nearai_hub.base_url,\n            auth=CONFIG.auth,\n        )\n        client = InferenceClient(client_config)\n        env = DDOTSEnvironment(self.agents, problem_id, description, client)\n        env.write_file(\".solved\", str(False))\n\n        try:\n            env.run(description, max_iterations=self.max_iterations)\n            env.write_file(\".solved\", str(env.solved))\n\n        except Exception as e:\n            print(f\"Error running task: {e}\")\n\n        finally:\n            if self.save_snapshots:\n                snapshot = env.create_snapshot()\n                with open(self._saved_trajectories / f\"{problem_id}.tar.gz\", \"wb\") as f:\n                    f.write(snapshot)\n\n        return env.solved\n</code></pre>"},{"location":"api/#nearai.solvers.gsm8k_solver","title":"gsm8k_solver","text":""},{"location":"api/#nearai.solvers.gsm8k_solver.GSM8KSolverStrategy","title":"GSM8KSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the GSM8K dataset.</p> Source code in <code>nearai/solvers/gsm8k_solver.py</code> <pre><code>class GSM8KSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the GSM8K dataset.\"\"\"\n\n    SHOTS = 8\n\n    def __init__(self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\") -&gt; None:  # noqa: D107\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"gsm8k\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"gsm8k\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        parsed_datum: GSM8KDatum = GSM8KDatum(**datum)\n\n        problem_shots_indices = list(range(0, self.SHOTS))\n        problem_shots = list(\n            map(\n                lambda i: GSM8KDatum(**self.dataset_ref[\"train\"][i]).model_dump(),\n                problem_shots_indices,\n            )\n        )\n\n        session = self.start_inference_session(\"\")\n        session.add_system_message(\n            dedent(\n                \"\"\"\n                    You are a helpful assistant. You're goal is to answer word based math questions.\n                    \"\"\"\n                + \"\\n\\n\"\n                + \"Here are some examples of math questions and their answers:\"\n                + \"\\n\\n\".join([f\"Question: {shot['question']}\\nAnswer: {shot['answer']}\" for shot in problem_shots])\n                + \"\\n\\n\"\n                + \"Now, answer the next question provided in the user prompt. \"\n                + \"Think step by step about how to solve the problem. \"\n                + \"Then, provide the answer.\"\n            )\n        )\n        res_output = session.run_task(parsed_datum.question).strip()\n\n        ## cleanup the output\n        session = self.start_inference_session(\"\")\n        res_refined_output = session.run_task(\n            dedent(\n                f\"\"\"\n                    You are a helpful assistant. You're goal is to answer math questions.\n\n                    You have just answered a math question with the following response:\n\n                    --- BEGIN RESPONSE ---\n                    {res_output}\n                    --- END RESPONSE ---\n\n                    Please refine your answer.\n\n                    Only output the final number *without units* as your answer. Nothing else.\n                    \"\"\"\n            )\n        ).strip()\n        res_refined_output = res_refined_output.replace(\"$\", \"\").replace(\",\", \"\")\n        if \" \" in res_refined_output:\n            res_refined_output = res_refined_output.split(\" \")[0]\n        try:\n            res_refined_output = str(int(res_refined_output))\n        except Exception:\n            pass\n        try:\n            res_refined_output = str(int(float(res_refined_output)))\n        except Exception:\n            pass\n\n        refined_answer = parsed_datum.answer.replace(\"$\", \"\").replace(\",\", \"\")\n        print(res_refined_output, refined_answer)\n        return res_refined_output == refined_answer\n</code></pre>"},{"location":"api/#nearai.solvers.hellaswag_solver","title":"hellaswag_solver","text":""},{"location":"api/#nearai.solvers.hellaswag_solver.HellaswagSolverStrategy","title":"HellaswagSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MMLU dataset.</p> Source code in <code>nearai/solvers/hellaswag_solver.py</code> <pre><code>class HellaswagSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MMLU dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 8\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return f\"hellaswag_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"hellaswag\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = HellaswagDatum(**datum).model_dump()\n\n        choices = [\"A\", \"B\", \"C\", \"D\"]\n        example_problems_indices = list(range(0, 5 * self.shots, 5))\n        example_problems = list(\n            map(\n                lambda d: HellaswagDatum(**d).model_dump(),\n                [self.dataset_ref[\"validation\"][i] for i in example_problems_indices],\n            )\n        )\n        base_prompt = Template(\n            open(PROMPTS_FOLDER / \"hellaswag_verbose_answer.j2\").read(),\n            trim_blocks=True,\n        ).render(\n            example_problems=example_problems,\n            challenge_problem=datum,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"hellaswag_extract_answer.j2\").read(),\n            trim_blocks=True,\n        ).render(\n            challenge_problem=datum,\n            answer_text=response,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(extract_answer_prompt)\n\n        try:\n            answer = choices.index(response)\n            return bool(answer == int(datum[\"label\"]))\n        except Exception:\n            print(\"Failed to parse answer\")\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.lean_solver","title":"lean_solver","text":""},{"location":"api/#nearai.solvers.lean_solver.LeanSolverStrategy","title":"LeanSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy to evaluate against Lean problems.</p> Source code in <code>nearai/solvers/lean_solver.py</code> <pre><code>class LeanSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy to evaluate against Lean problems.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\"\n    ) -&gt; None:\n        super().__init__(model, agent)\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        assert self.dataset_evaluation_name\n        return self.dataset_evaluation_name\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"lean\"]\n\n    def solve(self, datum: dict) -&gt; Tuple[bool, dict]:  # noqa: D102\n        lean_datum = LeanDatum.model_validate(datum)\n        lean_datum.url = load_repository(lean_datum.url)\n\n        info: dict = {}\n        info[\"verbose\"] = {}\n\n        lean_task = LeanTaskInfo(\n            lean_datum.url,\n            lean_datum.commit,\n            lean_datum.filename,\n            lean_datum.theorem,\n            load_theorem(lean_datum),\n        )\n        info[\"verbose\"][\"theorem_raw\"] = lean_task.theorem_raw\n\n        base_prompt = Template(open(PROMPTS_FOLDER / \"lean_answer.j2\").read(), trim_blocks=True).render(\n            url=lean_task.url,\n            commit=lean_task.commit,\n            filepath=lean_task.filename,\n            theorem_name=lean_task.theorem,\n            theorem_raw=lean_task.theorem_raw,\n            begin_marker=BEGIN_MARKER,\n            end_marker=END_MARKER,\n        )\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        json_response = extract_between_markers(response)\n        if not json_response:\n            info[\"error\"] = \"Failed to extract between markers.\"\n            info[\"verbose\"][\"response\"] = response\n            return False, info\n\n        tactics = parse_tactics(json_response)\n        if not tactics:\n            info[\"error\"] = \"Failed to parse tactics.\"\n            info[\"verbose\"][\"response\"] = json_response\n            return False, info\n\n        # Sometimes, there are timeout errors.\n        num_attempts = 3\n        info[\"tactics\"] = tactics\n        for i in range(0, num_attempts):\n            if i != 0:\n                info[\"check_solution_attempts\"] = f\"{i+1} (max: {num_attempts})\"\n            try:\n                r, m = check_solution(lean_datum, tactics)\n                if r:\n                    info[\"verbose\"][\"check_solution_message\"] = m\n                else:\n                    info[\"check_solution_message\"] = m\n                return r, info\n            except Exception as e:\n                if i == num_attempts - 1:\n                    error_message = f\"Exception while checking solution: {str(e)}.\"\n                    print(error_message)\n                    info[\"error\"] = error_message\n        return False, info\n</code></pre>"},{"location":"api/#nearai.solvers.lean_solver.load_theorem","title":"load_theorem","text":"<pre><code>load_theorem(task: LeanDatum) -&gt; str\n</code></pre> <p>Use local copy of the repository.</p> Source code in <code>nearai/solvers/lean_solver.py</code> <pre><code>def load_theorem(task: LeanDatum) -&gt; str:\n    \"\"\"Use local copy of the repository.\"\"\"\n    repo = LeanGitRepo(task.url, task.commit)\n    theorem = Theorem(repo, task.filename, task.theorem)\n    with Dojo(theorem) as (_, state):\n        return state.pp\n</code></pre>"},{"location":"api/#nearai.solvers.livebench_solver","title":"livebench_solver","text":""},{"location":"api/#nearai.solvers.livebench_solver.LiveBenchSolverStrategy","title":"LiveBenchSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the live bench dataset.</p> Source code in <code>nearai/solvers/livebench_solver.py</code> <pre><code>class LiveBenchSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the live bench dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: str, model: str = \"\", agent: str = \"\", step: str = \"all\"\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.step = step\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        return \"live_bench\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"live_bench\"]\n\n    def get_custom_tasks(self) -&gt; List[dict]:  # noqa: D102\n        return [{\"summary\": \"all\"}]\n\n    @property\n    def evaluated_entry_name(self) -&gt; str:  # noqa: D102\n        name = \"\"\n        if self.agent:\n            name = self.agent_name()\n            if self.model_name != \"\":\n                name += f\"_with_model_{self.model_name}\"\n        else:\n            name = self.model_name\n        assert \"/\" not in name\n        return name.lower()\n\n    @SolverStrategyClassProperty\n    def scoring_method(self) -&gt; SolverScoringMethod:  # noqa: D102\n        return SolverScoringMethod.Custom\n\n    def solve(self, _datum: dict) -&gt; Tuple[bool, dict]:  # noqa: D102\n        if self.step == \"gen_model_answer\":\n            self.gen_model_answer()\n            return True, {}\n        if self.step == \"gen_ground_truth_judgement\":\n            return self.gen_ground_truth_judgement(), {}\n        if self.step == \"show_livebench_results\":\n            return self.show_livebench_results()\n        if self.step == \"all\":\n            self.gen_model_answer()\n            if not self.gen_ground_truth_judgement():\n                return False, {}\n            return self.show_livebench_results()\n        return False, {}\n\n    def gen_model_answer(self) -&gt; None:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step gen_model_answer -----------\")\n        print(\"\")\n        list_of_question_files = glob.glob(f\"{self.dataset_ref}/**/question.jsonl\", recursive=True)\n        for question_file in list_of_question_files:\n            questions = load_questions_jsonl(question_file)\n            bench_name = os.path.dirname(question_file).split(str(self.dataset_ref))[-1]\n            answer_file = _get_answer_file_path(bench_name, self.evaluated_entry_name)\n            print(f\"Questions from {question_file}\")\n            print(f\"Output to {answer_file}\")\n            self.run_eval(questions, answer_file)\n\n    def run_eval(self, questions, answer_file) -&gt; None:  # noqa: D102\n        answer_file = os.path.expanduser(answer_file)\n\n        # Load existing answers\n        existing_answers = set()\n        if os.path.exists(answer_file):\n            print(\n                f\"Answer file {answer_file} exists. Will skip already answered questions. Delete this file if that is not intended.\"  # noqa: E501\n            )\n            with open(answer_file, \"r\") as fin:\n                for line in fin:\n                    answer = json.loads(line)\n                    existing_answers.add(answer[\"question_id\"])\n\n        for question in tqdm(questions):\n            if question[\"question_id\"] in existing_answers:\n                continue\n            choices = self.answer_question(question)\n\n            ans_json = {\n                \"question_id\": question[\"question_id\"],\n                \"answer_id\": shortuuid.uuid(),\n                \"model_id\": self.evaluated_entry_name,\n                \"choices\": choices,\n                \"tstamp\": time.time(),\n            }\n\n            os.makedirs(os.path.dirname(answer_file), exist_ok=True)\n            with open(answer_file, \"a\") as fout:\n                fout.write(json.dumps(ans_json) + \"\\n\")\n\n    def answer_question(self, question) -&gt; List[dict]:  # noqa: D102\n        turns = []\n        session = self.start_inference_session(question[\"question_id\"])\n        for qs in question[\"turns\"]:\n            output = session.run_task(qs)\n            turns.append(output)\n\n        return [{\"index\": 0, \"turns\": turns}]\n\n    def gen_ground_truth_judgement(self) -&gt; bool:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step gen_ground_truth_judgement -----------\")\n        print(\"\")\n        script_path = \"nearai/projects/live_bench/gen_ground_truth_judgement.sh\"\n\n        try:\n            # Run the script without capturing output\n            subprocess.run([\"/bin/bash\", script_path, self.evaluated_entry_name, self.dataset_ref], check=True)\n            return True\n\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while running the script: {e}\")\n            return False\n\n    def show_livebench_results(self) -&gt; Tuple[bool, dict]:  # noqa: D102\n        print(\"\")\n        print(\"----------- Step show_livebench_results -----------\")\n        print(\"\")\n        script_path = \"nearai/projects/live_bench/show_livebench_results.sh\"\n\n        try:\n            # Run the script without capturing output\n            subprocess.run([\"/bin/bash\", script_path, self.evaluated_entry_name], check=True)\n\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while running the script: {e}\")\n            return False, {}\n\n        return self.create_result_dict()\n\n    def read_csv_to_dict(self, file_path) -&gt; dict:  # noqa: D102\n        file_path = os.path.expanduser(file_path)\n        with open(file_path, \"r\") as f:\n            reader = csv.DictReader(f)\n            matching_rows = [row for row in reader if row[\"model\"] == self.evaluated_entry_name]\n            return matching_rows[-1] if matching_rows else {}  # Get the last matching row\n\n    def create_result_dict(self) -&gt; Tuple[bool, dict]:  # noqa: D102\n        tasks_data = self.read_csv_to_dict(_get_all_tasks_csv_file())\n        groups_data = self.read_csv_to_dict(_get_all_groups_csv_file())\n\n        if not tasks_data or not groups_data:\n            return False, {}  # Return None if the model is not found in either file\n\n        result: dict = {\"tasks\": {}, \"groups\": {}}\n\n        for key, value in tasks_data.items():\n            if key != \"model\":\n                result[\"tasks\"][key] = float(value)\n\n        for key, value in groups_data.items():\n            if key != \"model\":\n                result[\"groups\"][key] = float(value)\n\n        return True, result\n\n    def get_evaluation_metrics(self, tasks_results: List[Tuple[bool, Any]]) -&gt; Dict[str, Any]:  # noqa: D102\n        results: Dict[str, Dict[str, Any]] = tasks_results[-1][1]\n        if len(results) == 0:\n            raise ValueError(\"Cache empty. Rerun the job with --force. Use --step arg to specify a step.\")\n        metrics: Dict[str, Any] = {\"average\": results[\"groups\"][\"average\"]}\n\n        for group, score in results[\"groups\"].items():\n            if group == \"average\":\n                continue\n            metrics[f\"group/{group}\"] = score\n\n        for task, score in results[\"tasks\"].items():\n            metrics[f\"task/{task}\"] = score\n\n        return metrics\n</code></pre>"},{"location":"api/#nearai.solvers.mbpp_solver","title":"mbpp_solver","text":""},{"location":"api/#nearai.solvers.mbpp_solver.MBPPSolverStrategy","title":"MBPPSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MBPP dataset.</p> Source code in <code>nearai/solvers/mbpp_solver.py</code> <pre><code>class MBPPSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MBPP dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 3\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        prefix = self.dataset_evaluation_name if self.dataset_evaluation_name else \"mbpp\"\n        return f\"{prefix}_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"mbpp\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = MBPPDatum(**datum).model_dump()\n\n        ## Allow LLM to think \"out loud\" for it's answer\n        function_name = get_function_name(datum[\"code\"])\n        example_problems = list(islice(self.dataset_ref[\"prompt\"], self.shots))\n        base_prompt = Template(open(PROMPTS_FOLDER / \"mbpp_verbose_answer.j2\").read(), trim_blocks=True).render(\n            function_name=function_name,\n            example_problems=example_problems,\n            challenge_problem=datum,\n        )\n        response = self.start_inference_session(str(datum[\"task_id\"])).run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"mbpp_extract_answer.j2\").read(), trim_blocks=True\n        ).render(\n            function_name=function_name,\n            answer_text=response,\n        )\n        response = self.start_inference_session(str(datum[\"task_id\"])).run_task(extract_answer_prompt)\n\n        ## Parse the python code\n        python_code_blocks = parse_python_code_block(response) + parse_code_block(response)\n        code = \"\"\n        if len(python_code_blocks) == 0:\n            code = response\n        else:\n            code = python_code_blocks[0]\n\n        ## Evaluate the code\n        try:\n            for test in datum[\"test_list\"] + datum[\"challenge_test_list\"]:\n                test_code = code + \"\\n\" + test\n                if not run_with_timeout(test_code):\n                    return False\n            return True\n        except Exception:\n            return False\n</code></pre>"},{"location":"api/#nearai.solvers.mmlu_solver","title":"mmlu_solver","text":""},{"location":"api/#nearai.solvers.mmlu_solver.MMLUSolverStrategy","title":"MMLUSolverStrategy","text":"<p>               Bases: <code>SolverStrategy</code></p> <p>Solver strategy for the MMLU dataset.</p> Source code in <code>nearai/solvers/mmlu_solver.py</code> <pre><code>class MMLUSolverStrategy(SolverStrategy):\n    \"\"\"Solver strategy for the MMLU dataset.\"\"\"\n\n    def __init__(  # noqa: D107\n        self, dataset_ref: Union[Dataset, DatasetDict], model: str = \"\", agent: str = \"\", shots: int = 8\n    ) -&gt; None:\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n        self.shots = shots\n\n    def evaluation_name(self) -&gt; str:  # noqa: D102\n        prefix = self.dataset_evaluation_name if self.dataset_evaluation_name else \"mmlu\"\n        return f\"{prefix}_{self.shots}shots\"\n\n    def compatible_datasets(self) -&gt; List[str]:  # noqa: D102\n        return [\"mmlu\"]\n\n    def solve(self, datum: dict) -&gt; bool:  # noqa: D102\n        datum = MMLUDatum(**datum).model_dump()\n\n        choices = [\"A\", \"B\", \"C\", \"D\"]\n        example_problems_indices = list(range(0, 5 * self.shots, 5))\n        example_problems = list(\n            map(\n                lambda d: MMLUDatum(**d).model_dump(),\n                [self.dataset_ref[\"dev\"][i] for i in example_problems_indices],\n            )\n        )\n        base_prompt = Template(open(PROMPTS_FOLDER / \"mmlu_verbose_answer.j2\").read(), trim_blocks=True).render(\n            example_problems=example_problems,\n            challenge_problem=datum,\n            choices=choices,\n        )\n\n        response = self.start_inference_session(\"\").run_task(base_prompt)\n\n        ## Extract the answer from the response\n        extract_answer_prompt = Template(\n            open(PROMPTS_FOLDER / \"mmlu_extract_answer.j2\").read(), trim_blocks=True\n        ).render(\n            challenge_problem=datum,\n            answer_text=response,\n            choices=choices,\n        )\n        response = self.start_inference_session(\"\").run_task(extract_answer_prompt)\n\n        try:\n            answer = choices.index(response)\n            return bool(answer == datum[\"answer\"])\n        except Exception:\n            print(\"Failed to parse answer\")\n            return False\n</code></pre>"},{"location":"api/#nearai.tests","title":"tests","text":""},{"location":"api/#nearai.tests.test_provider_models","title":"test_provider_models","text":""},{"location":"api/#nearai.tests.test_provider_models.TestMatchProviderModel","title":"TestMatchProviderModel","text":"<p>               Bases: <code>TestCase</code></p> <p>Unit tests for get_provider_namespaced_model.</p> Source code in <code>nearai/tests/test_provider_models.py</code> <pre><code>class TestMatchProviderModel(unittest.TestCase):\n    \"\"\"Unit tests for get_provider_namespaced_model.\"\"\"\n\n    def __init__(self, method_name=\"runTest\"):  # noqa: D107\n        super().__init__(method_name)\n        self.provider_models = ProviderModels(CONFIG.get_client_config())\n\n    def test_fireworks(self):  # noqa: D102\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"fireworks::accounts/yi-01-ai/models/yi-large\"),\n            (\"fireworks\", \"fireworks::accounts/yi-01-ai/models/yi-large\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"accounts/yi-01-ai/models/yi-large\"),\n            (\"fireworks\", \"fireworks::accounts/yi-01-ai/models/yi-large\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"llama-v3-70b-instruct\"),\n            (\"fireworks\", \"fireworks::accounts/fireworks/models/llama-v3-70b-instruct\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"yi-01-ai/yi-large\"),\n            (\"fireworks\", \"fireworks::accounts/yi-01-ai/models/yi-large\"),\n        )\n\n    def test_hyperbolic(self):  # noqa: D102\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"hyperbolic::StableDiffusion\"),\n            (\"hyperbolic\", \"hyperbolic::StableDiffusion\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n            (\"hyperbolic\", \"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"hyperbolic::Meta-Llama-3.1-70B-Instruct\"),\n            (\"hyperbolic\", \"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n        )\n\n    def test_registry_with_multiple_providers(self):  # noqa: D102\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"llama-3.1-70b-instruct\"),\n            (\"fireworks\", \"fireworks::accounts/fireworks/models/llama-v3p1-70b-instruct\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"llama-3.1-70b-instruct\", provider=\"hyperbolic\"),\n            (\"hyperbolic\", \"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n        )\n        self.assertEqual(\n            self.provider_models.match_provider_model(\"near.ai/llama-3.1-70b-instruct\", provider=\"hyperbolic\"),\n            (\"hyperbolic\", \"hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n        )\n</code></pre>"},{"location":"contributing/","title":"Contribute to <code>nearai</code>","text":"<p>Everyone is welcome to contribute, and we value everybody's contribution. Code contributions are not the only way to help the community. Answering questions, helping others, and improving documentation are also immensely valuable.</p> <p>It also helps us if you spread the word! Reference the library in blog posts about the awesome projects it made possible, or even simply \u2b50\ufe0f the repository to say thank you.</p> <p>This guide was heavily inspired by the huggingface transformers guide to contributing.</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to contribute","text":"<p>There are several ways you can contribute to <code>nearai</code>:</p> <ul> <li>Contribute to <code>nearai</code></li> <li>Ways to contribute</li> <li>Fixing outstanding issues</li> <li>Submitting a bug-related issue or feature request<ul> <li>Did you find a bug?</li> <li>Do you want a new feature?</li> </ul> </li> <li>Contribute Documentation</li> <li>Create a Pull Request<ul> <li>Pull request checklist</li> <li>Sync a forked repository with upstream main</li> </ul> </li> </ul>"},{"location":"contributing/#fixing-outstanding-issues","title":"Fixing outstanding issues","text":"<p>If you notice an issue with the existing code and have a fix in mind, feel free to start contributing and open a Pull Request!</p>"},{"location":"contributing/#submitting-a-bug-related-issue-or-feature-request","title":"Submitting a bug-related issue or feature request","text":"<p>Do your best to follow these guidelines when submitting a bug-related issue or a feature request. It will make it easier for us to come back to you quickly and with good feedback.</p>"},{"location":"contributing/#did-you-find-a-bug","title":"Did you find a bug?","text":"<p><code>nearai</code> is alpha software. This means there is a possibility of encountering issues in the code. With help from users like you who report problems, we can make it more robust and reliable.</p> <p>Before you report an issue, we would really appreciate it if you could make sure the bug was not already reported (use the search bar on GitHub under Issues). Your issue should also be related to bugs in the library itself, and not your code.</p> <p>Once you've confirmed the bug hasn't already been reported, please include the following information in your issue so we can quickly resolve it:</p> <ul> <li>What did you do?</li> <li>What did you expect to happen?</li> <li>What happened instead?</li> <li>Your OS type and version and Python, PyTorch and versions where applicable.</li> <li>A short, self-contained, code snippet that allows us to reproduce the bug in   less than 30s.</li> <li>The full traceback if an exception is raised.</li> <li>Attach any other additional information, like screenshots, you think may help.</li> </ul> <p>To get the OS and software versions automatically, run the following command:</p> <pre><code>uname -a\n</code></pre>"},{"location":"contributing/#do-you-want-a-new-feature","title":"Do you want a new feature?","text":"<p>If there is a new feature you'd like to see in <code>nearai</code>, please open an issue and describe:</p> <ol> <li>What is the motivation behind this feature? Is it related to a problem or frustration with the library? Is it a feature related to something you need for a project? Is it something you worked on and think it could benefit the community?</li> </ol> <p>Whatever it is, we'd love to hear about it!</p> <ol> <li>Describe your requested feature in as much detail as possible. The more you can tell us about it, the better we'll be able to help you.</li> <li>Provide a code snippet that demonstrates the feature usage.</li> <li>If the feature is related to a paper, please include a link.</li> </ol>"},{"location":"contributing/#contribute-documentation","title":"Contribute Documentation","text":"<p>If you discover any errors or omissions in our documentation, please open an issue and describe:</p> <ul> <li>Which explanation or code snippet is incorrect</li> <li>What concept is not clear or missing</li> <li>If you know, what would be the correct explanation or code snippet</li> </ul> <p>If you think you can contribute a fix for the issue, please feel free to open a Pull Request.</p> <p>To preview your changes locally, you will need to install all the dependencies for the documentation, particularly:</p> <ul> <li><code>mkdocs</code></li> <li><code>mkdocs-material</code></li> <li><code>mkdocs-autorefs</code></li> <li><code>mkdocs-minify-plugin</code></li> <li><code>mkdocsstrings</code></li> </ul> <p>All these dependencies can be easily installed through <code>pip</code> or <code>poetry</code>:</p> pippoetry <pre><code>pip install mkdocs mkdocs-material mkdocs-autorefs mkdocs-minify-plugin \"mkdocstrings[python]\" \"mkdocs-material[imaging]\"\n</code></pre> <pre><code>poetry install --with docs\n</code></pre> <p>Then simply test your changes locally using <code>mkdocs serve</code></p> <p>Cairo Graphics</p> <p>If you encounter a problem with <code>cairo</code>, please follow the mkdocs-material Requirements Guide</p>"},{"location":"contributing/#create-a-pull-request","title":"Create a Pull Request","text":"<p>Before writing any code, we strongly advise you to search through the existing PRs or issues to make sure nobody is already working on the same thing. If you are unsure, it is always a good idea to open an issue to get some feedback.</p> <p>You will need basic <code>git</code> proficiency to contribute to <code>nearai</code>. While <code>git</code> is not the easiest tool to use, it has the greatest manual. Type <code>git --help</code> in a shell and enjoy! If you prefer books, Pro Git is a very good reference. We also recommend asking any available AGI to help you with <code>git</code>.</p> <p>Follow the steps below to start contributing:</p> <ol> <li> <p>Fork the repository by    clicking on the Fork button on the repository's page. This creates a copy of the code    under your GitHub user account.</p> </li> <li> <p>Clone your fork to your local disk, and add the base repository as a remote:</p> </li> </ol> <pre><code>git clone git@github.com:&lt;your Github handle&gt;/nearai.git\ncd nearai\ngit remote add upstream https://github.com/nearai/nearai.git\n</code></pre> <ol> <li>Create a new branch to hold your development changes:</li> </ol> <pre><code>git checkout -b a-descriptive-name-for-my-changes\n</code></pre> <p>\ud83d\udea8 Do not work on the <code>main</code> branch!</p> <ol> <li> <p>Set up a development environment (follow steps in the README):</p> </li> <li> <p>Develop the features in your branch.</p> </li> </ol> <p>As you work on your code, you should make sure it functions as intended.</p> <p><code>nearai</code> relies on <code>ruff</code> and <code>mypy</code> to format and type check its source code    consistently. After you make your changes and are ready to PR them, ensure that    your code is formatted and type-checked by running:</p> <pre><code>./scripts/format_check.sh\n</code></pre> <pre><code>./scripts/lint_check.sh\n</code></pre> <pre><code>./scripts/type_check.sh\n</code></pre> <p>Once you're happy with your changes, add the changed files with <code>git add</code> and    record your changes locally with <code>git commit</code>:</p> <pre><code>git add modified_file.py\ngit commit\n</code></pre> <p>Please remember to write good commit    messages to clearly communicate the changes you made!</p> <p>To keep your copy of the code up to date with the original    repository, rebase your branch on <code>upstream/branch</code> before you open a pull request or if requested by a maintainer:</p> <pre><code>git fetch upstream\ngit rebase upstream/main\n</code></pre> <p>Push your changes to your branch:</p> <pre><code>git push -u origin a-descriptive-name-for-my-changes\n</code></pre> <p>If you've already opened a pull request, you'll need to force push with the <code>--force</code> flag. Otherwise, if the pull request hasn't been opened yet, you can just push your changes normally.</p> <ol> <li> <p>Now you can go to your fork of the repository on GitHub and click on Pull Request to open a pull request. Make sure you tick off all the boxes on our checklist below. When you're ready, you can send your changes to the project maintainers for review.</p> </li> <li> <p>It's ok if maintainers request changes, it happens to our core contributors    too! So everyone can see the changes in the pull request, work in your local    branch and push the changes to your fork. They will automatically appear in    the pull request.</p> </li> </ol>"},{"location":"contributing/#pull-request-checklist","title":"Pull request checklist","text":"<ul> <li>The pull request title should summarize your contribution.</li> <li>If your pull request addresses an issue, please mention the issue number in the pull request description to make sure they are linked (and people viewing the issue know you are working on it).</li> <li>To indicate a work in progress please prefix the title with <code>[WIP]</code>. These are useful to avoid duplicated work, and to differentiate it from PRs ready to be merged.</li> <li>Don't add any images, videos and other non-text files that'll significantly weigh down the repository. Instead, reference them by URL.</li> </ul>"},{"location":"contributing/#sync-a-forked-repository-with-upstream-main","title":"Sync a forked repository with upstream main","text":"<p>When updating the main branch of a forked repository, please follow these steps to avoid pinging the upstream repository which adds reference notes to each upstream PR, and sends unnecessary notifications to the developers involved in these PRs.</p> <ol> <li>When possible, avoid syncing with the upstream using a branch and PR on the forked repository. Instead, merge directly into the forked main.</li> <li>If a PR is absolutely necessary, use the following steps after checking out your branch:</li> </ol> <pre><code>git checkout -b your-branch-for-syncing\ngit pull --squash --no-commit upstream main\ngit commit -m '&lt;your message without GitHub references&gt;'\ngit push --set-upstream origin your-branch-for-syncing\n</code></pre>"},{"location":"decentralization/","title":"Decentralization","text":"<p>This document outlines process of progressive decentralization of NEAR AI capabilities.</p> <p>NEAR AI project approaches decentralization in a progressive way, starting with delivering best researcher, developer and user experience possible and progressively decentralizing and enabling privacy and trust features over time.</p> Component Objective Registry Decentralized storage with support of private and encrypted items Training / Fine-tuning Leveraging decentralized set of nodes to train models in a provable way Agent runner Using trusted execution environment to run agents privately and Agent memory Using FHE to do retrieval from encrypted storage Inter-agent communication Peer-to-peer protocol that supports identity,  payments and dispute resolution"},{"location":"inference/","title":"NEAR AI Inference API (OpenAI Compatible)","text":"<p>NEAR AI provides an OpenAI-compatible API for inference, allowing you to easily integrate powerful language models into your applications.</p> <p>This guide covers the basic inference endpoints and how to use them. For recommended usage of NEAR AI inference in agents explore agent examples below or jump to Model Match section.</p> <p>Agent examples are available here: nearai examples, nearai_langchain examples</p>"},{"location":"inference/#getting-started","title":"Getting Started","text":"<ol> <li>Install all dependencies</li> </ol> <p>a. using <code>pip</code>:</p> <pre><code># Create a virtual environment\npython -m venv nearai_env\n\n# Activate the virtual environment\n# On Windows:\n# nearai_env\\Scripts\\activate\n# On macOS and Linux:\nsource nearai_env/bin/activate\n\n# Install the package in editable mode\npip install -e .\n</code></pre> <p>b. using poetry:</p> <pre><code>poetry install\n</code></pre> <ol> <li> <p>Set up authentication:</p> </li> <li> <p>Log in to NEAR AI using the CLI: <code>nearai login</code></p> </li> <li> <p>The auth object will be saved in <code>~/.nearai/config.json</code></p> </li> <li> <p>Import the required libraries and set up the client</p> </li> </ol> <pre><code>import openai\nimport json\nimport os\nimport nearai\n\nhub_url = \"https://api.near.ai/v1\"\n\n# Login to NEAR AI Hub using nearai CLI.\n# Read the auth object from ~/.nearai/config.json\nauth = nearai.config.load_config_file()[\"auth\"]\nsignature = json.dumps(auth)\n\nclient = openai.OpenAI(base_url=hub_url, api_key=signature)\n</code></pre>"},{"location":"inference/#list-models","title":"List Models","text":"<p>To list available models, use the <code>models.list()</code> method:</p> <pre><code>models = client.models.list()\nprint(models)\n</code></pre> <p>Different providers have different models, and different model formats. For example, fireworks has a format <code>fireworks::accounts/&lt;namespace&gt;/models/&lt;model_name&gt;</code>. To get all unique providers, do:</p> <pre><code>providers = set([model.id.split(\"::\")[0] for model in models])\nprint(providers)\n</code></pre>"},{"location":"inference/#create-a-chat-completion","title":"Create a Chat Completion","text":"<p>To create a chat completion, use the <code>chat.completions.create()</code> method. Here's an example:</p> <pre><code>completion = client.chat.completions.create(\n  model=\"fireworks::accounts/fireworks/models/qwen2p5-72b-instruct\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n  ]\n)\n\nprint(completion.choices[0].message.content)\n</code></pre> <p>This will send a request to the specified model with the given messages and return the model's response. The response can be accessed through the <code>choices</code> array in the returned object.</p>"},{"location":"inference/#error-handling","title":"Error Handling","text":"<p>When using the API, it's important to handle potential errors. Here's an example of how to implement basic error handling:</p> <pre><code>try:\n  completion = client.chat.completions.create(\n    model=\"fireworks::accounts/fireworks/models/qwen2p5-72b-instruct\",\n    messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n    ]\n  )\n  print(completion.choices[0].message.content)\nexcept openai.APIError as e:\n  print(f\"An API error occurred: {e}\")\nexcept Exception as e:\n  print(f\"An unexpected error occurred: {e}\")\n</code></pre>"},{"location":"inference/#model-match","title":"Model Match","text":"<p>For agents written in Python, we expect the model name to be given either in <code>metadata.json</code> or <code>env.completion</code> as <code>&lt;optional_namespace&gt;/&lt;model_name&gt;</code>. Examples: - \"llama-v3-70b-instruct\" - \"yi-01-ai/yi-large\"</p> <p>\"model_provider\" may also be optionally specified in <code>metadata.json</code>.</p> <p>Existing provider models are matched against the provided model name, and provider model full path is automatically determined using <code>def match_provider_model()</code> function. This function is executed when: - <code>env.completion()</code> function is called. - (<code>nearai_langchain</code> library) <code>orchestrator.chat_model</code> inference is called.</p>"},{"location":"inference/#additional-features","title":"Additional Features","text":"<p>The NEAR AI Inference API also supports other features such as:</p> <ol> <li>Streaming responses</li> <li>Function calling</li> <li>Custom parameters (temperature, max_tokens, etc.)</li> </ol> <p>For more information on these features, please refer to the full API documentation.</p>"},{"location":"inference/#conclusion","title":"Conclusion","text":"<p>This guide covers the basics of using the NEAR AI Inference API. By following these steps, you should be able to authenticate, list models, and create chat completions. For more advanced usage and detailed information, please refer to the complete API documentation or explore the provided examples.</p>"},{"location":"near_events/","title":"Running Agents Based on Events from the NEAR Blockchain","text":""},{"location":"near_events/#overview","title":"Overview","text":"<p>The NEAR AI HUB monitors the latest blocks from the NEAR blockchain and can trigger agents when it detects <code>EVENT_JSON</code> entries following the <code>nearai</code> standard in transactions.</p>"},{"location":"near_events/#example-of-an-event-log-entry","title":"Example of an Event Log Entry","text":"<pre><code>{\n  \"standard\": \"nearai\",\n  \"version\": \"0.1.0\",\n  \"event\": \"run_agent\",\n  \"data\": [\n    {\n      \"message\": \"Hello from NEAR Blockchain\",\n      \"agent\": \"user.near/agent-name/latest\",\n      \"max_iterations\": null,\n      \"env_vars\": null,\n      \"signer_id\": \"account.near\",\n      \"referral_id\": null,\n      \"request_id\": null,\n      \"amount\": \"0\"\n    }\n  ]\n}\n</code></pre> <p>Example Transaction.</p> <p>When such an event is detected, the agent specified in the <code>agent</code> field (e.g., <code>user.near/agent-name/latest</code>) will be automatically triggered. The agent will receive a JSON string containing the following object as its input:</p> <pre><code>{\n  \"event\": \"run_agent\",\n  \"message\": \"...\",\n  \"receipt_id\": \"...\",\n  // Other fields from the `data` object in the logs.\n}\n</code></pre> <p>To allow your agent to be invoked in this way, add a function that parses the incoming user message as a JSON string. If the required values for <code>event</code> and <code>message</code> are present, it should take the appropriate actions. The agent is not required to trust the data sent by the NEAR AI HUB and can independently verify the blockchain by reading the necessary block based on the <code>receipt_id</code>.</p>"},{"location":"private-ml-sdk/","title":"Private and Verifiable AI","text":"<p>Thanks to a combination of cryptographic techniques and secure hardware, NEAR AI is private and verifiable every step of the way. From agent interactions to model training, our infrastructure ensures data privacy and result integrity throughout the entire process.</p> <p></p> <p>NEAR AI leverages technology from Intel and NVIDIA to keep your data safe and isolated</p> <p>To execute the agent securely, NEAR AI uses Intel TDX technology to create a Confidential Virtual Machine (CVM). This CVM is a virtual machine that runs in an isolated environment within the CPU, where a Docker container containing the agent's code and user data is deployed and executed. </p> <p>For model inference, an encrypted communication channel is established using NVIDIA TEE technology, which creates a Confidential Space within the GPU. The model runs in this isolated environment, protected from external access.</p> <p>It's important to note that the CPU and GPU Confidential Spaces operate in complete isolation from the rest of the system - neither the operating system nor the hypervisor (VM manager) can access the code or data being processed.</p> Want to dive deeper? <p>All technology that powers NEAR AI is open-sourced. If you want to dive deeper and learn more about how this secure and private AI is being developed, check out the Private ML SDK on GitHub. </p>"},{"location":"private-ml-sdk/#how-can-i-trust-it","title":"How Can I Trust It?","text":"<p>Before deployment, NEAR AI verifies both proper hardware configuration and Docker container image hash integrity. This ensures a secure runtime environment and that the exact code being executed matches expectations.</p> <p>After execution, NEAR AI validated security through TEE attestations - cryptographic proofs verify both a secure environment and data/code integrity. </p>"},{"location":"private-ml-sdk/#you-can-view-and-verify-these-attestations-yourself-or-publish-them-on-chain-with-near-protocol-to-allow-anyone-to-verify-them-this-provides-a-cryptographically-verifiable-chain-of-trust-through-the-entire-stack-from-agent-to-inference-to-hardware-all-the-way-down-to-the-certifications-from-the-chip-manufacturers","title":"You can view and verify these attestations yourself or publish them on-chain with NEAR Protocol to allow anyone to verify them. This provides a cryptographically verifiable chain of trust through the entire stack from agent to inference to hardware; all the way down to the certifications from the chip manufacturers.","text":""},{"location":"private-ml-sdk/#how-can-i-use-it","title":"How Can I Use it?","text":"<p>Right now we are beta testing this technology, but soon we will open it to the public for everyone to use it. In the near future, all agents and models will run in this secure environment, ensuring that your data is always safe and the results are correct.</p> <p>Stay tuned for more updates!</p>"},{"location":"vector-stores/","title":"Vector Stores","text":"<p>================</p> <p>\u26a0\ufe0f Warning: This text was generated by NEAR AI using vector store example.</p>"},{"location":"vector-stores/#introduction","title":"Introduction","text":"<p>Vector Stores are a powerful feature in NEAR AI that allows you to store and manage large amounts of data in a vectorized format. This enables efficient searching and retrieval of data, making it ideal for applications such as natural language processing, image recognition, and more.</p>"},{"location":"vector-stores/#creating-a-vector-store","title":"Creating a Vector Store","text":"<p>To create a Vector Store, you can use the <code>client.beta.vector_stores.create</code> method, passing in a name for the store and any additional metadata.</p>"},{"location":"vector-stores/#example","title":"Example","text":"<pre><code>client = openai.OpenAI(base_url=base_url, api_key=json.dumps(auth))\n\n# Create a vector store\nvs = client.beta.vector_stores.create(name=\"example_vector_store\")\nprint(f\"Vector store created: {vs}\")\n</code></pre>"},{"location":"vector-stores/#uploading-files","title":"Uploading Files","text":"<p>To upload files to a Vector Store, you can use the <code>client.files.create</code> method, passing in the file contents and metadata.</p>"},{"location":"vector-stores/#example_1","title":"Example","text":"<pre><code># Upload a file to the vector store\nuploaded_file = client.files.create(\n    file=open(\"example_file.txt\", \"rb\"),\n    purpose=\"assistants\",\n)\nattached_file = client.beta.vector_stores.files.create(\n    vector_store_id=vs.id,\n    file_id=uploaded_file.id,\n)\nprint(f\"File uploaded and attached: {uploaded_file.filename}\")\n</code></pre>"},{"location":"vector-stores/#retrieving-files","title":"Retrieving Files","text":"<p>To retrieve a file from a Vector Store, you can use the <code>client.files.download</code> method, passing in the file ID.</p>"},{"location":"vector-stores/#example_2","title":"Example","text":"<pre><code># Retrieve a file from the vector store\nretrieved_file = client.files.download(uploaded_file.id)\nprint(f\"File retrieved: {retrieved_file}\")\n</code></pre>"},{"location":"vector-stores/#deleting-files","title":"Deleting Files","text":"<p>To delete a file from a Vector Store, you can use the <code>client.files.delete</code> method, passing in the file ID.</p>"},{"location":"vector-stores/#example_3","title":"Example","text":"<pre><code># Delete a file from the vector store\ndeleted_file = client.files.delete(uploaded_file.id)\nprint(f\"File deleted: {deleted_file}\")\n</code></pre>"},{"location":"vector-stores/#searching-the-vector-store","title":"Searching the Vector Store","text":"<p>To search a Vector Store, you can use the <code>client.post</code> method, passing in the search query and any additional metadata.</p>"},{"location":"vector-stores/#example_4","title":"Example","text":"<pre><code># Search the vector store\nsearch_query = \"example search query\"\nsearch_response = client.post(\n    path=f\"{base_url}/vector_stores/{vs.id}/search\",\n    body={\"query\": search_query},\n    cast_to=dict,\n)\nprint(f\"Search results for '{search_query}':\")\nprint(f\"- {search_response}\")\n</code></pre>"},{"location":"vector-stores/#obtaining-llm-responses","title":"Obtaining LLM Responses","text":"<p>To obtain LLM responses using a Vector Store, you can use the <code>inference.query_vector_store</code> method, passing in the Vector Store ID, search query, and any additional metadata.</p>"},{"location":"vector-stores/#example_5","title":"Example","text":"<pre><code>def generate_llm_response(messages, processed_results) -&gt; str:\n    SYSTEM_PROMPT = \"\"\"You're an AI assistant that writes technical documentation. You can search a vector store for \n    information relevant to the user's query. Use the provided vector store results to inform your response, but don't \n    mention the vector store directly.\"\"\"\n\n    vs_results = \"\\n=========\\n\".join(\n        [f\"{result.get('chunk_text', 'No text available')}\" for result in processed_results]\n    )\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        *messages,\n        {\n            \"role\": \"system\",\n            \"content\": f\"User query: {messages[-1]['content']}\\n\\nRelevant information:\\n{vs_results}\",\n        },\n    ]\n    return inference.completions(model=\"qwen2p5-72b-instruct\", messages=messages, max_tokens=16000)\n\n# Get an LLM response using the vector store\nsearch_query = \"example search query\"\nclient_config = ClientConfig(base_url=CONFIG.nearai_hub.base_url, auth=CONFIG.auth)\ninference = InferenceRouter(client_config)\nvector_results = inference.query_vector_store(vs.id, search_query)\nprocessed_results = process_vector_results([vector_results])\nllm_response = generate_llm_response(messages, processed_results)\nprint(llm_response[\"choices\"][0][\"message\"][\"content\"])\n</code></pre> <p>Note: This is just a general example and you may need to modify it to fit your specific use case.</p>"},{"location":"vector-stores/#helpful-links","title":"Helpful links*:","text":"<ul> <li>Load local files into the vector store: vector_store.py</li> <li>Load a GitHub repository into the vector store: vector_store_from_source.py</li> <li>Create this help document: vector_store_build_doc.py</li> </ul> <p>* Helpful links were provided by the editor </p>"},{"location":"agents/quickstart/","title":"NEAR AI Quickstart","text":"<p>In this Quickstart you will learn how to setup NEAR AI and then use it to build &amp; interact with an AI agent in less than one minute. \ud83c\udfc3\u200d\u2642\ufe0f</p> <p>NEAR AI Agents are programs that can act autonomously to solve a task, while adapting and reacting to their environment.  These agents can use various AI models, store data to remember past interactions, communicate with other agents, use tools to  interact with the environment, and much more.</p>"},{"location":"agents/quickstart/#setup","title":"Setup","text":""},{"location":"agents/quickstart/#pre-requisites","title":"Pre-Requisites","text":"<ul> <li>NEAR Account</li> <li>NEAR AI CLI</li> </ul>"},{"location":"agents/quickstart/#installing-near-ai-cli","title":"Installing NEAR AI CLI","text":"piplocal <pre><code>python3 -m pip install nearai\n</code></pre> <pre><code>git clone git@github.com:nearai/nearai.git\ncd nearai\npip install -e .\n</code></pre> Python Version <p>If you do not have python, or your version is not compatible, we recommend that you use miniconda or pyenv to manage your installations, as they both allow you to easily switch between python versions.</p> pyenvconda <pre><code>pyenv install 3.11\npyenv local 3.11 # or use global\n</code></pre> <pre><code>conda create -n myenv python=3.11\nconda activate myenv\n</code></pre>"},{"location":"agents/quickstart/#login-to-near-ai","title":"Login to NEAR AI","text":"<p>To create a new agent, first login with a NEAR Account:</p> <pre><code>nearai login\n</code></pre> Don't have a NEAR Account? <p>If you do not have a NEAR account, you can create one for free using wallets listed at wallet.near.org. </p> <p>If you are unsure of which one to choose, try out Bitte or Meteor Wallet.</p> <p>You'll be provided with a URL to login with your NEAR account.</p> <p>Example:</p> <pre><code>$&gt; nearai login\n\nPlease visit the following URL to complete the login process: https://auth.near.ai?message=Welcome+to+NEAR+AI&amp;nonce=&lt;xyzxyzxyzxyzx&gt;&amp;recipient=ai.near&amp;callbackUrl=http%3A%2F%2Flocalhost%3A63130%2Fcapture\n</code></pre> <p>After successfully logging in, you will see a confirmation screen. Close it and return to your terminal.</p> <p></p> Tip <p>If you have already logged in on <code>near-cli</code>, you know your account's private key, or you have the credentials on another device, you can use the following commands to login:</p> <pre><code>### Login with NEAR Account ID Only\nnearai login --accountId name.near\n\n### Login with Account ID and Private Key\nnearai login --accountId name.near --privateKey key\n\n### Login Remotely (only displays the login URL)\nnearai login --remote\n</code></pre>"},{"location":"agents/quickstart/#create-an-agent","title":"Create an Agent","text":"<p>After logging in, you can create a new agent by running the following command:</p> <p><pre><code>nearai agent create\n</code></pre> You will then be prompted to provide a few details about your agent:</p> <ol> <li>The name of your agent.</li> <li>A short description of your agent.</li> <li>Initial instructions for the agent (which can be edited later).</li> </ol> <p></p> <p>Once you have complete these three prompts, you'll see a summary to verify the information is correct:</p> <p></p> <p>If everything looks good, press <code>y</code> to build your agent. Once complete, you should see a confirmation screen similar to this:</p> <p></p> <p>Here you will find:</p> <ol> <li> <p>Where the agent was created:</p> <p><code>/home_directory/.nearai/regisitry/&lt;your-account.near&gt;/&lt;agent-name&gt;/0.0.1</code></p> </li> <li> <p>Useful commands to get started interacting with it:</p> <pre><code># Run agent locally\nnearai agent interactive &lt;path-to-agent&gt; --local\n\n# Select from a list of agents you created to run locally\nnearai agent interactive --local\n\n# Upload agent to NEAR AI's public registry\nnearai registry upload &lt;path-to-agent&gt;\n</code></pre> </li> </ol> <p>Success! You now have a new AI Agent ready to use!  </p>"},{"location":"agents/quickstart/#agent-files","title":"Agent Files","text":"<p>During the agent creation process, <code>nearai</code> builds your agent in your local AI registry located at:</p> <p><code>/home_directory/.nearai/registry/&lt;your-account.near&gt;/&lt;agent-name&gt;/0.0.1</code> </p> <p>This folder contains two files that define your agent:</p> <ol> <li><code>metadata.json</code>: Contains information / configuration about your agent.</li> <li><code>agent.py</code>: Python code that executes each time your agent receives a prompt.</li> </ol>"},{"location":"agents/quickstart/#metadatajson","title":"<code>metadata.json</code>","text":"<p>This file contains information about your agent including optional configuration for the model it will use, Llama 3.1 70B Instruct being the default. To use a different model, select one from app.near.ai/models and update your JSON file defaults. To use whichever model is the latest default model at app.near.ai, remove the model defaults from your metadata.json.</p> <p>Additionally, you can fine tune and serve a model to fit your specific needs. (See Fine Tuning)</p> metadata.json<pre><code>{\n  \"name\": \"example-agent\",\n  \"version\": \"0.0.1\",\n  \"description\": \"NEAR AI docs example agent ;)\",\n  \"category\": \"agent\",\n  \"tags\": [],\n  \"details\": {\n    \"agent\": {\n      \"defaults\": {\n        \"model\": \"llama-v3p1-70b-instruct\",\n        \"model_provider\": \"fireworks\",\n        \"model_temperature\": 1.0,\n        \"model_max_tokens\": 16384\n      }\n    }\n  },\n  \"show_entry\": true\n}\n</code></pre>"},{"location":"agents/quickstart/#agentpy","title":"<code>agent.py</code>","text":"<p>This file contains the code that executes each time your agent receives a prompt. By default it will use simple instructions provided by the user during the creation process. </p> <p>For more information on how to use the environment object, see The Agent Environment.</p> <p>For additional examples, see the NEAR AI Official Agents or the NEAR AI Public Registry.</p> agent.py<pre><code>from nearai.agents.environment import Environment\n\n\ndef run(env: Environment):\n    # A system message guides an agent to solve specific tasks.\n    prompt = {\"role\": \"system\", \"content\": \"You are a helpful agent that will educate users about NEAR AI.\"}\n\n    # Use the model set in the metadata to generate a response\n    result = env.completion([prompt] + env.list_messages())\n\n    # Store the result in the chat history\n    env.add_reply(result)\n\n    # Give the prompt back to the user\n    env.request_user_input()\n\nrun(env)\n</code></pre>"},{"location":"agents/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you have the basics down, here are some key areas to focus on next that will help you better understand what is possible when building with NEAR AI:</p>"},{"location":"agents/quickstart/#explore-the-registry","title":"Explore the Registry \u2192","text":"<p>The NEAR AI Registry is your hub for agent discovery and collaboration. Browse community-created agents, learn from examples, and share your own creations with others.</p>"},{"location":"agents/quickstart/#master-threads","title":"Master Threads \u2192","text":"<p>Threads power agent execution and interaction. Learn to structure conversations, manage file attachments, and create coordinated multi-agent interactions - all within organized conversation threads.</p>"},{"location":"agents/quickstart/#explore-the-environment","title":"Explore the Environment \u2192","text":"<p>The environment object unlocks NEAR AI's powerful features:</p> <ul> <li>Create natural conversations with advanced message handling</li> <li>Leverage AI models for intelligent decision-making</li> <li>Enable agent-to-agent communication</li> <li>Extend capabilities with custom tools</li> </ul>"},{"location":"agents/quickstart/#learn-about-secrets","title":"Learn About Secrets \u2192","text":"<p>Keep your agent secure with proper secrets management. Store API keys safely and connect to external services with confidence.</p>"},{"location":"agents/registry/","title":"Agent Registry: Finding and Publishing Agents","text":"<p>NEAR AI agents can be deployed and hosted in a common registry, allowing the community to share their creations. This registry is used by the NEAR AI Developer Hub to store and serve agents.</p> <p>Let's take a look at how we can navigate this registry, download agents, and contribute our own agents to the ecosystem.</p> <p>Note</p> <p>The agent registry is backed by an S3 bucket with metadata stored in a database.</p>"},{"location":"agents/registry/#finding-an-agent","title":"Finding an Agent","text":"<p>There are two main ways to navigate the agent registry to discover agents: </p> <ul> <li>NEAR AI Developer Hub</li> <li>NEAR AI CLI</li> </ul> <p>For the rest of this guide, we will use the CLI to find and deploy agents. </p> <p>Tip</p> <p>Refer to the Quickstart Guide to learn how to install the CLI and login to the AI Developer Hub.</p>"},{"location":"agents/registry/#view-all-agents","title":"View all agents","text":"<p>To view all agents with <code>nearai</code> CLI, run:</p> <pre><code>nearai registry list --category agent\n</code></pre> Example Output <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 entry                           \u2502 description             \u2502 tags  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 zavodil.near/ai16z-docs/1.03    \u2502 AI agent with AI16Z ... \u2502 agent \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 flatirons.near/common-tool...   \u2502 A library of common ..  \u2502 llama \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 jayzalowitz.near/example_a...   \u2502 Example agent           \u2502       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ...                             \u2502 ...                     \u2502 ...   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"agents/registry/#filtering-agents","title":"Filtering Agents","text":"<p>You can further filter the agents with two flags:</p> <ul> <li><code>--namespace</code> : The developer that created it</li> <li><code>--tags</code>: Any tags that were added to the agent</li> </ul> <p>For example, to find all agents created by <code>gagdiez.near</code> with the tag <code>template</code>, run:</p> <pre><code>nearai registry list  --category agent \\\n                      --namespace gagdiez.near \\\n                      --tags template \\\n                      --show_all\n</code></pre> <p>Tip</p> <p>You can use the <code>info</code> command to get more details about a specific agent, for example:</p> <pre><code>nearai registry info gagdiez.near/hello-ai/latest\n</code></pre>"},{"location":"agents/registry/#downloading-an-agent","title":"Downloading an Agent","text":"<p>Once you find an agent that you would like to download, use the <code>download</code> command to save it locally. Agent details are passed in the following format:</p> <pre><code>nearai registry download &lt;account.near&gt;/&lt;agent_name&gt;/&lt;version&gt;\n</code></pre> <p>The <code>version</code> can be a specific version number, or <code>latest</code> to download the most recent version.</p> <p>Example: </p> <pre><code># Download a hello world agent\nnearai registry download gagdiez.near/hello-ai/latest\n</code></pre> <p>This command saves the agent locally in <code>.nearai/registry</code> under your home directory.</p> <p>The example above would save to: <code>~/.nearai/registry/gagdiez.near/hello-ai/latest</code>.</p> <p>Tip</p> <p>The <code>--force</code> flag allows you to overwrite the local agent with the version from the registry.</p>"},{"location":"agents/registry/#uploading-an-agent","title":"Uploading an Agent","text":"<p>If you created an agent and would like to share it with others, you can upload it to the registry. To upload an agent, you must be logged in.</p> <p>The <code>upload</code> command requires the path to the agent folder stored locally, for example:</p> <pre><code>nearai registry upload ~/.nearai/registry/&lt;your-account.near&gt;/&lt;agent_folder&gt;\n</code></pre> <p>The folder must contain:</p> <ul> <li><code>agent.py</code>: Agent logic</li> <li><code>metadata.json</code>: Agent information (ex: description, tags, and model, etc.)</li> </ul> Example <code>metadata.json</code> file metadata.json<pre><code>{\n\"name\": \"hello-ai\",\n\"version\": \"0.0.1\",\n\"description\": \"A friendly agent\",\n\"category\": \"agent\",\n\"tags\": [],\n\"details\": {\n  \"agent\": {\n    \"defaults\": {\n      \"model\": \"llama-v3p1-70b-instruct\",\n      \"model_provider\": \"fireworks\",\n      \"model_temperature\": 1.0,\n      \"model_max_tokens\": 16384\n    }\n  }\n},\n\"show_entry\": true\n}\n</code></pre> <p>Tags</p> <p>Remember to add tags to your agent to make it easier for others to find it in the registry, for example:</p> <pre><code>{ \"tags\": [\"travel\", \"assistant\", \"vacation\"] }\n</code></pre> <p>Danger</p> <p>All files in this folder will be uploaded to the registry which is PUBLIC! Make sure you are not including any sensitive data.</p> <p>Warning</p> <p>You can't remove or overwrite a file once it's uploaded, but you can hide the entire agent by setting the <code>\"show_entry\": false</code> field in the <code>metadata.json</code> file</p>"},{"location":"agents/running/","title":"Running an Agent","text":"<p>Agents can be run locally or remotely. When running locally, you can run them interactively or as a task. When running remotely, you can use the NEAR AI API to run them.</p>"},{"location":"agents/running/#running-the-agent-locally","title":"Running the Agent Locally","text":"<p>You can execute it in two different ways: interactively or as a task.</p> <p>Know that you can also run agents directly on the Web AI Hub, you don't need to download an agent if you just want to see how they work.</p> <p>Always Review the Code</p> <p>Agents can execute arbitrary code on your machine, so please always review the agent's code before running it!</p> <p>By default, you will find the agent's code on the local directory <code>~/.nearai/registry</code>, there, check the agent's <code>agent.py</code> file either by using the <code>cat</code> command or opening it in a text editor.</p> <pre><code># Checking gagdiez.near/hello-ai/latest code\ncd ~/.nearai/registry/gagdiez.near/hello-ai/latest\ncat agent.py\n</code></pre>"},{"location":"agents/running/#interactive-run","title":"Interactive Run","text":"<p>Interactive runs execute the agent on a loop, allowing you to chat with it interactively until you decide to exit (using the <code>exit</code> command), or quit the session using <code>ctrl+c</code>.</p> <pre><code># Download the agent from the registry\nnearai registry download gagdiez.near/hello-ai/latest\n\n# Running the agent by absolute path\nnearai agent interactive ~/.nearai/registry/gagdiez.near/hello-ai/latest --local\n</code></pre>"},{"location":"agents/running/#running-as-a-task","title":"Running as a Task","text":"<p>When running an agent as a task, we simply provide an input and let the agent execute it without any user interaction.</p> <pre><code>nearai agent task ~/.nearai/registry/gagdiez.near/hello-ai/latest \"write a poem about the sorrow of losing oneself, but end on a positive note\" --local\n</code></pre>"},{"location":"agents/running/#running-the-agent-remotely","title":"Running the Agent Remotely","text":"<p>Agents can be run through the <code>/thread/runs</code>, <code>/thread/{thread_id}/runs</code> or  <code>/agent/runs</code> endpoints. The /thread syntax matches the OpenAI / LangGraph API. The /agent syntax is NEAR AI specific.</p> <p>You will need to pass a signed message to authenticate. This example uses the credentials written by <code>nearai login</code> to your <code>~/.nearai/config.json</code> file.</p> <pre><code>auth_json=$(jq -c '.auth' ~/.nearai/config.json);\n\ncurl \"https://api.near.ai/v1/threads/runs\" \\\n      -X POST \\\n      --header 'Content-Type: application/json' \\\n      --header \"Authorization: Bearer $auth_json\" \\\n-d @- &lt;&lt;'EOF'\n  {\n    \"agent_id\": \"flatirons.near/xela-agent/5.0.1\",\n    \"new_message\":\"Build a backgammon game\",\n    \"max_iterations\": \"1\"\n  }\nEOF\n</code></pre> <p>The full message will look like this. A <code>thread_id</code> param can also be passed to continue a previous conversation.  <pre><code>curl \"https://api.near.ai/v1/threads/runs\" \\\n      -X POST \\\n      --header 'Content-Type: application/json' \\\n      --header 'Authorization: Bearer {\"account_id\":\"your_account.near\",\"public_key\":\"ed25519:YOUR_PUBLIC_KEY\",\"signature\":\"A_REAL_SIGNATURE\",\"callback_url\":\"https://app.near.ai/\",\"message\":\"Welcome to NEAR AI Hub!\",\"recipient\":\"ai.near\",\"nonce\":\"A_UNIQUE_NONCE_FOR_THIS_SIGNATURE\"}' \\\n-d @- &lt;&lt;'EOF'\n  {\n    \"agent_id\": \"flatirons.near/xela-agent/5.0.1\",\n    \"thread_id\": \"a_previous_thread_id\",\n    \"new_message\":\"Build a backgammon game\", \n    \"max_iterations\": \"2\"\n  }\nEOF\n</code></pre></p> Remote results <p>The results of the /agent/runs endpoints are either an error or the resulting thread_id.</p> <p>\"thread_579e1cf3f42742c785218106\"</p> <p>Threads follow the OpenAI / LangGraph api standard. <code>/threads/{thread_id}/messages</code> will return the messages on the thread. See the full NEAR AI OpenAPI spec here: https://api.near.ai/openapi.json</p>"},{"location":"agents/secrets/","title":"Hub Secrets (WIP)","text":"<p>Secrets enhance the agent framework by allowing both the agent author and the agent user to send private information to the runner, which is managed and executed by us.</p> <ul> <li>Reading Secrets: To read secrets, user authentication via an NEAR account is required.</li> <li>Secrets Distribution: Secrets are only provided to our runner.</li> <li>Secrets Encryption: Secrets are encrypted in the database using a master key.</li> </ul> <p>The agent has two types of input variables: agent variables and user variables. Each type has public and secret variables.</p>"},{"location":"agents/secrets/#agent-public-vars","title":"Agent public vars","text":"<p><code>metadata.json/details/env_vars</code> - Provided by the agent author. These are publicly available parameters that can be modified during updates or forks.  For example: <code>api_url</code>.</p>"},{"location":"agents/secrets/#user-public-vars","title":"User public vars","text":"<p><code>env_vars</code> - Provided by the user via CLI or URL parameter.  For example: <code>refId</code> in https://app.near.ai/agents/casino.near/game/1?refId=ad.near</p> <p>We add secrets (which can be added for all versions of the agent or for a specific one).</p>"},{"location":"agents/secrets/#agent-private-vars","title":"Agent private vars","text":"<p>The agent author can add a secret for their agent.  For example: <code>Github_API_Token</code>.</p>"},{"location":"agents/secrets/#user-private-vars","title":"User private vars","text":"<p>The user can add a secret for a specific agent (if required by the agent author).  For example: <code>private_key</code>.</p> <p>In the end, all these data end up in env_vars as a single key-value object. If multiple agents are running, each agent only sees its own secrets.</p>"},{"location":"agents/secrets/#priority-of-records","title":"Priority of Records:","text":"<ul> <li>agent_vars: Agent variables from metadata have a lower priority than agent secrets.</li> <li>user_vars: User variables from URL/CLI have a higher priority than user secrets.</li> <li>final vars: User variables have a higher priority than agent variables.</li> </ul>"},{"location":"agents/secrets/#current-endpoints","title":"Current endpoints","text":"<ul> <li>api-url/v1/get_user_secrets (GET)</li> <li>api-url/v1/create_hub_secret (POST)</li> <li>api-url/v1/remove_hub_secret (POST)</li> </ul>"},{"location":"agents/threads/","title":"Threads","text":"<p>Every agent execution happens within a conversation thread, which is isolated from other threads. Threads allow agents to maintain a message history and persist files in time so the user can continue the conversation later.</p> <p>Info</p> <p>You can find how agents persist messages and files in the Environment: Messages &amp; Files section.</p>"},{"location":"agents/threads/#starting-a-thread","title":"Starting a Thread","text":"<p>If we start an agent without specifying an existing thread, a new thread is created. Let's try this by executing an agent using the interactive mode:</p> <pre><code>nearai agent interactive ~/.nearai/registry/&lt;your-account.near&gt;/hello-ai/0.0.1 --local\n\n&gt; Hello, my name is Guille, please remember it\n\n# Example Output:\n# ...\n# thread_id: thread_43c64803c0a948bc9a8eb8e8\n\n# Assistant: Nice to meet you, Guille! I've made a note of your name, so feel free to ask me anything or start a conversation, and I'll be sure to address you by your name throughout our chat. How's your day going so far, Guillermo?\n</code></pre> <p>We can see in the output that a new <code>thread_id</code> - <code>thread_43c64803c0a948bc9a8eb8e8</code> - was created for this conversation.</p>"},{"location":"agents/threads/#resuming-a-thread","title":"Resuming a Thread","text":"<p>If we want to resume a conversation thread with an agent, we can specify the thread ID when running the agent:</p> <pre><code>nearai agent interactive ~/.nearai/registry/&lt;your-account.near&gt;/hello-ai/0.0.1 --local --thread_id thread_43c64803c0a948bc9a8eb8e8\n\n&gt; What is my name?\n\n# Assistant: Your name is Guille\n</code></pre>"},{"location":"agents/threads/#messages-and-files","title":"Messages and Files","text":"<p>Agents can access and add messages and files on each thread, learn more about it in the Environment: Messages &amp; Files section.</p>"},{"location":"agents/env/calling_other_agents/","title":"Calling another agent","text":"<p>Agents can call other agents to interact with them using the <code>run_agent</code> method. To call an agent, provide the agent's id. Optionally, a query can be passed to the agent.</p> <pre><code>```python\nresult = env.run_agent(\"travel.primitives.near/trip-organizer/latest\", query=\"Plan a two-day trip to Buenos Aires\")\nprint(result)\n\n# thread_312f2ea5e42742c785218106\n</code></pre> <p>The result of the <code>run_agent</code> method is a string containing the thread ID where the external agent executed.</p> <p>Shared Environment</p> <p>With the <code>SAME</code> and <code>FORK</code> thread modes below the agent being called will receive the thread,  meaning it can access all the messages and files from the current conversation.  Moreover, the called agent will be able to add messages and files to the current thread.</p>"},{"location":"agents/env/calling_other_agents/#thread-mode","title":"Thread Mode","text":"<p>The <code>run_agent</code> method has an optional <code>thread_mode</code> parameter to control whether the called agent should  write to: the same thread (ThreadMode.SAME), a forked thread (ThreadMode.FORK), or a child thread (ThreadMode.CHILD). By default, <code>thread_mode</code> is set to <code>ThreadMode.FORK</code>. <code>ThreadMode.SAME</code> causes the called agent to write to the current thread. <code>ThreadMode.FORK</code> creates a new thread for the called agent and copies all messages and files from the current thread. <code>ThreadMode.CHILD</code> creates a new thread for the called agent and copies only the value of the <code>query</code> parameter to the new thread.</p>"},{"location":"agents/env/calling_other_agents/#run-mode-experimental","title":"Run Mode (Experimental)","text":"<p>The <code>run_agent</code> method has an optional <code>run_mode</code> parameter to control whether the calling agent should be called again after the called agent finishes. By default, <code>run_mode</code> is set to <code>RunMode.SIMPLE</code>. <code>RunMode.SIMPLE</code> calls the agent once. <code>RunMode.WITH_CALLBACK</code> calls the calling agent again after the called agent finishes. This is designed to be used with  <code>ThreadMode.CHILD</code>.</p> <p>To use RunMode.WITH_CALLBACK, the calling agent MUST check whether it was called on the parent thread or the child thread. This can be done by fetching the current thread with <code>env.get_thread()</code> and checking whether it has a <code>parent_thread_id</code>.</p> <p>```python</p> <p></p> <p>When a thread is forked, the agent we are calling will work on a copy of the thread, meaning that they have access to all files and messages created so far, but any message or file they create will be part of their own thread.</p> <p>With ThreadMode.SAME, the called agent will work in the same thread as the current agent, meaning that they have access to all files and messages created so far, and any message or file they create will be part of the current thread.</p>"},{"location":"agents/env/calling_other_agents/#schedule-a-run","title":"Schedule a run","text":"<p>Agents can schedule a run for a later time using the <code>schedule_run</code> method. The scheduled run will be executed  by the scheduler using hub credentials.</p>"},{"location":"agents/env/inference/","title":"Inference","text":"<p>The <code>completion</code> method is used to run a prompt on a specific model, using a specific provider.</p> <p>If only the prompt is provided, the inference will be run on the model and provider specified in the agent's metadata.</p> <pre><code>messages = env.list_messages()\nresult = env.completion(messages)\n\nprint(\"Messages:\", messages)\nprint(\"Result:\", result)\n</code></pre> Example Output <pre><code>Messages: [{'id': 'msg_1149aa85884b4fe8abc7d859', 'content': 'Hello', 'role': 'user'}]\n\nResult: Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n</code></pre>"},{"location":"agents/env/inference/#generating-an-image","title":"Generating an Image","text":"<p>The <code>generate_image</code> method is used to generate an image based on the provided description.</p> <pre><code>description = \"A puppy in the garden\"\nimage = env.generate_image(description)\n\n# Extract the base64 data from the first item in `data`\nb64_data = image.data[0].b64_json\n\n# Decode the base64 image data\nimage_data = base64.b64decode(b64_data)\n\n# Write the image file\nimage_file = env.write_file(\"puppy_image.png\", image_data)\n</code></pre> <p>Agent Example</p> <p>Check out this Agent example to learn how to use <code>generate_image</code> in your AI agent logic.</p>"},{"location":"agents/env/inference/#overriding-the-default-model","title":"Overriding the Default Model","text":"<p>To run the inference on a model different from the default one, you can pass the <code>MODEL</code> or <code>PROVIDER::MODEL</code> as second argument:</p> <pre><code>messages = env.list_messages()\nresult = env.completion([prompt] + messages, \"fireworks::qwen2p5-72b-instruct\")\n</code></pre> Example Output <pre><code>Messages: [{'id': 'msg_1149aa85884b4fe8abc7d859', 'content': 'Hello', 'role': 'user'}]\n\nResult: Hello! How can I assist you today? Is there something specific you'd like to talk about or any questions you have?\n</code></pre> <p>Tip</p> <p><code>completions</code>: returns the full llm response for more control.</p> <p>Using Models Locally: LangChain / LangGraph</p> <p>The example agent langgraph-min-example has metadata that specifies the <code>langgraph-0-1-4</code> framework to run on langgraph version 1.4. In addition, the agent.py code contains an adaptor class, <code>AgentChatModel</code> that maps LangChain inference operations to <code>env.completions</code> calls.</p>"},{"location":"agents/env/messages_files/","title":"Messages and Files","text":"<p>Agents interact with the users through messages, and can also access and create files. This page provides an overview of how agents can work with messages and files.</p> Quick Overview <ul> <li>Each run of an agent is executed in a separate thread, which contains messages and files.</li> <li>The agent can access the messages in the current thread using <code>env.list_messages()</code>.</li> <li>The agent can save temporary files to track the progress of a task.</li> <li>By default, the entire message history is stored in a file named <code>chat.txt</code>. The agent can add messages there by using <code>env.add_reply()</code>.</li> <li>During its operation, the agent creates a file named <code>.next_agent</code>, which stores the role of the next participant expected in the dialogue (either <code>user</code> or <code>agent</code>) during the next iteration of the loop. The agent can control this value using <code>env.set_next_actor()</code>.</li> <li>The agent can use local imports from the home folder or its subfolders. It is executed from a temporary folder within a temporary environment.</li> </ul>"},{"location":"agents/env/messages_files/#thread-messages","title":"Thread Messages","text":"<p>The environment provides methods for agents to access and interact with the messages in the current thread. Messages are stored in a list, with each message containing an <code>id</code>, <code>content</code>, and <code>role</code> field.</p>"},{"location":"agents/env/messages_files/#accessing-messages","title":"Accessing Messages","text":"<p>Agents can access the messages from the current thread using the <code>list_messages</code> method:</p> agent.py<pre><code>def run(env: Environment):\n  messages = env.list_messages()\n  print(messages)\n</code></pre> Example Output <pre><code>[{'id': 'msg_9b676ae4ad324ca58794739d', 'content': 'Hi', 'role': 'user'},\n  {'id': 'msg_58693367bcee42669a85cb69', 'content': \"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\", 'role': 'assistant'},\n  {'id': 'msg_16acda223c294213bc3c814e', 'content': 'help me decide how to decorate my house!', 'role': 'user'}]\n</code></pre>"},{"location":"agents/env/messages_files/#adding-messages","title":"Adding Messages","text":"<p>Agents can add new messages to the thread using the <code>add_reply</code> method:</p> agent.py<pre><code>def run(env: Environment):\n  env.add_reply(\"I have finished\")\n</code></pre>"},{"location":"agents/env/messages_files/#files","title":"Files","text":"<p>Agents have access to two types of files through the environment:</p> <ol> <li>Those created within the current conversation thread</li> <li>Those uploaded with the agent to the registry</li> </ol>"},{"location":"agents/env/messages_files/#storing-data","title":"Storing Data","text":"<p>To create a new file in the thread we can use the <code>write_file</code> method from the environment:</p> agent.py<pre><code>def run(env: Environment):\n  env.write_file('file.txt', 'hello thread')\n</code></pre> Where is the file stored? <p>When running the agent locally, a temporary folder will be created to store each thread data. We can check exactly where the file is stored by using the <code>python debugger</code>:</p> agent.py<pre><code>def run(env: Environment):\n  env.write_file('file.txt', 'hello thread')\n  import ipdb; ipdb.set_trace()  # Call the ipdb debugger\n</code></pre> <p>After running the agent, we will be dropped into the debugger, where we can check the current working directory:</p> <pre><code>nearai agent interactive ~/.nearai/registry/&lt;your-account.near&gt;/hello-ai/0.0.1 --local --thread_id thread_43c64803c0a948bc9a8eb8e8\n\nipdb&gt; import os; os.getcwd() # Check the current working directory\n'/private/var/folders/v6/pw4e3e3r5t6h8i9oihtd9d7d1234df/T/agent_7e312s678b987sa4vc4s2zxs2s1w1345'\n</code></pre> <p>We can see that the current working directory is a temporary folder. Go ahead and start the <code>agent</code> again without the <code>--thread_id</code> parameter, you will see that the working directory changes.</p>"},{"location":"agents/env/messages_files/#accessing-files","title":"Accessing Files","text":"<p>To list the files in the thread storage, we can use the <code>list_files_from_thread</code> method from the environment:</p> agent.py<pre><code>def run(env: Environment):\n  files = env.list_files_from_thread()\n  content = env.read_file('file.txt')\n\n  print('Files:', files)\n  print('Content of file.txt:', content)\n</code></pre> Example Output <pre><code>Files [FileObject(id='file_31aab645e3214a13b402e321', bytes=12, created_at=1734733634, filename='file.txt', object='file', purpose='assistants', status='uploaded', status_details='File information retrieved successfully')]\n\nContent of file.txt hello thread\n</code></pre>"},{"location":"agents/env/messages_files/#logging","title":"Logging","text":"<p>You can turn on agent logging by passing an environment variable of <code>DEBUG</code> with a value of true.  In the UI this is set on the Run page of an agent while logged in as the agent author. Once this is set,  logs from either of the methods below will be written to the thread. The 'show logs' button (next to send message) toggles whether the logs show in the thread.</p> <ul> <li><code>add_system_log</code>: adds a system or environment log that is then saved into \"system_log.txt\".</li> <li><code>add_agent_log</code>: any agent logs may go here. Saved into \"agent_log.txt\".</li> </ul>"},{"location":"agents/env/near/","title":"Near","text":""},{"location":"agents/env/near/#interacting-with-near-blockchain","title":"Interacting with NEAR Blockchain","text":"<p>The NEAR AI toolkit provides environment methods for interacting with the NEAR blockchain. This framework is designed for seamless integration using a private RPC when running an agent on a NEAR AI hosted runner.</p>"},{"location":"agents/env/near/#features","title":"Features","text":"<ul> <li>Private RPC Integration: NEAR AI provides a private RPC for agents to interact with the NEAR blockchain. This private RPC is optimized for both read operations (e.g., querying contract states) and write operations (e.g., sending transactions or modifying contract state). It ensures secure and reliable communication between hosted agents and the NEAR network, reducing latency and improving overall performance.</li> <li>Retry Mechanism: Both view and call methods include a robust retry mechanism to handle transient network or RPC errors.</li> </ul>"},{"location":"agents/env/near/#setting-up-the-near-account","title":"Setting Up the Near Account","text":"<pre><code>near = env.set_near(account_id, private_key)\n</code></pre> <p>This creates you an Account object from the <code>py-near</code> Python library. More details: py-near Account</p> <p>Important</p> <p>Ensure that the <code>account_id</code>and <code>private_key</code> are never exposed in plain text within the agent's code. We recommend using secrets to handle these credentials securely.</p> <p>Parameters: - <code>account_id</code>: The NEAR account ID (e.g., \"example.near\") that will act as the account for interactions - <code>private_key</code>: The private key associated with the account_id - <code>rpc_addr</code>: (Optional) A custom RPC address for connecting to the NEAR network. If not provided, the default NEAR RPC address is used.</p> <ul> <li>Example: <pre><code>near = env.set_near(\"account.near\", \"ed25519:3ABCD...XYZ\")\n</code></pre></li> </ul> <p>Once called, the <code>near</code> object is ready for use. Note that <code>near.view</code> can be used without providing to <code>env.set_near()</code> <code>account_id</code> or <code>private_key</code>.</p>"},{"location":"agents/env/near/#near-view-method","title":"NEAR VIEW Method","text":"<p>Performs a read-only operation on the NEAR blockchain. This is used to query the state of a contract without modifying it. Examples include retrieving contract states, or querying other read-only data.</p> <p>The result object contains the transaction details, including the logs and block hash, and any returned values. For more details on the format of the result object, refer to the py-near documentation.</p> <pre><code>near = env.set_near()\nawait near.view(\n    contract_id: str,\n    method_name: str,\n    args: dict,\n    block_id: Optional[int] = None,\n    threshold: Optional[int] = None,\n    max_retries: int = 3\n)\n</code></pre> <p>Parameters: - <code>contract_id</code>: The NEAR account ID of the smart contract you want to query. - <code>method_name</code>: The name of the view method to call on the contract. - <code>args</code>: A dictionary of arguments to pass to the view method. - <code>block_id</code>: (Optional) The block ID to query. Defaults to the latest block. - <code>threshold</code>: (Optional) A threshold parameter for advanced queries. - <code>max_retries</code>: (Optional) The maximum number of retry attempts in case of transient errors (default is 3, max is 10).</p> <p>Returns: - The result of the view method call, typically containing the queried data.</p> <p>Example: <pre><code>near = env.set_near()\nresult = await near.view(\n    contract_id=\"wrap.near\",\n    method_name=\"ft_balance_of\",\n    args={\n        \"account_id\": \"user.near\"\n    }\n)\n\nprint(\"Wrap.NEAR Balance:\", result.result)\n</code></pre></p>"},{"location":"agents/env/near/#near-call-method","title":"NEAR CALL Method","text":"<p>Executes a state-changing operation on the NEAR blockchain. This is used to call methods on contracts that can modify state, transfer tokens, or perform other operations requiring gas and/or attached tokens.</p> <p>The result object contains the transaction details, including the status, transaction hash, and any returned values. For more details on the format of the result object, refer to the py-near documentation.</p> <pre><code>near = env.set_near(\"user.near\", \"ed25519:3ABCD...XYZ\")\nawait near.call(\n    contract_id: str,\n    method_name: str,\n    args: dict,\n    gas: int = DEFAULT_ATTACHED_GAS,\n    amount: int = 0,\n    nowait: bool = False,\n    included: bool = False,\n    max_retries: int = 3\n)\n</code></pre> <p>Parameters: - <code>contract_id</code>: The NEAR account ID of the smart contract you want to call. - <code>method_name</code>: The name of the method to invoke on the contract. - <code>args</code>: A dictionary of arguments to pass to the method. - <code>gas</code>: (Optional) The amount of gas to attach for execution (default: DEFAULT_ATTACHED_GAS).  - <code>amount</code>: (Optional) The amount of NEAR tokens to attach to the transaction (default: 0).  - <code>nowait</code>: (Optional) If True, the call will not wait for transaction confirmation.  - <code>included</code>: (Optional) If True, ensures the transaction is included in the block before returning. - <code>max_retries</code>: (Optional) The maximum number of retry attempts in case of transient errors (default is 3, max is 10). Use this parameter only if necessary, as there is a risk that the transaction might be sent multiple times.</p> <p>Returns: - The result of the contract method call, including transaction details and status.</p> <p>Example: <pre><code>result = await env.near.call(\n    contract_id=\"wrap.near\",\n    method_name=\"ft_transfer\",\n    args={\n        \"receiver_id\": \"example.near\",\n        \"amount\": \"1000000\"\n    },\n    gas=30000000000000,\n    amount=1\n)\n\nif \"SuccessValue\" in result.status:\n    print(\"tx\", result.transaction.hash)\n</code></pre></p>"},{"location":"agents/env/near/#near-get-balance","title":"NEAR GET BALANCE","text":"<p><code>get_balance</code> retrieves the NEAR token balance of a given account. </p> <p>Parameters: - <code>account_id</code> : (Optional) The ID of the account to retrieve the balance for. If not provided, the balance of the current account is retrieved.</p> <p>Example: <pre><code>near = env.set_near(\"alice.near\")\nprint(await near.get_balance())\nprint(await near.get_balance(\"bob.near\"))\n</code></pre></p>"},{"location":"agents/env/overview/","title":"The Environment Object","text":"<p>Each time an agents executes it receives an environment, which gives it access to features such as:</p> <ul> <li>Retrieve messages in the conversation, both from the user and the agent</li> <li>Request input from the user</li> <li>Read and write files on the agent's storage</li> <li>Call other agents</li> </ul>"},{"location":"agents/env/overview/#available-variables","title":"Available variables","text":"<ul> <li><code>signer_account_id</code>: get the NEAR Account ID of the signer </li> </ul>"},{"location":"agents/env/tools/","title":"Tools &amp; Commands","text":"<p>NEAR AI supports function based tool calling where the LLM can decide to call one of the functions (Tools) that you pass it. You can register your own function or use any of the built-in tools (list_files, read_file, write_file, exec_command, query_vector_store, request_user_input).</p> <p>The tool registry supports OpenAI style tool calling and Llama style. When a llama model is explicitly passed to completion(s)_and_run_tools a system message is added to the conversation. This system message contains the tool definitions and instructions on how to invoke them  using <code>&lt;function&gt;</code> tags.</p> <p>To tell the LLM about your tools and automatically execute them when selected by the LLM, call one of these environment methods:</p> <ul> <li><code>completion_and_run_tools</code>: Allows tools to be passed and processes any returned tool_calls by running the tool</li> <li><code>completions_and_run_tools</code>: Handles tool calls and returns the full llm response.</li> <li><code>completion_and_get_tools_calls</code>: Returns completion message and/or tool calls from OpenAI or Llama tool formats.</li> </ul> <p>By default, these methods will add both the LLM response and tool invocation responses to the message list.  You do not need to call <code>env.add_message</code> for these responses. This behavior allows the LLM to see its call then tool responses in the message list on the next iteration or next run.  This can be disabled by passing <code>add_to_messages=False</code> to the method.</p> <ul> <li><code>get_tool_registry</code>: returns the tool registry, a dictionary of tools that can be called by the agent. By default it is populated with the tools listed above for working with files and commands plus <code>request_user_input</code>. To register a function as a new tool, call <code>register_tool</code> on the tool registry, passing it your function.  <pre><code>def my_tool():\n    \"\"\"A simple tool that returns a string. This docstring helps the LLM know when to call the tool.\"\"\"\n    return \"Hello from my tool\"\n\ntool_registry = env.get_tool_registry()\ntool_registry.register_tool(my_tool)\ntool_def = tool_registry.get_tool_definition('my_tool')\nresponse = env.completions_and_run_tools(messages, tools=[tool_def])\n</code></pre></li> </ul> <p>To pass all the built in tools plus any you have registered use the <code>get_all_tool_definitions</code> method. <pre><code>all_tools = env.get_tool_registry().get_all_tool_definitions()\nresponse = env.completions_and_run_tools(messages, tools=all_tools)\n</code></pre> If you do not want to use the built-in tools, use <code>get_tool_registry(new=True)</code> <pre><code>    tool_registry = env.get_tool_registry(new=True)\n    tool_registry.register_tool(my_tool)\n    tool_registry.register_tool(my_tool2)\n    response = env.completions_and_run_tools(messages, tools=tool_registry.get_all_tool_definitions())\n</code></pre></p>"},{"location":"agents/env/tools/#terminal-commands","title":"Terminal Commands","text":"<p>Agents have access to the local terminal through the environment, the following methods are available:</p> Method Description <code>list_terminal_commands()</code> Lists the history of terminal commands executed by the agent <code>exec_command(command)</code> Executes the terminal <code>command</code> and returns the output"},{"location":"agents/env/variables/","title":"Environment Variables","text":"<p>When working with agents, managing configuration parameters through environment variables can provide a flexible way to adjust settings without altering the underlying code. This approach is particularly useful when dealing with sensitive information or configuration that needs to be customized without modifying the agent's codebase.</p>"},{"location":"agents/env/variables/#storing-environment-variables","title":"Storing Environment Variables","text":"<p>Environment variables can be stored in a metadata.json file. Here\u2019s an example of how to structure this file:</p> <pre><code>{\n  \"details\": {\n    \"env_vars\": {\n      \"id\": \"id_from_env\",\n      \"key\": \"key_from_env\"\n    }\n  }\n}\n</code></pre>"},{"location":"agents/env/variables/#accessing-environment-variables-in-code","title":"Accessing Environment Variables in Code","text":"<p>In your agent\u2019s code, you can access these environment variables using Python\u2019s os module or by accessing the env_vars dictionary directly.</p> <p>To retrieve an environment variable in the agent code:</p> <pre><code># Using env.env_vars\nvalue = env.env_vars.get('VARIABLE_NAME', 'default_value')\n\n# Using os.environ\nimport os\nvalue = os.environ.get('VARIABLE_NAME', 'default_value')\n\n# Or using globals()\nvalue = globals()['env'].env_vars.get('VARIABLE_NAME', 'default_value')\n</code></pre> <p>This allows users to fork the agent, modify the environment variables in <code>metadata.json</code>, and achieve the desired behavior without changing the code itself.</p>"},{"location":"agents/env/variables/#running-the-agent-with-environment-variables","title":"Running the agent with Environment Variables","text":"<p>You can also pass environment variables directly when launching the agent. This can be useful for overriding or extending the variables defined in <code>metadata.json</code> and handling Sensitive Information: If your agent needs to interact with APIs or services that require secret keys or credentials, you can pass these as environment variables instead of hardcoding them. This ensures that sensitive information is not exposed in publicly accessible code.</p> <p>To run the agent with environment variables, use the following command:</p> <pre><code>nearai agent interactive user.near/agent/1 --local --env_vars='{\"foo\":\"bar\"}'\n</code></pre>"},{"location":"agents/env/variables/#example","title":"Example","text":"<p>Consider an agent <code>zavodil.near/test-env-agent/1</code> that has configurable environment variables.</p>"},{"location":"assistants/integrate/","title":"Integrate NEAR AI assistant","text":"<p>NEAR AI offers a powerful Assistant that answers questions, queries other agents, and more. You can integrate the Assistant into your own applications by using the Assistant API.</p> <p>NEAR AI Assistants API is compatible with OpenAI Assistants API.</p>"},{"location":"assistants/integrate/#step-0-login-into-near-account","title":"Step 0: Login into NEAR account","text":""},{"location":"assistants/integrate/#javascript-client-side-useful-for-wallets","title":"JavaScript, client side. Useful for wallets.","text":"<p>From client side, you can use the following function to sign the message and get the required NEAR AI authorization token.</p> <pre><code>async function login(wallet, message, nonce, recipient, callbackUrl) {\n    const signedMessage = await wallet.signMessage({\n        message,\n        nonce,\n        recipient,\n        callbackUrl\n    });\n    return {\n        signature: signedMessage.signature,\n        accountId: signedMessage.accountId,\n        publicKey: signedMessage.publicKey,\n        message,\n        nonce,\n        recipient,\n        callbackUrl\n    };\n}\n\n// Generate nonce based on current time in milliseconds\nconst nonce = String(Date.now());\nconst recipient = YOUR_RECIPIENT_ADDRESS;\nconst callbackUrl = YOUR_CALLBACK_URL;\n\n// Example usage of login function\nconst auth = await login(wallet, \"Login to NEAR AI\", nonce, recipient, callbackUrl);\n</code></pre>"},{"location":"assistants/integrate/#python","title":"Python","text":"<p>In Python, we recommend using the <code>nearai</code> CLI to login into your NEAR account. More details here.</p> <pre><code>nearai login\n</code></pre>"},{"location":"assistants/integrate/#step-1-create-a-thread","title":"Step 1: Create a Thread","text":"<p>A Thread represents a conversation between a user and one or many Assistants. You can create a Thread when a user (or your AI application) starts a conversation with your Assistant.</p>"},{"location":"assistants/integrate/#create-a-thread","title":"Create a Thread","text":"<p>In JavaScript:</p> <pre><code>import OpenAI from \"openai\";\nconst openai = new OpenAI({\n    baseURL: \"https://api.near.ai/v1\",\n    apiKey: `Bearer ${JSON.stringify(auth)}`,\n});\n\nconst thread = await openai.beta.threads.create();\n</code></pre> <p>In Python:</p> <pre><code>import openai\n\nclient = openai.OpenAI(\n    api_key=\"YOUR_NEARAI_SIGNATURE\",\n    base_url=\"https://api.near.ai/v1\",\n)\n\nthread = client.beta.threads.create()\n</code></pre>"},{"location":"assistants/integrate/#step-2-add-a-message-to-the-thread","title":"Step 2: Add a Message to the Thread","text":"<p>The contents of the messages your users or applications create are added as Message objects to the Thread. Messages can contain both text and files. There is a limit of 100,000 Messages per Thread and we smartly truncate any context that does not fit into the model's context window.</p>"},{"location":"assistants/integrate/#add-a-message-to-the-thread","title":"Add a Message to the Thread","text":"<p>In JavaScript:</p> <pre><code>const message = await openai.beta.threads.messages.create(\n  thread.id,\n  {\n    role: \"user\",\n    content: \"Help me plan my trip to Tokyo\"\n  }\n);\n</code></pre> <p>In Python:</p> <pre><code>message = client.beta.threads.messages.create(\n    thread_id=thread.id,\n    role=\"user\",\n    content=\"Help me plan my trip to Tokyo\"\n)\n</code></pre>"},{"location":"assistants/integrate/#step-3-create-a-run","title":"Step 3: Create a Run","text":"<p>Once all the user Messages have been added to the Thread, you can Run the Thread with any Assistant. Creating a Run uses the model and tools associated with the Assistant to generate a response. These responses are added to the Thread as assistant Messages.</p> <p>Runs are asynchronous, which means you'll want to monitor their status by polling the Run object until a terminal status is reached. For convenience, the 'create and poll' SDK helpers assist both in creating the run and then polling for its completion.</p>"},{"location":"assistants/integrate/#create-a-run","title":"Create a Run","text":"<p>In JavaScript:</p> <pre><code>const assistant_id = \"near-ai-agents.near/assistant/latest\"\nlet run = await openai.beta.threads.runs.createAndPoll(\n  thread.id,\n  { \n    assistant_id: assistant_id,\n  }\n);\n</code></pre> <p>In Python:</p> <pre><code>assistant_id = \"near-ai-agents.near/assistant/latest\"\nrun = client.beta.threads.runs.create_and_poll(\n    thread_id=thread.id,\n    assistant_id=assistant_id,\n)\n</code></pre> <p>Once the Run completes, you can list the Messages added to the Thread by the Assistant.</p> <pre><code>if (run.status === 'completed') {\n  const messages = await openai.beta.threads.messages.list(\n    run.thread_id\n  );\n  for (const message of messages.data.reverse()) {\n    console.log(`${message.role} &gt; ${message.content[0].text.value}`);\n  }\n} else {\n  console.log(run.status);\n}\n</code></pre> <pre><code>if run.status == 'completed':\n    messages = client.beta.threads.messages.list(run.thread_id)\n    for message in messages:\n        print(f\"{message.role} &gt; {message.content[0].text.value}\")\nelse:\n    print(run.status)\n</code></pre> <p>You may also want to list the Run Steps of this Run if you'd like to look at any tool calls made during this Run.</p>"},{"location":"assistants/overview/","title":"Assistants/Agents API overview","text":"<p>The Assistants API allows you to build AI assistants within your own applications. An Assistant has instructions and can leverage models, tools, and files to respond to user queries. </p>"},{"location":"assistants/overview/#how-assistants-work","title":"How Assistants work","text":"<p>The Assistants API is designed to help developers build powerful AI assistants capable of performing a variety of tasks.</p> <p>The Assistants API is in beta and we are actively working on adding more functionality.</p> <ol> <li>Assistants can call various models with specific instructions to tune their personality and capabilities.</li> <li>Assistants can access multiple tools.</li> <li>Assistants can access persistent Threads. Threads simplify AI application development by storing message history and truncating it when the conversation gets too long for the model\u2019s context length. You create a Thread once, and simply append Messages to it as your users reply.</li> <li>Assistants can access files in several formats \u2014 either as part of their creation or as part of Threads between Assistants and users. When using tools, Assistants can also create files (e.g., images, spreadsheets, etc) and cite files they reference in the Messages they create.</li> </ol>"},{"location":"assistants/overview/#key-concepts","title":"Key Concepts","text":"Object What it represents Assistant Purpose-built AI that uses various models and calls tools. Thread A conversation session between an Assistant and a user. Threads store Messages and automatically handle truncation to fit content into a model\u2019s context. Message A message created by an Assistant or a user. Messages can include text, images, and other files. Messages stored as a list on the Thread. Run An invocation of an Assistant on a Thread. The Assistant uses its configuration and the Thread\u2019s Messages to perform tasks by calling models and tools. As part of a Run, the Assistant appends Messages to the Thread. Run Step A detailed list of steps the Assistant took as part of a Run. An Assistant can call tools or create Messages during its run. Examining Run Steps allows you to introspect how the Assistant is getting to its final results. Service Agent A specialized Agent called by the Assistant to accomplish a task such as purchasing, undertaking a swap, or generating a smart contract."},{"location":"assistants/overview/#next-steps","title":"Next Steps","text":"<p>Integrate an Assistant into your application</p>"},{"location":"models/benchmarks_and_evaluations/","title":"Benchmarks and Evaluations","text":"<p><code>Benchmarks</code> allow you to compare different agents and solvers on specific tasks, so you can determine which is the best fit for your needs.</p> <p><code>Evaluations</code> are the results of running benchmarks. They are stored in the registry and can be used to compare different agents and solvers.</p>"},{"location":"models/benchmarks_and_evaluations/#how-is-a-benchmark-implemented","title":"How is a benchmark implemented?","text":"<p>A benchmark is the combination of a dataset and a solver (more on this below). </p> <p>Once you have created a dataset and its solver, you can run the benchmark using the <code>benchmark</code> command.</p> <p>For example, to run the <code>mpbb</code> benchmark on the <code>llama v3</code>, you can use:</p> <pre><code>nearai benchmark run mbpp MBPPSolverStrategy \\\n    --model llama-v3-70b-instruct \\\n    --subset=train \\\n    --max_concurrent=1\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#adding-a-benchmark-dataset","title":"Adding a benchmark dataset","text":"<p><code>nearai</code> leverages huggingface datasets as the primitive when operating with datasets + benchmarks (see <code>load_dataset</code>). This means that to add a new benchmark, you need to create a new dataset and register it with the <code>nearai</code> registry (we will go over this in Implementing the \"3 digit addition\" benchmark).</p> <p>There is also a support for datasets of custom format.</p>"},{"location":"models/benchmarks_and_evaluations/#adding-a-solver","title":"Adding a solver","text":"<p>To implement a solver, you will need to implement the SolverStrategy interface under the <code>nearai.solvers</code> module. The most important method the solver should implement is the <code>solve</code> method. This method should take a datum, run your implementation specific agentic strategy / strategies, and return a result.</p>"},{"location":"models/benchmarks_and_evaluations/#implementing-the-3-digit-addition-benchmark","title":"Implementing the \"3 digit addition\" benchmark","text":"<p>In this section we will be implementing a benchmark we'll call \"3 digit addition\". The goal of the benchmark is to test an agents ability to add two 1-3 digit numbers together. The dataset will consist of 1000 examples of 3 digit addition problems and their solutions. The solver will adjudicate the agent answers and return a single accuracy score. While this benchmark is simple and can be solved with a simple program, it serves as a good example of how to implement a benchmark in <code>nearai</code>.</p>"},{"location":"models/benchmarks_and_evaluations/#step-1-creating-the-dataset","title":"Step 1: Creating the dataset","text":"<p>To create this dataset, we will first synthetically generate the data. We will then register the dataset with the <code>nearai</code> registry.</p> <pre><code>import random\nfrom itertools import product\n\nimport datasets\n\nSAMPLE_SIZE = 1000\nSEED = 42\nPATH = \"3_digit_addition\"\n\nrandom.seed(SEED)\ndatasets.Dataset.from_generator(\n    lambda: iter(\n        {\n            \"input\": f\"{a} + {b}\",\n            \"output\": str(a + b)\n        }\n        for a, b in random.sample(list(product(range(1000), range(1000))), SAMPLE_SIZE)\n    ),\n    features=datasets.Features(\n        {\n            \"input\": datasets.Value(\"string\"),\n            \"output\": datasets.Value(\"string\")\n        }\n    )\n).save_to_disk(PATH)\n</code></pre> <p>Now to upload the dataset to the registry we'll run the command:</p> <pre><code>nearai registry upload ./3_digit_addition\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#step-2-creating-the-solver","title":"Step 2: Creating the solver","text":"<p>To create the solver, we will implement the <code>SolverStrategy</code> interface. The solver will take in a datum, parse the input, execute any setup for the agent, run the agent, and return the correctness of the agents result.</p> Remember <p>To ensure this solver is registered with <code>nearai</code>:</p> <ol> <li>Write this implementation in the <code>nearai.solvers</code> module.</li> <li>Import it in the <code>__init__.py</code> file in the <code>nearai.solvers</code> module.</li> </ol> <pre><code># ... other imports ...\nfrom pydantic import BaseModel\nfrom huggingface import Dataset\nfrom nearai.solvers import SolverStrategy\n\nfrom typing import Dict, List\n\nclass ThreeDigitAdditionDatum(BaseModel):\n    input: str\n    output: str\n\nclass ThreeDigitAdditionSolver(SolverStrategy):\n    \"\"\"Solver for the 3 digit addition benchmark.\"\"\"\n\n    def __init__(self, dataset_ref: Dataset, model: str = \"\", agent: str = \"\"):\n        super().__init__(model, agent)\n        self.dataset_ref = dataset_ref\n\n    def evaluation_name(self) -&gt; str:\n        return \"3_digit_addition\"\n\n    def compatible_datasets(self) -&gt; List[str]:\n        return [\"3_digit_addition\"]\n\n    def solve(self, datum: Dict[str, str]) -&gt; bool:\n        datum = ThreeDigitAdditionDatum(**datum)\n        label = datum.input.replace(\" + \", \"+\")\n        session = self.start_inference_session(label)\n\n        goal = f\"\"\"Please add the following numbers together: {datum.input}\\n\\nOutput the result only.\"\"\"\n        result = session.run_task(goal).strip()\n        return result == datum.output\n</code></pre> <p>The code above can run for both models and agents. If both <code>model</code> and <code>agent</code> are given, the <code>model</code> value will be inserted into <code>agent</code> metadata.</p> <p>To check agent functionality to write files: <pre><code>    def solve(self, datum: Dict[str, str]) -&gt; bool:\n        datum = ThreeDigitAdditionDatum(**datum)\n        label = datum.input.replace(\" + \", \"+\")\n        session = self.start_inference_session(label)\n\n        goal = f\"\"\"Please add the following numbers together: {datum.input}\\n\\nOutput the result in a file called 'result.txt'.\"\"\"\n        session.run_task(goal)\n        with open(os.path.join(session.path, \"result.txt\"), \"r\") as f:\n            result = f.read().strip()\n        return result == datum.output\n</code></pre></p>"},{"location":"models/benchmarks_and_evaluations/#step-3-running-the-benchmark","title":"Step 3: Running the benchmark","text":"<p>To run the benchmark, we will use the <code>nearai</code> CLI. We will specify the dataset and solver we want to use.</p> <pre><code>nearai benchmark run near.ai/3_digit_addition/1.00 ThreeDigitAdditionSolver --agent ~/.nearai/registry/&lt;my_agent&gt;\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#benchmarks-cache","title":"Benchmarks Cache","text":"<p>Benchmark individual tasks and completion are cached in registry or locally. To see registry benchmark completion caches:</p> <pre><code>nearai benchmark list\n</code></pre> <p>To force execution and overwrite cache pass <code>--force</code> flag.</p> <pre><code>nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'llama-3p2-1b-instruct' --subset test --force\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#example-runs","title":"Example runs","text":"<pre><code>$ nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'llama-3p2-1b-instruct' --subset test\n$ nearai benchmark run near.ai/mmlu/1.0.0 MMLUSolverStrategy --model 'llama-v3p1-405b-instruct' --subset test\n$ nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'qwen2p5-72b-instruct' --subset test --agent ~/.nearai/registry/flatirons.near/example-travel-agent/1\n$ nearai benchmark run near.ai/live_bench/1.0.0 LiveBenchSolverStrategy --model 'qwen2p5-72b-instruct' --agent ~/.nearai/registry/flatirons.near/example-travel-agent/1\n</code></pre>"},{"location":"models/benchmarks_and_evaluations/#evaluations","title":"Evaluations","text":""},{"location":"models/benchmarks_and_evaluations/#recording-benchmark-result-as-an-evaluation","title":"Recording benchmark result as an evaluation","text":"<p>To record benchmark results as an evaluation, pass <code>--record</code>. It is strongly recommended to pass this flag after verifying successful run of the benchmark.</p> <pre><code>$ nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'llama-3p2-1b-instruct' --subset test\nFinal score: 131/500 - 26.20%\n$ nearai benchmark run near.ai/mbpp/1.0.0 MBPPSolverStrategy --model 'llama-3p2-1b-instruct' --subset test --record\n</code></pre> <p>That creates new evaluation entry in the registry: <pre><code>$ nearai registry list --category=evaluation\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 entry                                                                  \u2502 category   \u2502 description   \u2502 tags   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 alomonos.near/evaluation_mbpp_model_llama-v3p2-1b-                     \u2502 evaluation \u2502               \u2502        \u2502\n\u2502 instruct_provider_fireworks/0.0.1                                      \u2502            \u2502               \u2502        \u2502\n</code></pre></p>"},{"location":"models/benchmarks_and_evaluations/#view-evaluation-table","title":"View evaluation table","text":"<p>To view evaluation table in CLI: <pre><code>$ nearai evaluation table --num_columns=8\n$ nearai evaluation table --all_key_columns --num_columns=8\n$ nearai evaluation table --all_metrics\n</code></pre></p> <p>https://app.near.ai/evaluations has a functionality to choose any columns.</p>"},{"location":"models/benchmarks_and_evaluations/#submit-an-experiment","title":"Submit an experiment","text":"<p>To submit a new experiment run:</p> <pre><code>nearai submit --command &lt;COMMAND&gt; --name &lt;EXPERIMENT_NAME&gt; [--nodes &lt;NUMBER_OF_NODES&gt;] [--cluster &lt;CLUSTER&gt;]\n</code></pre> <p>This will submit a new experiment. The command must be executed from a folder that is a git repository (public github repositories, and private github repositories on the same organization as nearai are supported). The current commit will be used for running the command so make sure it is already available online. The diff with respect to the current commit will be applied remotely (new files are not included in the diff).</p> <p>On each node the environment variable <code>ASSIGNED_SUPERVISORS</code> will be available with a comma separated list of supervisors that are running the experiment. The current supervisor can be accessed via <code>nearai.CONFIG.supervisor_id</code>. See examples/prepare_data.py for an example.</p>"},{"location":"models/benchmarks_and_evaluations/#issues","title":"Issues","text":"<ul> <li>Overwriting existing evaluation entry is currently not supported</li> <li>litellm.Timeout errors when running benchmark</li> <li>Feature request: tag individual evaluation metrics</li> <li>Feature request: add view for a metric</li> <li>Feature request: add cost of running benchmark to evaluation results as a separate metric</li> <li>Feature request: hide evaluation results for hidden agents and models</li> <li>Capabilities Benchmarks Tracking: list of benchmarks we want to add</li> </ul>"},{"location":"models/fine_tuning/","title":"<code>nearai</code> fine-tuning guide","text":"<p>As a part of the <code>nearai</code> project, we provide a collection of tools to fine-tune and evaluate models. Fine-tuning is a set of techniques to tune model parameters in a parameter-efficient way to improve model performance on specific tasks. More commonly, fine-tuning is used to modify the behavior of a pre-trained model. Some examples of this are to produce structured output (JSON, XLM, etc), to produce stylized output (poetic, neutral, etc), or to respond properly to instruction based prompts.</p> <p>In this guide, we will walk through the process of fine-tuning <code>llama-3-8b-instruct</code> on the orca-math-word-problems-200k dataset to improve its performance on the gsm8k benchmark.</p>"},{"location":"models/fine_tuning/#step-1-create-the-dataset","title":"Step 1: Create the dataset","text":"<p>The two datasets we will be using are orca-math-word-problems-200k and gsm8k. Both datasets are a collection of word based math problems + answers. For convenience, we will download the datasets from huggingface, save it to disk, and then upload it into the <code>nearai</code> registry.</p> <pre><code>import re\nfrom textwrap import dedent\nfrom datasets import load_dataset, concatenate_datasets, DatasetDict\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n\nds_math_word_problems = load_dataset(\"HuggingFaceH4/orca-math-word-problems-200k\")\nds_gsm8k = load_dataset(\"openai/gsm8k\", \"main\")['train']\n\n## create new column by concatenating the 'question' and 'answer' columns\ndef to_q_a(x):\n    q_n_a = tokenizer.apply_chat_template(\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant. Please answer the math question.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": x[\"question\"]\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": x[\"answer\"]\n            }\n        ],\n        tokenize=False\n    )\n    return {\n        \"question_and_answer\": q_n_a\n    }\nds_math_word_problems = ds_math_word_problems.map(to_q_a)\n\ndef to_q_a_gsm8k(x):\n    q_n_a = tokenizer.apply_chat_template(\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant. Please answer the math question.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": x[\"question\"]\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": re.sub(r\"&lt;&lt;.*?&gt;&gt;\", \"\", x[\"answer\"])\n            }\n        ],\n        tokenize=False\n    )\n    return {\n        \"question_and_answer\": q_n_a\n    }\nds_gsm8k = ds_gsm8k.map(to_q_a_gsm8k)\n\n## combine the datasets\nds_combined = concatenate_datasets([ds_math_word_problems['train_sft'], ds_gsm8k])\nds_combined = ds_combined.remove_columns([col for col in ds_combined.column_names if col != \"question_and_answer\"])\n\n# Add a split on 'train'\nds_combined = ds_combined.train_test_split(test_size=0.0001, seed=42)\nds_dict = DatasetDict({\n    'train': ds_combined['train'],\n    'validation': ds_combined['test']\n})\nds_dict.save_to_disk(\"orca_math_gsm8k_train\")\n</code></pre> <pre><code>nearai registry upload ./orca_math_gsm8k_train\n</code></pre>"},{"location":"models/fine_tuning/#step-2-fine-tune-the-model","title":"Step 2: Fine-tune the model","text":"<p>Under the hood, <code>nearai</code> uses torchtune to manage the fine-tuning process. To launch a fine-tuning job you can use <code>nearai finetune</code>.</p> <p>Here is the command we used to fine-tune <code>llama-3-8b-instruct</code> on our combined <code>orca-math-word-problems-200k</code> and <code>gsm8k</code> dataset on an 8-GPU machine:</p> <pre><code>poetry run python3 -m nearai finetune start \\\n    --model llama-3-8b-instruct \\\n    --format llama3-8b \\\n    --tokenizer llama-3-8b-instruct/tokenizer.model \\\n    --dataset ./orca_math_gsm8k_train \\\n    --method nearai.finetune.text_completion.dataset \\\n    --column question_and_answer \\\n    --split train \\\n    --num_procs 8\n</code></pre> <p>To change the configuration of the fine-tuning job, edit <code>etc/finetune/llama3-8b.yml</code>.</p> <p>Included in the output of the command is the path to the fine-tuned model checkpoint. In this case, the path was <code>~.nearai/finetune/job-2024-08-29_20-58-08-207756662/checkpoint_output</code>. The path may/will be different based on the time you run the command.</p>"},{"location":"models/fine_tuning/#step-3-serve-the-fine-tuned-model","title":"Step 3: Serve the fine-tuned model","text":"<p>To serve fine-tuned models, we use vllm. Once we serve the fine-tuned model + the baseline model, we will benchmark it against both.</p> <pre><code>poetry run python3 -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Meta-Llama-3-8B-Instruct \\\n    --enable-lora \\\n    --lora-modules mynewlora=&lt;path_to_checkpoint&gt; \\\n    --tensor-parallel 8\n</code></pre> <p>Now we will run the <code>gsm8k</code> benchmark on both the baseline model and the fine-tuned model using <code>nearai benchmark</code>. The solvers will call our fine-tuned model and the baseline model through the vllm server.</p> <pre><code>python3 -m nearai benchmark run \\\n    cmrfrd.near/gsm8k/0.0.2 \\\n    GSM8KSolverStrategy \\\n    --subset test \\\n    --model local::meta-llama/Meta-Llama-3-8B-Instruct\n\npython3 -m nearai benchmark run \\\n    cmrfrd.near/gsm8k/0.0.2 \\\n    GSM8KSolverStrategy \\\n    --subset test \\\n    --model local::mynewlora\n</code></pre> <p>And we can see the results of the benchmark. And we can see that the fine-tuned model performs better than the baseline model.</p> <pre><code># meta-llama/Meta-Llama-3-8B-Instruct\nCorrect/Seen - 1061/1319 - 80.44%\n\n# fine tuned llama3-8b-instruct\nCorrect/Seen - 966/1319 - 73.24%\n</code></pre> <p>From these results, we can see that our fine-tuned model needs improvement to perform better than the baseline model.</p>"}]}